WEBVTT

00:00:00.000 --> 00:00:04.400
Large language models and chat-based AIs are kind of mind-blowing at the moment.


00:00:04.400 --> 00:00:10.640
Many of us are playing with them for working on code or just as a fun alternative to search,


00:00:10.640 --> 00:00:14.320
but others of us are building applications with AI at the core.


00:00:14.320 --> 00:00:21.200
And when doing that, the slight unpredictable nature and probabilistic style of LLMs makes


00:00:21.200 --> 00:00:23.840
writing and testing Python code very tricky.


00:00:23.840 --> 00:00:28.080
Interpromptimize, from Maxine Bocheman, and Preset.


00:00:28.080 --> 00:00:33.040
It's a framework for non-deterministic testing of LLMs inside of our applications.


00:00:33.040 --> 00:00:41.040
Let's dive inside the AIs with Max. This is Talk Python To Me, episode 417, recorded May 22nd, 2023.


00:00:54.160 --> 00:00:59.360
Welcome to Talk Python.me, a weekly podcast on Python. This is your host, Michael Kennedy.


00:00:59.360 --> 00:01:04.560
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,


00:01:04.560 --> 00:01:09.360
both on fosstodon.org. Be careful with impersonating accounts on other instances,


00:01:09.360 --> 00:01:14.000
there are many. Keep up with the show and listen to over seven years of past episodes at


00:01:14.000 --> 00:01:20.160
talkpython.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our


00:01:20.160 --> 00:01:25.840
YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be


00:01:25.840 --> 00:01:28.160
part of that episode.


00:01:28.160 --> 00:01:34.400
This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.


00:01:34.400 --> 00:01:41.320
Download your free trial of PyCharm Professional at talkpython.fm/done-with-pycharm.


00:01:41.320 --> 00:01:45.080
And it's brought to you by The Compiler Podcast from Red Hat.


00:01:45.080 --> 00:01:53.200
to an episode of their podcast to demystify the tech industry over at talkbython.fm/compiler.


00:01:53.200 --> 00:01:54.520
Max welcome to Talk Python to Me.


00:01:54.520 --> 00:01:59.520
>> Well it's good to be back on the show and now I know it's live too so no mistakes.


00:01:59.520 --> 00:02:01.640
I'm going to try to not say anything outrageous.


00:02:01.640 --> 00:02:04.920
>> People get the unfiltered versions though.


00:02:04.920 --> 00:02:05.920
>> Absolutely, absolutely.


00:02:05.920 --> 00:02:08.320
I love it when people come check out the live show.


00:02:08.320 --> 00:02:09.320
>> Welcome back.


00:02:09.320 --> 00:02:11.800
It's been a little while since you were on the show.


00:02:11.800 --> 00:02:13.080
About since September.


00:02:13.080 --> 00:02:15.040
We talked about supersets.


00:02:15.040 --> 00:02:17.080
We also talked a little bit about Airflow,


00:02:17.080 --> 00:02:19.320
some of the stuff that you've been working on.


00:02:19.320 --> 00:02:21.760
And now we're kind of circling back


00:02:21.760 --> 00:02:23.880
through this data side of things,


00:02:23.880 --> 00:02:26.820
but trying to bring AI into the whole story.


00:02:26.820 --> 00:02:28.520
So pretty cool project


00:02:28.520 --> 00:02:29.920
I'm looking forward to talking to you about.


00:02:29.920 --> 00:02:31.660
- Awesome, I'm excited to talk about it too.


00:02:31.660 --> 00:02:34.880
And these things are related in many ways.


00:02:34.880 --> 00:02:38.040
Like one common foundation is Python.


00:02:38.040 --> 00:02:40.040
Like a lot of these projects are in Python.


00:02:40.040 --> 00:02:41.120
They're data related.


00:02:41.120 --> 00:02:43.440
And here, problemize and prompt engineering


00:02:43.440 --> 00:02:46.880
and integrating AI is related in a way


00:02:46.880 --> 00:02:49.680
that we're building some AI features


00:02:49.680 --> 00:02:52.600
into super set right now and into presets.


00:02:52.600 --> 00:02:55.040
So it ties things together.


00:02:55.040 --> 00:02:57.520
- Yeah, I can certainly see a lot of synergy here.


00:02:57.520 --> 00:02:58.520
Before we dive into it,


00:02:58.520 --> 00:03:00.600
it hasn't been that long since you were on the show,


00:03:00.600 --> 00:03:02.440
but give us a quick update,


00:03:02.440 --> 00:03:03.600
just a bit about your background


00:03:03.600 --> 00:03:05.060
for people who don't know you.


00:03:05.060 --> 00:03:08.320
- Yeah, so my career is a career


00:03:08.320 --> 00:03:13.680
like maybe 20 or so years in data building, doing data engineering,


00:03:13.680 --> 00:03:18.360
doing, try to make useful data useful for organizations.


00:03:18.360 --> 00:03:22.500
Um, over the past decade or so, I've been very involved in open source.


00:03:22.500 --> 00:03:25.200
I started Apache Airflow 2014.


00:03:25.200 --> 00:03:28.920
So for those not familiar with Airflow, though, it's pretty well known now.


00:03:28.920 --> 00:03:33.180
It's used at, I heard like, I think it's like tens of thousands, I think above


00:03:33.180 --> 00:03:36.940
a hundred thousand companies are using Apache Airflow, which is kind of insane.


00:03:37.180 --> 00:04:06.820
to think about. It's like you started a little project. So for me, I started this project at Airbnb, and it really took off. And I think it's just like great, called project community fit, like, we'll really need it needed that was the right abstraction for people at the time, and still today, and it just, it just really took off. So I was working on orchestration. And then, then I was like, I just love things that are visual and interactive. So there was no great open source BI tool out there business intelligence. So this old data,


00:04:06.860 --> 00:04:10.740
data dashboarding, exploration, SQL IDE.


00:04:10.740 --> 00:04:13.620
So it's a playground for people trying to understand


00:04:13.620 --> 00:04:15.140
and visualize and explore data.


00:04:15.140 --> 00:04:17.780
So I started working on Apache Superset in 2000,


00:04:17.780 --> 00:04:20.500
it was like 15 or 16 at Airbnb too.


00:04:20.500 --> 00:04:23.100
And we also brought that to the Apache Software Foundation.


00:04:23.100 --> 00:04:27.020
So again, I had a very, very popular open source project


00:04:27.020 --> 00:04:29.220
that's used in like tens of thousands,


00:04:29.220 --> 00:04:32.220
maybe a hundred thousand organizations or so.


00:04:32.220 --> 00:04:33.100
And a great, like today,


00:04:33.100 --> 00:04:36.620
like it has become a super great open source


00:04:36.620 --> 00:04:38.780
or alternatives to Tableau, Looker,


00:04:38.780 --> 00:04:40.660
like all those business intelligence tool


00:04:40.660 --> 00:04:43.820
is very viable for organizations.


00:04:43.820 --> 00:04:46.020
And then quick plugs, I had pre-set,


00:04:46.020 --> 00:04:47.100
I had a company I started,


00:04:47.100 --> 00:04:50.060
I'm also an entrepreneur, I started a company


00:04:50.060 --> 00:04:51.340
a little bit more than four years ago


00:04:51.340 --> 00:04:52.500
around Apache Superset,


00:04:52.500 --> 00:04:56.780
and the idea is to bring Superset to the masses.


00:04:56.780 --> 00:04:59.700
So it's really like hosted, manage,


00:04:59.700 --> 00:05:02.020
state of the art Apache Superset for everyone


00:05:02.020 --> 00:05:03.260
with some bells and whistles.


00:05:03.260 --> 00:05:05.620
So the best Superset you can run,


00:05:05.620 --> 00:05:07.220
There's a free version too.


00:05:07.220 --> 00:05:09.720
So you can go and play and try it,


00:05:09.720 --> 00:05:11.100
today gets started in five minutes.


00:05:11.100 --> 00:05:14.340
So it's a little bit of a commercial pointer,


00:05:14.340 --> 00:05:16.980
but also very relevant to what I've been doing,


00:05:16.980 --> 00:05:18.460
personally over the past like four years.


00:05:18.460 --> 00:05:20.520
- Sure, it's some of the inspiration


00:05:20.520 --> 00:05:22.300
for some of the things we're gonna talk about as well


00:05:22.300 --> 00:05:25.320
and trying to bring some of the AI craziness


00:05:25.320 --> 00:05:26.940
back to products, right?


00:05:26.940 --> 00:05:29.300
From an engineering perspective, not just a,


00:05:29.300 --> 00:05:34.160
hey, look, I asked what basketball team was gonna win


00:05:34.160 --> 00:05:36.360
this year and it gave me this answer, right?


00:05:36.360 --> 00:05:38.040
- It's like, and I, caveat,


00:05:38.040 --> 00:05:40.920
I don't know anything that happened since 2021.


00:05:40.920 --> 00:05:43.400
So AI, or AI specifically,


00:05:43.400 --> 00:05:44.720
Bart is a little bit better at that,


00:05:44.720 --> 00:05:46.220
but it's like, you know,


00:05:46.220 --> 00:05:48.140
the last thing I read off of the internet


00:05:48.140 --> 00:05:49.840
was in fall 2021.


00:05:49.840 --> 00:05:52.640
Wow, it makes things a little bit challenging.


00:05:52.640 --> 00:05:53.760
But yeah, so we're looking,


00:05:53.760 --> 00:05:56.520
we're building, you know, AI features into preset,


00:05:56.520 --> 00:05:58.080
you know, as a commercial open source company,


00:05:58.080 --> 00:06:00.760
we need to build some differentiators too from supersets.


00:06:00.760 --> 00:06:02.240
So we contribute a huge amount,


00:06:02.240 --> 00:06:05.140
Like maybe 58% of the work we do at Preset


00:06:05.140 --> 00:06:06.640
is contributed back to Superset,


00:06:06.640 --> 00:06:09.020
but we're looking to build on differentiators.


00:06:09.020 --> 00:06:11.940
And we feel like AI is a great


00:06:11.940 --> 00:06:13.540
kind of commercial differentiator too,


00:06:13.540 --> 00:06:17.020
on top of Superset that makes people even more interested


00:06:17.020 --> 00:06:20.540
to come and run Preset too.


00:06:20.540 --> 00:06:21.540
- Yeah, excellent.


00:06:21.540 --> 00:06:25.460
And people, you say they were popular projects,


00:06:25.460 --> 00:06:28.580
like Airflow has 30,000 stars,


00:06:28.580 --> 00:06:31.300
Apache Superset has 50,000 stars,


00:06:31.300 --> 00:06:34.060
which puts it on par with Django and Flask


00:06:34.060 --> 00:06:36.020
for people's sort of mental models out there,


00:06:36.020 --> 00:06:37.820
which is, I would say, it's pretty well known.


00:06:37.820 --> 00:06:39.360
So awesome.


00:06:39.360 --> 00:06:42.940
- Yeah, stars are kind of vanity metric in some ways, right?


00:06:42.940 --> 00:06:46.300
Not necessarily usefulness or value delivered,


00:06:46.300 --> 00:06:49.180
but it's a proxy for popularity and hype, you know?


00:06:49.180 --> 00:06:50.820
So it gives a good sense.


00:06:50.820 --> 00:06:53.620
And I think like at 50,000 stars,


00:06:53.620 --> 00:06:54.740
if you look at, you know,


00:06:54.740 --> 00:06:57.460
it's probably in the top 100 of GitHub projects.


00:06:57.460 --> 00:07:00.700
If you remove the, in the top 100,


00:07:00.700 --> 00:07:03.260
there's a lot of documentations and guides


00:07:03.260 --> 00:07:05.760
and things that are not really open source projects.


00:07:05.760 --> 00:07:09.300
It's probably like top 100 open source project-ish,


00:07:09.300 --> 00:07:11.340
you know, in both cases, which--


00:07:11.340 --> 00:07:12.180
- Right.


00:07:12.180 --> 00:07:13.020
- Some of them, it doesn't matter.


00:07:13.020 --> 00:07:14.100
- Oh, like it's like, you start to,


00:07:14.100 --> 00:07:17.100
oh no, like how, like whether it's gonna take off and how,


00:07:17.100 --> 00:07:19.620
and you know, it's like, wow, it's just nice to see that.


00:07:19.620 --> 00:07:20.580
- Yeah, absolutely.


00:07:20.580 --> 00:07:22.420
I mean, on one hand, it's nice,


00:07:22.420 --> 00:07:24.660
but it doesn't necessarily make it better.


00:07:24.660 --> 00:07:26.360
But it does mean there's a lot of people using it,


00:07:26.360 --> 00:07:30.500
there's a lot of polish, there's a lot of PRs and stuff


00:07:30.500 --> 00:07:32.580
that have been submitted to make it work,


00:07:32.580 --> 00:07:34.120
a lot of things that can plug into, right?


00:07:34.120 --> 00:07:35.500
So there's certainly a value


00:07:35.500 --> 00:07:38.260
for having a project popular versus unpopular.


00:07:38.260 --> 00:07:39.080
- Oh my God, yes.


00:07:39.080 --> 00:07:40.500
And I would say one thing is,


00:07:40.500 --> 00:07:43.460
all the dark, like call it like secondary assets


00:07:43.460 --> 00:07:46.920
outside of the core projects documentation.


00:07:46.920 --> 00:07:48.860
And there will be a lot of like use cases


00:07:48.860 --> 00:07:51.140
and testimonials and reviews


00:07:51.140 --> 00:07:54.260
and people bending the framework in various ways


00:07:54.260 --> 00:07:56.100
and forks and plugins.


00:07:56.100 --> 00:07:58.300
Another thing too, that people, I think,


00:07:58.300 --> 00:08:02.200
don't really understand the value of an in-software and open source is,


00:08:02.200 --> 00:08:04.540
or I'm sure people understand the value,


00:08:04.540 --> 00:08:07.440
but it's not talked about as just a whole battle-tested thing.


00:08:07.440 --> 00:08:13.240
Like when something is run at thousands of organizations in production for a long time,


00:08:13.240 --> 00:08:17.820
there's a lot of things that happen in a software that are very,


00:08:17.820 --> 00:08:20.800
very valuable for the incremental organization adopting it.


00:08:20.800 --> 00:08:25.140
>> Well, let's talk large language models for a second.


00:08:25.140 --> 00:08:28.940
So AI means different things to different people, right?


00:08:28.940 --> 00:08:31.700
They kind of get carved off as they find


00:08:31.700 --> 00:08:34.980
some kind of productive productized use, right?


00:08:34.980 --> 00:08:36.460
AI is this general term and like,


00:08:36.460 --> 00:08:38.980
oh, machine learning is now a thing that we've done


00:08:38.980 --> 00:08:40.900
or computer vision is a thing we've done.


00:08:40.900 --> 00:08:43.140
And the large language models are starting to find


00:08:43.140 --> 00:08:45.380
their own special space.


00:08:45.380 --> 00:08:49.220
So maybe we could talk a bit about a couple of examples


00:08:49.220 --> 00:08:51.420
just so people get a sense.


00:08:51.420 --> 00:08:54.980
To me, ChatGPT seems like the most well-known.


00:08:54.980 --> 00:08:55.620
What do you think?


00:08:55.620 --> 00:09:00.780
>> Yeah. I'll say if you think about what is a large language model,


00:09:00.780 --> 00:09:02.900
and what are some of the leaps there?


00:09:02.900 --> 00:09:06.740
I'm not an expert, so I'm going to try to put my foot in my mouth.


00:09:06.740 --> 00:09:09.020
But some things that I think are interesting,


00:09:09.020 --> 00:09:12.500
a large language model is a big neural network


00:09:12.500 --> 00:09:15.820
that is trained on a big corpus of text.


00:09:15.820 --> 00:09:19.780
I think one of the big leaps we've seen is unsupervised learning.


00:09:19.780 --> 00:09:22.580
Really often, like in machine learning in the past,


00:09:22.580 --> 00:09:27.120
or pre-LLMs, we would have very specific


00:09:27.120 --> 00:09:30.520
training set and outcomes we were looking for.


00:09:30.520 --> 00:09:33.480
And then the training data would have to be really structured.


00:09:33.480 --> 00:09:35.740
Here what we're doing with large language models is


00:09:35.740 --> 00:09:38.680
feeding a lot of, a huge corpus of text,


00:09:38.680 --> 00:09:43.240
and what the large language model is trying to do or resolve is to chain words, right?


00:09:43.240 --> 00:09:45.380
So it's trying to predict the next word,


00:09:45.380 --> 00:09:49.040
which seems like you would be able to put words together


00:09:49.040 --> 00:09:54.480
that kind of makes sense, but like you wouldn't think that consciousness, not just like consciousness,


00:09:54.480 --> 00:09:58.080
but intelligence would come out of that. But somehow it does, right? Like if you chain,


00:09:58.080 --> 00:10:04.480
it's like if you say, you know, Hapthi Dhati sits on a, it's really clear it's going to be wall,


00:10:04.480 --> 00:10:10.960
you know, the next word. But if you push this idea much further with a very large corpus of like


00:10:10.960 --> 00:10:15.920
human knowledge, somehow there's some really amazing stuff that does happen. All these


00:10:15.920 --> 00:10:19.360
are distinguished models and I think that realization around


00:10:19.360 --> 00:10:22.960
happened at around you know, at GPT-3,


00:10:22.960 --> 00:10:26.800
3.5 getting pretty good and then at 4 like, oh my god, this


00:10:26.800 --> 00:10:29.760
stuff can really kind of seems like it can


00:10:29.760 --> 00:10:34.400
think or be smart and be very helpful. Yeah, the thing that I think impresses me


00:10:34.400 --> 00:10:37.760
the most about these is they seem and people can


00:10:37.760 --> 00:10:41.280
tell me it's statistics and you know, I'll believe them but it seems


00:10:41.280 --> 00:10:43.920
like they have an understanding of the context


00:10:43.920 --> 00:10:48.240
of what they're talking about more than just predicting like Humpty Dumpty sat on the what?


00:10:48.240 --> 00:10:53.440
It sat on the wall, right? Obviously, that's what was likely to come next when you see those


00:10:53.440 --> 00:10:58.880
that set of words. But there's an example that I like to play with, which I copied. I'll give it a


00:10:58.880 --> 00:11:04.320
little thing. I'll say, hey, here's a program, Python program. I'm going to ask you questions


00:11:04.320 --> 00:11:09.520
about it. Let's call it Arrow. And it's like this is a highly nested program that function that


00:11:09.520 --> 00:11:13.680
tests whether something's a platypus. I saw this example somewhere and I thought, okay, this is


00:11:13.680 --> 00:11:17.160
is pretty cool, but it has this, if it's a mammal,


00:11:17.160 --> 00:11:19.800
then if it has fur, then if it has a beak,


00:11:19.800 --> 00:11:22.400
then if it has a tail, and you can just do stuff


00:11:22.400 --> 00:11:25.520
that's really interesting, like I'll ask it to--


00:11:25.520 --> 00:11:27.920
- Is bird or something like that, or is like--


00:11:27.920 --> 00:11:29.480
- Yeah, yeah, yeah, rewrite it,


00:11:29.480 --> 00:11:34.480
write it using guarding clauses to be not nested, right?


00:11:34.480 --> 00:11:36.400
- Oh, yeah.


00:11:36.400 --> 00:11:39.040
- Right, and it'll say, sure, here we go,


00:11:39.040 --> 00:11:41.520
and instead of being if this, then nest if this,


00:11:41.520 --> 00:11:44.080
then if this, it'll do, if not, if not return false, right?


00:11:44.080 --> 00:11:45.640
Which is really cool.


00:11:45.640 --> 00:11:47.680
And that's kind of a pretty interesting one.


00:11:47.680 --> 00:11:50.000
But like, this is the one,


00:11:50.000 --> 00:11:51.880
this is the example that I think is crazy.


00:11:51.880 --> 00:11:56.800
It's rewrite arrow to test for crocodiles.


00:11:56.800 --> 00:12:00.240
- Using that, it's like what people would call a one shot,


00:12:00.240 --> 00:12:01.760
a few shot example of like,


00:12:01.760 --> 00:12:05.520
hey, here's an example of the kind of stuff I might want.


00:12:05.520 --> 00:12:07.800
There's some different ways to do that.


00:12:07.800 --> 00:12:09.360
But it's a pattern in prop engineering


00:12:09.360 --> 00:12:11.680
where you'll say you have a zero shot, one shot,


00:12:11.680 --> 00:12:13.800
a few shot examples we get into.


00:12:13.800 --> 00:12:16.880
But it does feel like it understands the code, right?


00:12:16.880 --> 00:12:18.640
Like what you're trying to do.


00:12:18.640 --> 00:12:19.960
- Right, just for people listening,


00:12:19.960 --> 00:12:22.400
it said, okay, here's a function isCrocodile.


00:12:22.400 --> 00:12:25.840
If not self.isReptile, if not self.hasScales,


00:12:25.840 --> 00:12:27.040
these are false examples, right?


00:12:27.040 --> 00:12:28.960
And if it has four legs and a long snout,


00:12:28.960 --> 00:12:29.860
it can swim, right?


00:12:29.860 --> 00:12:32.880
Like it rewrote the little tests and stuff, right?


00:12:32.880 --> 00:12:35.600
In a way, that seems really unlikely


00:12:35.600 --> 00:12:38.360
that it's just predicting likelihood


00:12:38.360 --> 00:12:40.160
'cause it's never seen anything like this really,


00:12:40.160 --> 00:12:42.880
which is really, it's pretty mind blowing I think.


00:12:42.880 --> 00:12:45.680
- Or it had, like it read the entire internet


00:12:45.680 --> 00:12:47.680
and all of GitHub and like had steps to say


00:12:47.680 --> 00:12:49.760
it has seen some of the things.


00:12:49.760 --> 00:12:51.400
I think that's mind boggling, it's just like,


00:12:51.400 --> 00:12:53.880
like when you think about what it did there


00:12:53.880 --> 00:12:57.840
is it read the entire conversation so far,


00:12:57.840 --> 00:13:00.840
your input prompt, and it has like a system prompt


00:13:00.840 --> 00:13:02.120
ahead of time that says, you know,


00:13:02.120 --> 00:13:05.320
your ChatGPT, try to be helpful to people


00:13:05.320 --> 00:13:08.160
and here's a bunch of things you should or should not say,


00:13:08.160 --> 00:13:12.460
and unbiased, you try to be concise and good and virtuous.


00:13:12.460 --> 00:13:15.860
And if you want to find all sorts of jailbreaks out of that, but I can, all


00:13:15.860 --> 00:13:20.660
it does from that point on is I try to predict the next word, which is kind


00:13:20.660 --> 00:13:25.020
of insane that it gets to, you know, the amount of structure that we see.


00:13:25.020 --> 00:13:25.540
Right, right.


00:13:25.540 --> 00:13:27.100
That's a lot of structure there.


00:13:27.100 --> 00:13:27.340
Right.


00:13:27.340 --> 00:13:28.980
And so pretty impressive.


00:13:28.980 --> 00:13:31.020
And ChatGPT is starting to grow.


00:13:31.020 --> 00:13:33.560
You know, if you've got version four and you can start using some of the plugins,


00:13:33.560 --> 00:13:34.900
it's going to keep going crazy there.


00:13:34.900 --> 00:13:38.260
Other examples are simply a just released lemur,


00:13:38.260 --> 00:13:40.100
which is a large language model,


00:13:40.100 --> 00:13:42.940
but really focused on transcribing speech,


00:13:42.940 --> 00:13:44.460
which I think is kind of cool.


00:13:44.460 --> 00:13:47.260
Microsoft reduced Microsoft security,


00:13:47.260 --> 00:13:49.540
released Microsoft security copilot,


00:13:49.540 --> 00:13:52.140
which is a large language model to talk about


00:13:52.140 --> 00:13:55.780
things like Nginx misconfiguration and stuff like that.


00:13:55.780 --> 00:14:00.620
There's just a lot of stuff out there that's coming along here.


00:14:00.620 --> 00:14:03.020
A lot of thousands of hours of learning type of thing.


00:14:03.020 --> 00:14:06.860
On the open source front too, there's this whole ethical thing,


00:14:06.860 --> 00:14:11.980
like should everyone and anyone have access to open source models doing that?


00:14:11.980 --> 00:14:13.420
Well, we don't really understand.


00:14:13.420 --> 00:14:17.980
And we probably shouldn't get into the ethics part of the debate here


00:14:17.980 --> 00:14:21.820
because that's a whole series of episodes we probably won't want to get into.


00:14:21.820 --> 00:14:25.500
But what's interesting is Databricks came up with a model for what's called


00:14:25.500 --> 00:14:30.460
phasebook, came up with one called LAM, or the open source, or the weights.


00:14:30.460 --> 00:14:34.700
So you have the model topology with pre-trained weights.


00:14:34.700 --> 00:14:37.820
In some cases, there's open source corpus of training that are also


00:14:37.820 --> 00:14:41.140
coming out and are also open source.


00:14:41.140 --> 00:14:44.180
And I mean, it's like, and these these open source models


00:14:44.180 --> 00:14:47.260
are somewhat competitive or increasingly competitive


00:14:47.260 --> 00:14:49.780
with GPT-4.


00:14:49.780 --> 00:14:52.940
Yeah, which is kind of crazy.


00:14:52.940 --> 00:14:57.060
And some of them don't are where GPT-4 has limitations.


00:14:57.060 --> 00:14:58.540
They break through these limitations.


00:14:58.540 --> 00:15:09.100
So one thing that's really important as a current limitation of the GPT models and LLMs is the prompt window, the token prompt window.


00:15:09.100 --> 00:15:19.260
So basically when you ask a question, it's been trained and has machine learn with data up to, I think in the case of GPT-3.5 or 4,


00:15:19.260 --> 00:15:23.980
it's the corpus of training goes all the way to fall 2021.


00:15:23.980 --> 00:15:26.180
So if you ask, "Who is the current president of the United States?"


00:15:26.180 --> 00:15:30.980
It just doesn't know, or it will tell you, "As of 2021, it is this person."


00:15:30.980 --> 00:15:33.980
But if you're trying to do tasks,


00:15:33.980 --> 00:15:37.180
like what I've been working on, we'll probably get into later in the conversation,


00:15:37.180 --> 00:15:40.480
is trying to generate SQL, it doesn't know your table.


00:15:40.480 --> 00:15:42.980
So you have to say, "Hey, here's all the tables in my database.


00:15:42.980 --> 00:15:45.780
Now can you generate SQL that does X on top of it?"


00:15:45.780 --> 00:15:51.580
And that context window is limited and increasing.


00:15:51.580 --> 00:15:54.180
But some of these open source models have different types of limitations.


00:15:54.180 --> 00:16:00.780
They might have like 2x, 4x, 5x, 10x, you know, 10x limitation that GPT-4 might have.


00:16:00.780 --> 00:16:06.280
This portion of Talk Python to Me is brought to you by JetBrains,


00:16:06.280 --> 00:16:09.380
who encourage you to get work done with PyCharm.


00:16:09.380 --> 00:16:14.580
PyCharm Professional is the complete IDE that supports all major Python workflows,


00:16:14.580 --> 00:16:16.580
including full stack development.


00:16:16.580 --> 00:16:20.080
That's front-end JavaScript, Python backend and data support,


00:16:20.080 --> 00:16:23.280
as well as data science workflows with Jupyter.


00:16:23.280 --> 00:16:25.640
PyCharm just works out of the box.


00:16:25.640 --> 00:16:29.720
Some editors provide their functionality through piecemeal add-ins


00:16:29.720 --> 00:16:32.600
that you put together from a variety of sources.


00:16:32.600 --> 00:16:35.360
PyCharm is ready to go from minute one.


00:16:35.360 --> 00:16:38.120
And PyCharm thrives on complexity.


00:16:38.120 --> 00:16:41.120
The biggest selling point for me personally is that PyCharm


00:16:41.120 --> 00:16:44.440
understands the code structure of my entire project,


00:16:44.440 --> 00:16:48.960
even across languages such as Python and SQL and HTML.


00:16:48.960 --> 00:16:53.480
If you see your editor completing statements just because the word appears elsewhere in


00:16:53.480 --> 00:16:58.000
the file, but it's not actually relevant to that code block, that should make you really


00:16:58.000 --> 00:16:59.000
nervous.


00:16:59.000 --> 00:17:02.720
I've been a happy paying customer of PyCharm for years.


00:17:02.720 --> 00:17:07.600
Hardly a workday passes that I'm not deep inside PyCharm working on projects here at


00:17:07.600 --> 00:17:09.240
Talk Python.


00:17:09.240 --> 00:17:13.100
What tool is more important to your productivity than your code editor?


00:17:13.100 --> 00:17:15.480
You deserve one that works the best.


00:17:15.480 --> 00:17:22.360
So download your free trial of PyCharm professional today at talkpython.fm/donewithpycharm and


00:17:22.360 --> 00:17:23.360
get work done.


00:17:23.360 --> 00:17:26.340
That link is in your podcast player show notes.


00:17:26.340 --> 00:17:33.000
Thank you to PyCharm from JetBrains for sponsoring the show and keeping Talk Python going strong.


00:17:33.000 --> 00:17:39.240
It's interesting to ask questions, but it's more interesting from a software developer


00:17:39.240 --> 00:17:44.860
perspective of, can I teach it a little bit more about what my app needs to know or what


00:17:44.860 --> 00:17:46.900
what my app structure is, right?


00:17:46.900 --> 00:17:50.520
In your case, I want to use Superset


00:17:50.520 --> 00:17:52.620
to ask the database questions.


00:17:52.620 --> 00:17:54.900
But if I'm gonna bring in AI,


00:17:54.900 --> 00:17:57.620
it needs to understand the database structure


00:17:57.620 --> 00:18:00.900
so that when I say, help me do a query to do this thing,


00:18:00.900 --> 00:18:03.140
it needs to know what the heck to do, right?


00:18:03.140 --> 00:18:04.580
- The table, it needs to know,


00:18:04.580 --> 00:18:08.100
so there's just stuff it knows and the stuff it can't know.


00:18:08.100 --> 00:18:10.540
And some of it goes, is related to the fact


00:18:10.540 --> 00:18:13.420
that whether this information is public on the internet,


00:18:13.420 --> 00:18:15.980
whether it has happened to be trained against it.


00:18:15.980 --> 00:18:18.340
And then if it's a private, there's just no hope


00:18:18.340 --> 00:18:21.540
that it would know about, you know, your own internal documents


00:18:21.540 --> 00:18:23.540
or your database structures. In our case,


00:18:23.540 --> 00:18:26.220
it speaks SQL very, very well.


00:18:26.220 --> 00:18:28.620
So as we get into this example, like how to get


00:18:28.620 --> 00:18:31.540
GPT to generate good SQL


00:18:31.540 --> 00:18:33.380
in the context of a tool like SuperSep


00:18:33.380 --> 00:18:35.620
or SQL Lab, which is our SQL IDE.


00:18:35.620 --> 00:18:38.100
So it knows how to speak SQL super well.


00:18:38.100 --> 00:18:41.220
It knows the different dialects of SQL very, very well.


00:18:41.220 --> 00:18:43.700
It knows its functions, its dates functions,


00:18:43.700 --> 00:18:45.780
which a lot of the SQL,


00:18:45.780 --> 00:18:47.380
and like the engineers only call it like,


00:18:47.380 --> 00:18:48.820
yeah, I can never remember


00:18:48.820 --> 00:18:51.540
like what Postgres date this function is.


00:18:51.540 --> 00:18:54.180
But GPT or GPT models just,


00:18:54.180 --> 00:18:56.100
it knows SQL and knows the dialects


00:18:56.100 --> 00:18:57.620
and knows the mechanics of SQL.


00:18:57.620 --> 00:18:59.860
It understands data modeling, foreign keys,


00:18:59.860 --> 00:19:02.580
drawings, primary, all this stuff it understands.


00:19:02.580 --> 00:19:05.700
It knows nothing about your specific database,


00:19:05.700 --> 00:19:09.140
the schema names and the table names,


00:19:09.140 --> 00:19:10.980
the column names that they might be able to use.


00:19:10.980 --> 00:19:13.980
that's where we need to start providing some context.


00:19:13.980 --> 00:19:15.540
And this context window is limited.


00:19:15.540 --> 00:19:19.780
So it's like, how do you use that context well,


00:19:19.780 --> 00:19:21.900
or as well as possible?


00:19:21.900 --> 00:19:24.980
And that's the field and some of the ideas behind those


00:19:24.980 --> 00:19:26.780
prompt crafting and prompt engineering,


00:19:26.780 --> 00:19:30.100
which we can get into once we get there,


00:19:30.100 --> 00:19:31.500
maybe we're there already.


00:19:31.500 --> 00:19:35.580
- Yeah, yeah, well, yeah, I think where I see this stuff going


00:19:35.580 --> 00:19:38.340
is from this general purpose knowledge,


00:19:38.340 --> 00:19:44.260
starting to bring in more private or personal or internal type of information, right?


00:19:44.260 --> 00:19:48.500
Like our data about our customers is like structured like this in a table.


00:19:48.500 --> 00:19:49.620
And here's what we know about them.


00:19:49.620 --> 00:19:53.460
Now let us ask questions about our company and our thing, right?


00:19:53.460 --> 00:19:56.580
And it's like starting to make inroads in that direction, I think.


00:19:56.580 --> 00:19:58.740
Yeah, what, you know, one thing to know about is,


00:19:58.740 --> 00:20:04.740
there's different approaches to teach or provide that context.


00:20:04.740 --> 00:20:08.980
So one would be to build your own model from scratch, right?


00:20:08.980 --> 00:20:10.900
And that's pretty prohibitive.


00:20:10.900 --> 00:20:12.980
So you'd have to find the right corpus.


00:20:12.980 --> 00:20:15.580
And instead of starting with a model that knows SQL


00:20:15.580 --> 00:20:17.540
and needs to know your table and context,


00:20:17.540 --> 00:20:20.980
you have to start from zero and very prohibitive.


00:20:20.980 --> 00:20:22.980
Another one is you start from a base model


00:20:22.980 --> 00:20:24.860
at some point of some kind.


00:20:24.860 --> 00:20:27.940
There's a topology, so there's different layers


00:20:27.940 --> 00:20:30.020
and number of neurons and it knows some things.


00:20:30.020 --> 00:20:32.060
And then you load up some weights that are open source.


00:20:32.060 --> 00:20:37.740
and then you say, I'm going to tune this model to teach it my database schema


00:20:37.740 --> 00:20:39.620
as in basically my own corpus of data.


00:20:39.620 --> 00:20:43.220
So it could be your data dictionaries, could be your internal documents,


00:20:43.220 --> 00:20:46.700
it could be your GitHub code, your dbt projects.


00:20:46.700 --> 00:20:49.980
If you have one of your Airflow DAGs, be like, I'm going to dump all this stuff


00:20:49.980 --> 00:20:54.180
in the model that will get baked into their own network itself.


00:20:54.180 --> 00:20:57.660
That's doable pretty primitive in this era.


00:20:57.660 --> 00:21:00.500
If you have the challenge that we have at Preset,


00:21:00.500 --> 00:21:03.100
which is we have multiple customers with different schemas,


00:21:03.100 --> 00:21:04.500
we can't have spillover.


00:21:04.500 --> 00:21:07.600
So you have to train a model for each one of our customers


00:21:07.600 --> 00:21:09.800
and serve a model for each one of our customers.


00:21:09.800 --> 00:21:11.800
So it's still pretty prohibitive.


00:21:11.800 --> 00:21:14.500
And a lot of people fall back on this third or fourth method


00:21:14.500 --> 00:21:16.600
that I would call prompt engineering,


00:21:16.600 --> 00:21:19.300
which is I'm going to use the base model,


00:21:19.300 --> 00:21:23.600
the OpenAI API, or just an API on LLM.


00:21:23.600 --> 00:21:26.400
And then I will, if no SQL already, I'll just say,


00:21:26.400 --> 00:21:28.700
hey, here's a bunch of tables that you might want to use.


00:21:28.700 --> 00:21:30.460
can you generate SQL on top of it?


00:21:30.460 --> 00:21:34.660
So then that's just a big request with a lot of context.


00:21:34.660 --> 00:21:37.820
Then we have to start thinking about maximizing


00:21:37.820 --> 00:21:40.300
the use of that context window to pass the information


00:21:40.300 --> 00:21:43.740
that's most relevant within the limits allowed


00:21:43.740 --> 00:21:45.100
by the specific model.


00:21:45.100 --> 00:21:48.660
- Right, and that starts to get into reproducibility,


00:21:48.660 --> 00:21:52.420
accuracy, and just those limitations,


00:21:52.420 --> 00:21:55.040
which is kind of an engineering type of thing, right?


00:21:55.040 --> 00:21:55.880
- Yeah.


00:21:55.880 --> 00:21:57.540
- And then, you know, maybe a topic too,


00:21:57.540 --> 00:22:00.660
And this conversation is based on that recent blog post


00:22:00.660 --> 00:22:03.580
and the flow, just going back to the flow of that blog post,


00:22:03.580 --> 00:22:08.820
we started by establishing the premise that everyone is trying to bring AI


00:22:08.820 --> 00:22:10.220
into their product today.


00:22:10.220 --> 00:22:12.660
Thousands of product builders are currently exploring ways


00:22:12.660 --> 00:22:17.420
to harness the power of AI in the products and experiences they create.


00:22:17.420 --> 00:22:20.380
That's the premise for us with Text-to-SQL and SQL Lab


00:22:20.380 --> 00:22:22.180
as part of the SuperSet preset.


00:22:22.180 --> 00:22:26.340
But I don't know, if you think of any product, any startup,


00:22:26.340 --> 00:22:27.860
any SaaS product you use.


00:22:27.860 --> 00:22:29.180
If you work at HubSpot today,


00:22:29.180 --> 00:22:31.860
you're trying to figure out how to leverage AI


00:22:31.860 --> 00:22:36.780
to build, you know, sales chatbots or SDR chatbots.


00:22:36.780 --> 00:22:39.180
So everyone everywhere is trying to figure that out.


00:22:39.180 --> 00:22:42.740
The challenge is, I guess, very probabilistic


00:22:42.740 --> 00:22:45.180
in a different interface to anything we know.


00:22:45.180 --> 00:22:46.780
Like, you know, engineers would be like,


00:22:46.780 --> 00:22:49.980
"Oh, let's look at an API and leverage it."


00:22:49.980 --> 00:22:53.540
And APIs are very, very deterministic in general.


00:22:53.540 --> 00:22:57.740
AI is kind of a wild beast to tame.


00:22:57.740 --> 00:22:59.840
You ask, first the interface is


00:22:59.840 --> 00:23:01.500
language not code,


00:23:01.500 --> 00:23:03.600
and then what comes back is like


00:23:03.600 --> 00:23:05.940
semi-probabilistic in nature.


00:23:05.940 --> 00:23:07.440
And it could change underneath you.


00:23:07.440 --> 00:23:09.700
It's a little bit like web scraping in that regard.


00:23:09.700 --> 00:23:11.700
That like, it does the same, it does the same,


00:23:11.700 --> 00:23:13.500
and then, you know, something out there


00:23:13.500 --> 00:23:14.900
changed, not your code.


00:23:14.900 --> 00:23:16.900
And then a potentially different


00:23:16.900 --> 00:23:18.700
behavior comes back, right?


00:23:18.700 --> 00:23:19.800
Because they may have trained


00:23:19.800 --> 00:23:21.540
another couple of years, refined the model,


00:23:21.540 --> 00:23:24.700
switch the model, change the default temperature, all these things.


00:23:24.700 --> 00:23:26.940
Yeah, there's a lot that can happen there.


00:23:26.940 --> 00:23:31.020
One thing I noticed, like starting to work with what I would call prompt crafting,


00:23:31.020 --> 00:23:35.860
which is, you know, you work with GPT and you craft different prompt


00:23:35.860 --> 00:23:39.380
with putting emphasis in a place or another,


00:23:39.380 --> 00:23:42.340
or changing the order of things, or just changing a word, right?


00:23:42.340 --> 00:23:45.460
Just say like, important exclamation point,


00:23:45.460 --> 00:23:49.580
capitalize the words, you know, the reserved words in SQL,


00:23:49.580 --> 00:23:52.380
and then just the fact that you put import exclamation point


00:23:52.380 --> 00:23:54.860
will make it do it or not do it,


00:23:54.860 --> 00:23:56.980
changing from a model to another.


00:23:56.980 --> 00:23:58.980
So one thing that's great is the model,


00:23:58.980 --> 00:24:00.980
at least at OpenAI,


00:24:00.980 --> 00:24:04.060
they are immutable, as far as I know.


00:24:04.060 --> 00:24:08.100
But if you use GPT-3.5 Turbo, for instance,


00:24:08.100 --> 00:24:09.540
that's just one train model,


00:24:09.540 --> 00:24:13.060
I believe that that is immutable.


00:24:13.060 --> 00:24:15.060
The chatbot on top of it might


00:24:15.060 --> 00:24:17.060
get fine-tuned and change over time,


00:24:17.060 --> 00:24:19.940
but the model is supposed to be static.


00:24:19.940 --> 00:24:21.140
You mentioned temperature,


00:24:21.140 --> 00:24:22.980
it'd be kind of interesting to just mention


00:24:22.980 --> 00:24:24.660
for those who are not familiar with that.


00:24:24.660 --> 00:24:26.260
So when you interact with AI,


00:24:26.260 --> 00:24:28.980
one of the core parameters is temperature,


00:24:28.980 --> 00:24:32.500
and I think it's a value from zero to one,


00:24:32.500 --> 00:24:36.580
or I'm not sure how exactly you pass it,


00:24:36.580 --> 00:24:42.980
but it basically defines how creative you want to let the AI be.


00:24:42.980 --> 00:24:45.220
Like if you put it to zero,


00:24:45.220 --> 00:24:47.020
you're going to have something more deterministic.


00:24:47.020 --> 00:24:51.380
So asking the same question should lead to a similar or the same answer.


00:24:51.380 --> 00:24:52.940
Though not in my experience,


00:24:52.940 --> 00:24:54.820
it feels like it should, but it doesn't.


00:24:54.820 --> 00:24:56.100
But then if you put a higher,


00:24:56.100 --> 00:24:57.540
it will get more creative.


00:24:57.540 --> 00:25:02.260
Talk more about how that actually seem to work behind the scene.


00:25:02.260 --> 00:25:04.780
>> Yeah. Well, that variability


00:25:04.780 --> 00:25:07.540
seems to show up more in the image-based ones.


00:25:07.540 --> 00:25:09.420
So for example, this article,


00:25:09.420 --> 00:25:10.620
this blog post that you wrote,


00:25:10.620 --> 00:25:12.100
you have this image here and you said,


00:25:12.100 --> 00:25:14.900
"Oh, and I made this image from mid-journey.


00:25:14.900 --> 00:25:18.700
I've also got some examples of a couple that I did.


00:25:18.700 --> 00:25:19.900
Where did I stick them?


00:25:19.900 --> 00:25:21.020
Somewhere. Here we go.


00:25:21.020 --> 00:25:23.660
Where I asked just for YouTube thumbnails,


00:25:23.660 --> 00:25:27.420
I asked Midjourney for a radio astronomy example that I can use.


00:25:27.420 --> 00:25:31.460
Because here's one that's not encumbered by some licensing,


00:25:31.460 --> 00:25:34.100
but still looks cool and is representative.


00:25:34.100 --> 00:25:37.580
There it's like massive difference.


00:25:37.580 --> 00:25:40.460
I'm not sure how much difference I've seen.


00:25:40.460 --> 00:25:44.700
I know it will make some, but I haven't seen as dramatic of a difference on chat.


00:25:44.700 --> 00:25:47.100
- Well, on TNT, yeah. - Yeah.


00:25:47.100 --> 00:25:50.500
I'm not sure exactly how they introduce the variability


00:25:50.500 --> 00:25:53.940
on the generative images AI.


00:25:53.940 --> 00:25:56.460
I know it's like this multi-dimensional space


00:25:56.460 --> 00:25:59.380
with a lot of words and a lot of images in there.


00:25:59.380 --> 00:26:03.580
And then it's probably like where the location point of that,


00:26:03.580 --> 00:26:08.860
they randomize that point in that multi-dimensional space.


00:26:08.860 --> 00:26:11.020
For Chadgpd, it's pretty easy to reason about,


00:26:11.020 --> 00:26:13.100
and I might be wrong on this, again, I'm not an expert,


00:26:13.100 --> 00:26:18.340
But you know how the way it works is it writes, it takes the prompt and then it


00:26:18.340 --> 00:26:20.720
comes up with the next word sequentially.


00:26:20.720 --> 00:26:26.960
So for each word for the next word, so Humpty Dumpty sat on a, it might be wall


00:26:26.960 --> 00:26:32.400
at 99%, but like there might be 1% of, you know, a sense or something like that.


00:26:32.400 --> 00:26:38.580
And if you, you up the temperature, there's, it's more likely to pick


00:26:38.780 --> 00:26:42.620
the non-first word in that probability list,


00:26:42.620 --> 00:26:44.220
they probably do in a weighted way.


00:26:44.220 --> 00:26:47.980
Like it's possible that I take a second or third word randomly,


00:26:47.980 --> 00:26:50.420
and then of course, it's going to get a tree or decision tree.


00:26:50.420 --> 00:26:53.420
Once it picks a word, the next word is also changes.


00:26:53.420 --> 00:26:55.220
So as you up that,


00:26:55.220 --> 00:26:59.860
it goes down path that sends it into more creative or different.


00:26:59.860 --> 00:27:02.540
>> Right. Yeah, a little butterfly effect.


00:27:02.540 --> 00:27:07.060
It makes a different choice here and then it sends it down through the graph.


00:27:07.060 --> 00:27:29.500
Interesting. So one thing that you really pointed out here, and I think is maybe worth touching on a bit is this idea of prompt engineering. There's even places like learn prompting.org that like try to teach you how to talk to these things. And and you make a strong distinction between prompt crafting or just talking to the AI versus like really trying to put an engineering focus on it. You want to


00:27:29.500 --> 00:27:36.060
Yeah, I think it's a super important differentiation, but one that I'm proposing, right?


00:27:36.060 --> 00:27:40.860
So I don't think that people have settled as to what is one or what is the other.


00:27:40.860 --> 00:27:43.420
I think I saw a Reddit post recently that was like,


00:27:43.420 --> 00:27:45.660
"Prompt engineering is just a load of crap."


00:27:45.660 --> 00:27:51.820
Like, you know, anyone can go because they thought their understanding of prompt engineering was like,


00:27:51.820 --> 00:27:55.900
"Oh, you know, you fine tune or you craft your prompt and you say like,


00:27:55.900 --> 00:28:00.580
You are an expert AI working on, you know, creating molecules.


00:28:00.580 --> 00:28:01.900
Now, can you do this?


00:28:01.900 --> 00:28:05.300
And then, you know, by doing that, you might get a better outcome.


00:28:05.300 --> 00:28:09.220
Or one really interesting thing that people have been doing in trunk crafting


00:28:09.220 --> 00:28:14.700
that seemed to have like huge impact on there's been paper written on this specific


00:28:14.700 --> 00:28:21.260
just hint or craft tweak is let's proceed step by step.


00:28:21.260 --> 00:28:24.780
So basically, whatever the question you're asking,


00:28:24.780 --> 00:28:29.740
specifically around like more mathematicals or like things that require more systematic


00:28:29.740 --> 00:28:35.180
step-by-step thinking, the whole like just like let's think, let's expose this or let's go about


00:28:35.180 --> 00:28:42.700
it step by step makes it much better. So here you might be able to, well, so you know, if you had,


00:28:42.700 --> 00:28:48.940
if you had an example where ChatGPT-3 failed or ChatGPT-4 failed, you could just say,


00:28:48.940 --> 00:28:52.860
Colin, let's go step by step and it might succeed that time around.


00:28:52.860 --> 00:28:56.660
- Maybe you can get it to help you understand


00:28:56.660 --> 00:28:58.540
instead of just get the answer, right?


00:28:58.540 --> 00:29:03.340
Like factor this polynomial into its primary,


00:29:03.340 --> 00:29:04.900
you know, solutions or roots or whatever.


00:29:04.900 --> 00:29:06.060
And you're like, okay, show me,


00:29:06.060 --> 00:29:07.020
don't just show me the answer,


00:29:07.020 --> 00:29:09.460
show me step by step so I could understand


00:29:09.460 --> 00:29:11.620
and try to learn from what you've done, right?


00:29:11.620 --> 00:29:13.140
- Yeah, I mean, if you think about how,


00:29:13.140 --> 00:29:15.660
the way that it's trying to come up with a new word,


00:29:15.660 --> 00:29:18.660
if all it does is a language-based answer


00:29:18.660 --> 00:29:20.240
to a mathematical question,


00:29:20.240 --> 00:29:24.040
Like how many days are there between this date and that date?


00:29:24.040 --> 00:29:28.240
There's no, that specific example might not exist


00:29:28.240 --> 00:29:30.240
or it's kind of complicated for it to go about it.


00:29:30.240 --> 00:29:32.160
But if you say, let's think step by steps,


00:29:32.160 --> 00:29:34.240
okay, there's this many months,


00:29:34.240 --> 00:29:36.680
this month's duration is this long,


00:29:36.680 --> 00:29:39.600
there's this many days since the beginning of that month,


00:29:39.600 --> 00:29:42.200
it might get it right that, you know, that time around.


00:29:42.200 --> 00:29:45.040
- Right, or if it fails, you could pick up part way along


00:29:45.040 --> 00:29:46.600
where it has some more.


00:29:46.600 --> 00:29:48.060
- Yeah, you know, and then you could trace,


00:29:48.060 --> 00:29:50.040
I mean, just you too, like, I think, you know,


00:29:50.040 --> 00:29:52.040
one thing is like you should be extremely careful


00:29:52.040 --> 00:29:54.640
as like taking for granted that it's right all the time,


00:29:54.640 --> 00:29:57.440
you know, so that means like it also helps you


00:29:57.440 --> 00:30:00.520
review its process and where it might be wrong.


00:30:00.520 --> 00:30:02.680
But back to crafting versus engineering.


00:30:02.680 --> 00:30:05.200
So crafting would be the process that I think


00:30:05.200 --> 00:30:09.360
is more attached to a use ChatGPT every day,


00:30:09.360 --> 00:30:12.400
the same way that, you know, we've been trained at Googling,


00:30:12.400 --> 00:30:14.880
you know, over the past like two decades,


00:30:14.880 --> 00:30:18.040
you know, use quotes, use plus and minus,


00:30:18.040 --> 00:30:21.560
and you know which keywords to use intuitively,


00:30:21.560 --> 00:30:23.440
you know where it's going to get confused or not.


00:30:23.440 --> 00:30:26.600
So I think prompt crafting is a different version of that


00:30:26.600 --> 00:30:28.800
that's just more worthy.


00:30:28.800 --> 00:30:32.120
And if you're working with the AI to try to assist you,


00:30:32.120 --> 00:30:34.200
write your blog post or to try to assist you


00:30:34.200 --> 00:30:35.640
in any task really,


00:30:35.640 --> 00:30:39.000
just to be smart about how you bring the context,


00:30:39.000 --> 00:30:40.720
how you tell it to proceed,


00:30:40.720 --> 00:30:42.200
goes a very, very long way.


00:30:42.200 --> 00:30:44.240
So that's what I call prompt crafting,


00:30:44.240 --> 00:30:46.240
call it like one-off cases.


00:30:47.320 --> 00:30:50.960
what people do when they're interacting with, with the large language model.


00:30:50.960 --> 00:30:51.700
I think so.


00:30:51.700 --> 00:30:51.880
Right.


00:30:51.880 --> 00:30:56.080
Like it's not evident for a lot of people will are exploring the edge of where it


00:30:56.080 --> 00:30:58.760
fails and they love to see it fail.


00:30:58.760 --> 00:31:03.080
And, and then they, they don't think about like, Oh, what could I have told it to


00:31:03.080 --> 00:31:04.840
get the answer I was actually looking for?


00:31:04.840 --> 00:31:06.140
Like, I got you wrong.


00:31:06.140 --> 00:31:09.080
You know, it's as if I had that actor in a conversation.


00:31:09.080 --> 00:31:10.200
I like, ah, you're wrong.


00:31:10.200 --> 00:31:14.320
And I told you so, you know, so I think there's a lot of that online, but I think


00:31:14.440 --> 00:31:16.200
for all these examples that I've seen,


00:31:16.200 --> 00:31:18.560
I'm really tempted to take the prompt that they had


00:31:18.560 --> 00:31:21.200
and then give it an instruction or two or more


00:31:21.200 --> 00:31:23.000
and then figure out how to get it


00:31:23.000 --> 00:31:23.840
to come up with the right thing.


00:31:23.840 --> 00:31:26.920
So prompt crafting, super important skill.


00:31:26.920 --> 00:31:28.800
You know, you could probably get a boost of,


00:31:28.800 --> 00:31:30.640
for most knowledge information workers,


00:31:30.640 --> 00:31:33.320
you'll get a boost of 50% to 10X


00:31:33.320 --> 00:31:34.840
for a lot of the tasks you do every day


00:31:34.840 --> 00:31:36.120
if you use AI well.


00:31:36.120 --> 00:31:38.360
So it's great personal skill to have.


00:31:38.360 --> 00:31:41.400
- This portion of the Talk Python


00:31:41.400 --> 00:31:44.120
is sponsored by the compiler podcast from Red Hat.


00:31:44.120 --> 00:31:48.320
Just like you, I'm a big fan of podcasts, and I'm happy to share a new one from a highly


00:31:48.320 --> 00:31:53.040
respected open source company, Compiler, an original podcast from Red Hat.


00:31:53.040 --> 00:31:57.040
Do you want to stay on top of tech without dedicating tons of time to it?


00:31:57.040 --> 00:32:01.120
Compiler presents perspectives, topics, and insights from the tech industry, free from


00:32:01.120 --> 00:32:02.720
jargon and judgment.


00:32:02.720 --> 00:32:06.000
They want to discover where technology is headed beyond the headlines and create a place


00:32:06.000 --> 00:32:09.720
for new IT professionals to learn, grow, and thrive.


00:32:09.720 --> 00:32:13.440
Compiler helps people break through the barriers and challenges turning code into community


00:32:13.440 --> 00:32:16.260
at all levels of the enterprise.


00:32:16.260 --> 00:32:19.700
One recent and interesting episode is there, the Great Stack Debate.


00:32:19.700 --> 00:32:23.840
I love love love talking to people about how they architect their code, the tradeoffs and


00:32:23.840 --> 00:32:29.040
conventions they chose, and the costs, challenges, and smiles that result.


00:32:29.040 --> 00:32:31.480
This Great Stack Debate episode is like that.


00:32:31.480 --> 00:32:36.040
Check it out and see if software is more like an onion, or more like lasagna, or maybe even


00:32:36.040 --> 00:32:37.680
more complicated than that.


00:32:37.680 --> 00:32:41.600
It's the first episode in Compiler's series on software stacks.


00:32:41.600 --> 00:32:47.360
more about compiler at talkpython.fm/compiler. The link is in your podcast player show notes.


00:32:47.360 --> 00:32:53.040
And yes, you could just go search for compiler and subscribe to it, but follow that link and


00:32:53.040 --> 00:32:58.000
click on your player's icon to add it. That way they know you came from us. Our thanks to the


00:32:58.000 --> 00:33:01.600
compiler podcast for keeping this podcast going strong.


00:33:01.600 --> 00:33:09.200
Prop engineering, in my case, I'm like, you're building something, you're using an AI as an


00:33:09.200 --> 00:33:11.360
as an API behind the scene,


00:33:11.360 --> 00:33:15.680
you wanna pass it a bunch of relevant contexts,


00:33:15.680 --> 00:33:17.600
really specify what you wanna get out of it.


00:33:17.600 --> 00:33:20.120
Maybe you even wanna get a structured output, right?


00:33:20.120 --> 00:33:22.480
You might wanna get a JSON blob out of it.


00:33:22.480 --> 00:33:26.200
And you say, return a JSON blob with the following format,


00:33:26.200 --> 00:33:27.680
so it's more structured.


00:33:27.680 --> 00:33:29.840
So then to give all these instructions,


00:33:29.840 --> 00:33:32.080
is this idea of providing few shots too,


00:33:32.080 --> 00:33:34.760
you might be storing context in a vector database.


00:33:34.760 --> 00:33:36.560
I don't know if we're gonna get to that today,


00:33:36.560 --> 00:33:40.320
But there are ways to kind of structure


00:33:40.320 --> 00:33:42.280
and organize your potential embeddings


00:33:42.280 --> 00:33:43.840
or the things you want to pass as context.


00:33:43.840 --> 00:33:45.320
So there's a lot here.


00:33:45.320 --> 00:33:47.720
I think somewhere too, I talked about prompt engineering.


00:33:47.720 --> 00:33:49.000
If we'd scroll in the blog posts,


00:33:49.000 --> 00:33:51.440
like what is in prompt engineering,


00:33:51.440 --> 00:33:54.460
it will list the kind of things.


00:33:54.460 --> 00:33:56.520
- Yeah, here you go.


00:33:56.520 --> 00:33:57.340
- Oh yeah, right here.


00:33:57.340 --> 00:34:00.280
The definition of this is Chad's GPT's version of it.


00:34:00.280 --> 00:34:03.040
So when you do prompt engineering, you can add context,


00:34:03.040 --> 00:34:05.160
which that means that you're gonna have to retrieve context


00:34:05.160 --> 00:34:08.520
maybe from a database, from a user session,


00:34:08.520 --> 00:34:11.000
from your Redux store if you're in the front end, right?


00:34:11.000 --> 00:34:14.200
So you're gonna go and fetch the context that's relevant


00:34:14.200 --> 00:34:15.640
in context with the application,


00:34:15.640 --> 00:34:17.240
at least while building products.


00:34:17.240 --> 00:34:18.960
Specifying an answer format, you could just say,


00:34:18.960 --> 00:34:21.320
yes, I just want a yes or no, a Boolean,


00:34:21.320 --> 00:34:24.160
I want a JSON blob with not only the answer,


00:34:24.160 --> 00:34:27.520
but your confidence on that answer or something like that.


00:34:27.520 --> 00:34:30.440
Limiting scope, asking for pros and cons,


00:34:30.440 --> 00:34:33.400
incorporating verification or sourcing.


00:34:33.400 --> 00:34:35.900
So that's more, you know, if you iterate on a prompt,


00:34:35.900 --> 00:34:38.580
you're going to be rigorous about is this prompt better


00:34:38.580 --> 00:34:40.200
than the previous prompt I had?


00:34:40.200 --> 00:34:43.820
Like if I pass five rows of sample data


00:34:43.820 --> 00:34:47.620
while doing text to SQL, does it do better than if I,


00:34:47.620 --> 00:34:50.460
or does it do more poorly than if I pass 10 rows


00:34:50.460 --> 00:34:52.700
of sample data or provide a certain amount


00:34:52.700 --> 00:34:54.540
of column level statistics?


00:34:54.540 --> 00:34:56.820
So prompt engineering is not just from crafting.


00:34:56.820 --> 00:35:00.220
It is like bringing maybe the scientific method to it,


00:35:00.220 --> 00:35:02.120
bring some engineering of like


00:35:02.120 --> 00:35:03.880
fetching the right context and


00:35:03.880 --> 00:35:05.520
organizing it well and then


00:35:05.520 --> 00:35:06.560
measuring the outcome.


00:35:06.560 --> 00:35:07.320
Right, exactly.


00:35:07.320 --> 00:35:08.420
Something that comes out, you can


00:35:08.420 --> 00:35:10.420
measure and say this is 10%


00:35:10.420 --> 00:35:12.420
better by my metric than it


00:35:12.420 --> 00:35:14.620
was before with this additional data.


00:35:14.620 --> 00:35:16.120
Right. That's that's a big difference.


00:35:16.120 --> 00:35:17.820
Right. And then there's so many


00:35:17.820 --> 00:35:18.920
things moving, right?


00:35:18.920 --> 00:35:20.880
Like and everything is changing so


00:35:20.880 --> 00:35:22.280
fast in the space.


00:35:22.280 --> 00:35:24.680
You're like, oh, well, GPT5


00:35:24.680 --> 00:35:26.780
is out or GPT4 Turbo is half


00:35:26.780 --> 00:35:28.380
the price and then just came out.


00:35:28.380 --> 00:35:29.720
Now I'm just going to move to that.


00:35:29.720 --> 00:35:32.240
They're like, wait, is that performing better?


00:35:32.240 --> 00:35:34.880
Or, you know, like, what are the trade-offs?


00:35:34.880 --> 00:35:37.680
Or even I'm going to add, I'm going to move this section,


00:35:37.680 --> 00:35:40.040
you know, asking for a certain JSON format


00:35:40.040 --> 00:35:41.720
above this other section.


00:35:41.720 --> 00:35:45.320
I'm going to write important exclamation point do X.


00:35:45.320 --> 00:35:47.480
Does that improve my result?


00:35:47.480 --> 00:35:49.360
Or does that mess it up?


00:35:49.360 --> 00:35:52.120
And which one of my test case, perhaps,


00:35:52.120 --> 00:35:54.120
that succeeded before fails now,


00:35:54.120 --> 00:35:55.800
and which one failed before succeeds now?


00:35:55.800 --> 00:36:01.240
So you can be like, is that a better or worse iteration towards Michael?


00:36:01.240 --> 00:36:02.040
Right, right.


00:36:02.040 --> 00:36:06.840
Kind of training, bringing this unit testing TDD mindset.


00:36:06.840 --> 00:36:07.440
Yes.


00:36:07.440 --> 00:36:10.640
Yeah, so that's what we're getting deeper into the blog post, right?


00:36:10.640 --> 00:36:14.320
So the blog post, I'm talking about bringing this TDD,


00:36:14.320 --> 00:36:19.320
the test-driven development mindset to prompt engineering, right?


00:36:19.320 --> 00:36:22.120
And there's a lot of things that are in common.


00:36:22.120 --> 00:36:25.920
you can take and apply, kind of transfer just over.


00:36:25.920 --> 00:36:27.680
There are some things to that breakdown


00:36:27.680 --> 00:36:28.960
that are fundamentally different


00:36:28.960 --> 00:36:31.320
between testing a prompt or working with AI


00:36:31.320 --> 00:36:33.760
and working with, you know,


00:36:33.760 --> 00:36:37.800
just a bit more deterministic code testing type framework


00:36:37.800 --> 00:36:38.800
to get into that.


00:36:38.800 --> 00:36:40.560
- Yeah, yeah, for sure.


00:36:40.560 --> 00:36:43.560
So you called out a couple of reasons


00:36:43.560 --> 00:36:45.920
of why TDD is important for prompt engineering.


00:36:45.920 --> 00:36:47.440
Maybe we could run through those.


00:36:47.440 --> 00:36:51.440
- Yeah, so, you know, the first thing is


00:36:51.440 --> 00:36:54.280
the AI model is not a deterministic things,


00:36:54.280 --> 00:36:59.280
or you use a modern API or a GraphQL REST API,


00:36:59.280 --> 00:37:02.720
the format of what you ask is extremely clear,


00:37:02.720 --> 00:37:04.460
and then the format of what you get back


00:37:04.460 --> 00:37:06.620
is defined by a schema.


00:37:06.620 --> 00:37:08.100
It's like very deterministic,


00:37:08.100 --> 00:37:10.200
pretty guaranteed that you do the same request,


00:37:10.200 --> 00:37:13.600
so you get the same output-ish, or at least format.


00:37:13.600 --> 00:37:15.360
With AI, that's not the case, right?


00:37:15.360 --> 00:37:20.340
So it's much more unpredictable and probabilistic by nature.


00:37:20.340 --> 00:37:22.260
The second one is handling complexity.


00:37:22.260 --> 00:37:25.180
So AI systems are complex, black boxy,


00:37:25.180 --> 00:37:26.820
kind of unpredictable too.


00:37:26.820 --> 00:37:30.260
So embrace that and assume that you might get


00:37:30.260 --> 00:37:32.300
something really creative out of there


00:37:32.300 --> 00:37:33.620
for better or for worse.


00:37:33.620 --> 00:37:37.900
And then reducing risk, like you're shipping product,


00:37:37.900 --> 00:37:40.980
you know, if you're shipping product, writing product,


00:37:40.980 --> 00:37:44.740
you don't want to see any sort of like bias


00:37:44.740 --> 00:37:47.980
or weird thing like the AI could go crazy.


00:37:47.980 --> 00:37:52.380
And yeah, there are examples of AI is going crazy before like Tay.


00:37:52.380 --> 00:37:54.220
Do you remember Microsoft Tay?


00:37:54.220 --> 00:37:56.660
I don't know that one, but I know of other examples.


00:37:56.660 --> 00:37:57.140
If you want it.


00:37:57.140 --> 00:37:57.420
Yeah.


00:37:57.420 --> 00:38:00.820
I mean, it came out and it was like this, this sort of just, you know, I'm here to


00:38:00.820 --> 00:38:05.100
learn from you internet and people just turned it into a racist and made it do all


00:38:05.100 --> 00:38:06.140
sorts of horrible things.


00:38:06.140 --> 00:38:09.500
And they had to shut it down a couple of days later because it just, it's like,


00:38:09.500 --> 00:38:12.460
Whoa, it met the internet and the internet is mean.


00:38:12.460 --> 00:38:14.100
So, that's not great.


00:38:14.340 --> 00:38:16.900
- Yeah, train it on 4chan or let it,


00:38:16.900 --> 00:38:19.940
you know, go crawl 4chan and read it.


00:38:19.940 --> 00:38:21.140
It's not always gonna be nice.


00:38:21.140 --> 00:38:22.740
- So bad, right?


00:38:22.740 --> 00:38:24.740
I mean, you don't entirely control


00:38:24.740 --> 00:38:26.220
what's gonna come out of those things.


00:38:26.220 --> 00:38:29.340
And so, it's a little more predictable, right?


00:38:29.340 --> 00:38:31.980
- And it's not even like, you don't entirely control.


00:38:31.980 --> 00:38:33.940
It's like, I think, yeah, like basically,


00:38:33.940 --> 00:38:36.260
you know, control might be a complete illusion.


00:38:36.260 --> 00:38:38.380
Like even the people who work at OpenAI


00:38:38.380 --> 00:38:41.300
don't fully understand what's happening in there.


00:38:41.300 --> 00:38:42.460
(laughing)


00:38:42.460 --> 00:38:49.580
Yeah, like, well, it read a bunch of stuff and it's predicting the next word and it gets most things right.


00:38:49.580 --> 00:38:54.380
By the way, like they do a lot around this idea of like, not necessarily TDD, but there's a whole eval framework


00:38:54.380 --> 00:38:58.100
so you can submit your evaluation functions to open AI.


00:38:58.100 --> 00:39:05.580
And as they train the next version of things, they include that in what their evaluation system or the new thing.


00:39:05.580 --> 00:39:12.020
So say, if I wanted to go and contribute back a bunch of our text to SQL type use cases as they call evals,


00:39:12.020 --> 00:39:14.500
then they would take that into consideration


00:39:14.500 --> 00:39:16.820
when they train their next models.


00:39:16.820 --> 00:39:19.100
All right, so going down the list, reducing risk, right?


00:39:19.100 --> 00:39:21.140
So you're integrating some of that beast


00:39:21.140 --> 00:39:23.620
that's not fully tamed into your product.


00:39:23.620 --> 00:39:25.660
So you probably want to make sure it's tamed enough


00:39:25.660 --> 00:39:28.060
to live inside your product.


00:39:28.060 --> 00:39:29.700
Continuous improvements,


00:39:29.700 --> 00:39:32.180
that should have been maybe the first one in the list


00:39:32.180 --> 00:39:34.100
is you're iterating on your prompts,


00:39:34.100 --> 00:39:36.340
you're trying to figure out a best context,


00:39:36.340 --> 00:39:39.980
you're trying different model versions,


00:39:39.980 --> 00:39:45.980
maybe you're trying some open source models or the latest GPT cheaper and greater thing.


00:39:45.980 --> 00:39:49.180
So you want to make sure that as you iterate,


00:39:49.180 --> 00:39:52.540
you're getting to the actual outcomes that you want systematically.


00:39:52.540 --> 00:39:56.300
And performance measurement too, like how long does it take? How much does it cost?


00:39:56.300 --> 00:40:00.380
You kind of need to have a handle on that.


00:40:00.380 --> 00:40:04.940
The new model might be 3% better on your corpus of tests,


00:40:04.940 --> 00:40:08.460
but it might be six times the price. Like, do you want, are you okay?


00:40:08.460 --> 00:40:11.100
Right, right. Or just slower from a user perspective.


00:40:11.100 --> 00:40:11.460
Yeah.


00:40:11.460 --> 00:40:12.620
Time to interaction.


00:40:12.620 --> 00:40:15.340
You know, that's one thing with AI we're realizing now is


00:40:15.340 --> 00:40:20.900
a lot of the prompts on four will be like, you know, two to one to seven seconds,


00:40:20.900 --> 00:40:25.260
which in the Google era, you know, there's been some really great papers out of Google


00:40:25.260 --> 00:40:28.620
early on that prove that, you know,


00:40:28.620 --> 00:40:32.420
it's like 100 milliseconds as an impact on user behaviors.


00:40:32.420 --> 00:40:34.060
And I'll say, right.


00:40:34.060 --> 00:40:37.020
Yeah, people give up on checkout flows or whatever,


00:40:37.020 --> 00:40:41.040
and go into the next part of your site on a measurably on a hundred millisecond


00:40:41.040 --> 00:40:41.700
blocks, right?


00:40:41.700 --> 00:40:44.920
When you're talking, well, here's 7,000, you know, you're 70 of those.


00:40:44.920 --> 00:40:46.020
That's going to have an effect.


00:40:46.020 --> 00:40:46.460
Potentially.


00:40:46.460 --> 00:40:51.800
Oh, it has, has been proven and very intricate and usage pattern session


00:40:51.800 --> 00:40:54.120
duration session outcomes, right.


00:40:54.120 --> 00:40:56.580
And you know, a second is a mountain.


00:40:56.580 --> 00:41:00.520
if today, like we were at this AB test, Google between like whatever


00:41:00.520 --> 00:41:04.200
millisecond it's at now, like just one second or half a second.


00:41:04.400 --> 00:41:06.800
that the results coming out of that A/B test


00:41:06.800 --> 00:41:08.800
would show very, very different behaviors.


00:41:08.800 --> 00:41:09.640
- Wow.


00:41:09.640 --> 00:41:10.840
- I think there's some, no quote me on it,


00:41:10.840 --> 00:41:12.240
there's some really great papers, you know,


00:41:12.240 --> 00:41:15.080
written on TTI and just time-share interaction


00:41:15.080 --> 00:41:17.320
and the way it influenced user behavior.


00:41:17.320 --> 00:41:19.520
So we're still, you know, in the AI world,


00:41:19.520 --> 00:41:22.000
it has to, if you're gonna wait two to seven seconds


00:41:22.000 --> 00:41:23.400
for your prompt to come back,


00:41:23.400 --> 00:41:27.160
it's gotta add some real important value to what's happening.


00:41:27.160 --> 00:41:28.000
- Yeah, it does.


00:41:28.000 --> 00:41:30.800
I think it's interesting that it's presented as a chat.


00:41:30.800 --> 00:41:33.000
I think that gives people a little bit of a pause,


00:41:33.000 --> 00:41:34.120
like, oh, it's talking to me,


00:41:34.120 --> 00:41:36.120
So let's let it think for a second


00:41:36.120 --> 00:41:37.240
rather than it's a website


00:41:37.240 --> 00:41:38.620
that's supposed to give me an answer.


00:41:38.620 --> 00:41:40.840
- Yeah, compared to then I guess your basis


00:41:40.840 --> 00:41:44.580
for comparison is a human, not a website


00:41:44.580 --> 00:41:46.400
or not comparing against Google.


00:41:46.400 --> 00:41:47.240
So that's great.


00:41:47.240 --> 00:41:48.840
- Yeah, I asked it a really hard question.


00:41:48.840 --> 00:41:49.920
Give it some time, right?


00:41:49.920 --> 00:41:51.680
Like that's not normally how we think about these things.


00:41:51.680 --> 00:41:53.740
Okay, so you have a kind of a workflow


00:41:53.740 --> 00:41:56.000
from this engineering, building a product,


00:41:56.000 --> 00:41:59.480
testing like an AI inside of your product.


00:41:59.480 --> 00:42:01.040
You wanna walk us through your workflow here?


00:42:01.040 --> 00:42:09.000
Yeah, and you know, if you I think I looked at TDD, you know, and and the originally what is the normal like TDD type workflow.


00:42:09.000 --> 00:42:15.280
And I just adapted this little diagram to prop engineering, right?


00:42:15.280 --> 00:42:20.360
Because the whole idea of the blog post is to bring prop engine like TDD mindset to prop engineering.


00:42:20.360 --> 00:42:22.480
So this is where I went.


00:42:22.480 --> 00:42:27.280
But yeah, the workflow is like, okay, define the use case and desired AI behavior.


00:42:27.280 --> 00:42:28.880
what are you trying to solve with AI?


00:42:28.880 --> 00:42:31.720
In my case, the example that I'll use


00:42:31.720 --> 00:42:34.160
and try to reuse throughout this presentation


00:42:34.160 --> 00:42:39.160
is throughout this conversation is text to SQL.


00:42:39.160 --> 00:42:41.480
So like we're trying to, where the user from,


00:42:41.480 --> 00:42:44.520
what are they schema gets, get the AI to generate


00:42:44.520 --> 00:42:47.000
gets good useful SQL, find the right tables


00:42:47.000 --> 00:42:50.000
and columns to use that kind of stuff, create test cases.


00:42:50.000 --> 00:42:52.600
So it's like, okay, if I have this database


00:42:52.600 --> 00:42:56.080
and I have this from to give me my top five salary


00:42:56.080 --> 00:42:58.720
per department on this HR dataset,


00:42:58.720 --> 00:43:01.920
there's a fairly deterministic output to that.


00:43:01.920 --> 00:43:04.440
You could say the SQL is not necessarily deterministic.


00:43:04.440 --> 00:43:06.120
There's different ways to write that SQL.


00:43:06.120 --> 00:43:09.800
There's a deterministic data frame or results set that might come up.


00:43:09.800 --> 00:43:12.520
>> There is a right answer of the top five salaries.


00:43:12.520 --> 00:43:13.240
>> That's right.


00:43:13.240 --> 00:43:15.400
>> You can see like, am I getting, ultimately get that.


00:43:15.400 --> 00:43:18.520
>> It's great if it is deterministic,


00:43:18.520 --> 00:43:19.720
because you can test this.


00:43:19.720 --> 00:43:24.080
If you're trying to use AI to say write an essay about


00:43:24.080 --> 00:43:32.040
Nepalian, but apart, second conquest, you know, in less than 500 words, it's not as


00:43:32.040 --> 00:43:36.360
deterministic and it's hard to test whether the AI is good or not.


00:43:36.360 --> 00:43:38.480
So you might need human evaluators.


00:43:38.480 --> 00:43:44.640
But I would say in most AI products, or people are trying to bring AI to their product, in


00:43:44.640 --> 00:43:46.480
many cases more deterministic.


00:43:46.480 --> 00:43:51.360
Another example of like more deterministic would say like, oh, getting, if you say getting


00:43:51.360 --> 00:43:56.640
AI to write Python functions. It's like, oh, write a function that, you know, returns if a,


00:43:56.640 --> 00:44:04.720
if a number is prime, yes or no, like that you can get the function and test it in a


00:44:04.720 --> 00:44:09.200
deterministic kind of way. So anyways, just pointing out, it's better, you're only going


00:44:09.200 --> 00:44:14.400
to be able to have a TDD mindset if you have like somewhat deterministic, you know, outcome to the,


00:44:14.400 --> 00:44:16.800
- Right. - want to use the AI for. Then create


00:44:16.800 --> 00:44:19.440
So you create a prompt generator, so that would be your first version,


00:44:19.440 --> 00:44:21.600
and that takes the SQL example.


00:44:21.600 --> 00:44:25.140
It's given the 20 tables in this database,


00:44:25.140 --> 00:44:29.580
and there's columns and table names and data types and sample data.


00:44:29.580 --> 00:44:32.280
Generate SQL that answers the following user prompt,


00:44:32.280 --> 00:44:34.140
and then the user prompt would say something like,


00:44:34.140 --> 00:44:37.540
"Department by top five salary per department."


00:44:37.540 --> 00:44:42.400
And then we're getting, for people that are not on the visual stream,


00:44:42.400 --> 00:44:44.280
not YouTube, but on just audio,


00:44:44.280 --> 00:44:46.080
we're getting into the loop here where it's like,


00:44:46.140 --> 00:44:48.140
evaluate the results, refine the tests,


00:44:48.140 --> 00:44:50.140
refine the prompts, and then start over.


00:44:50.140 --> 00:44:53.140
And probably compile the results,


00:44:53.140 --> 00:44:55.640
keep track of the results so that you can compare.


00:44:55.640 --> 00:44:59.140
Not just like, "Oh, are you 3% better on your test cases?"


00:44:59.140 --> 00:45:03.640
But also, "Which tests that used to fail succeed now?"


00:45:03.640 --> 00:45:07.140
"Which tests that used to fail now?"


00:45:07.140 --> 00:45:10.640
And then once you're happy with the level of success you have,


00:45:10.640 --> 00:45:13.640
you can integrate the prompt into product


00:45:13.640 --> 00:45:15.640
or maybe upgrade?


00:45:15.640 --> 00:45:17.640
- Ship it. - Yeah, ship it.


00:45:17.640 --> 00:45:24.640
- Ship it. So I think it's probably a good time to jump over to your framework for this


00:45:24.640 --> 00:45:27.640
because PyTest and other testing frameworks in Python are great,


00:45:27.640 --> 00:45:32.640
but they're pretty low level compared to these types of questions you're trying to answer, right?


00:45:32.640 --> 00:45:38.640
Like, how has this improved over time for, you know, I was doing 83% right, right?


00:45:38.640 --> 00:45:40.520
PyTest asserts a true or a false.


00:45:40.520 --> 00:45:43.320
It doesn't assert that 83% is...


00:45:43.320 --> 00:45:45.040
- Yeah, and it's a part of CI.


00:45:45.040 --> 00:45:49.480
Like if any of your PyTest fail,


00:45:49.480 --> 00:45:52.400
you're probably gonna not CI, not allow CI,


00:45:52.400 --> 00:45:54.160
not even merge the PR, right?


00:45:54.160 --> 00:45:56.280
So one thing that's different between


00:45:56.280 --> 00:45:58.760
test-driven development and unit testing


00:45:58.760 --> 00:46:02.720
and prompt engineering is that the outcome is probabilistic.


00:46:02.720 --> 00:46:03.600
It's not true or false.


00:46:03.600 --> 00:46:06.040
It might just be like zero or one, right?


00:46:06.040 --> 00:46:09.240
where our spectrum fails.


00:46:09.240 --> 00:46:12.000
For a specific test, you're like,


00:46:12.000 --> 00:46:14.840
"If it gets this column, but not this other column,


00:46:14.840 --> 00:46:16.840
you succeed at 50%."


00:46:16.840 --> 00:46:18.640
So it's non-binary.


00:46:18.640 --> 00:46:21.120
It's also, you don't need perfection to ship.


00:46:21.120 --> 00:46:23.480
You just need better than the previous version


00:46:23.480 --> 00:46:25.320
or good enough to start with.


00:46:25.320 --> 00:46:27.320
So the mindset is,


00:46:27.320 --> 00:46:28.680
so there's a bunch of differences.


00:46:28.680 --> 00:46:29.880
And for those interested,


00:46:29.880 --> 00:46:32.120
we won't get into the blog post.


00:46:32.120 --> 00:46:33.600
I think I list out the things


00:46:33.600 --> 00:46:35.720
that are different between the two.


00:46:35.720 --> 00:46:37.720
I think it's a little bit above this.


00:46:37.720 --> 00:46:41.960
But, you know, the first thing I want to say is like the level of ambition of this project


00:46:41.960 --> 00:46:44.640
versus say an Airflow and SuperSet is like very low, right?


00:46:44.640 --> 00:46:50.880
So it's maybe more similar to a test, a unit test library,


00:46:50.880 --> 00:46:54.920
and no discredit to the great awesome unit test libraries out there,


00:46:54.920 --> 00:46:58.240
but you would think those are fairly simple and straightforward.


00:46:58.240 --> 00:47:02.480
Just the information architecture of a PyTest is probably simpler


00:47:02.480 --> 00:47:05.520
than the information architecture of a Django, for instance, right?


00:47:05.520 --> 00:47:10.160
is just like a different thing. And here, the level of ambition is much more much,


00:47:10.160 --> 00:47:15.320
you know, for this is is fairly simple. So promptimize is something that I


00:47:15.320 --> 00:47:23.800
created, which is a toolkit to help people write to evaluate and score and


00:47:23.800 --> 00:47:29.460
understand while they iterate on their while doing prompt engineering. So in


00:47:29.460 --> 00:47:33.200
this case, I think I talked about the use case of preset, which is a we have a


00:47:33.200 --> 00:47:37.160
a big corpus that luckily was contributed by,


00:47:37.160 --> 00:47:38.240
I forgot which university,


00:47:38.240 --> 00:47:42.640
but a bunch of PhD people did a text to SQL contest.


00:47:42.640 --> 00:47:43.920
- I think it was Yale.


00:47:43.920 --> 00:47:44.760
- Yale, yeah.


00:47:44.760 --> 00:47:46.240
- I think it was Yale, yeah.


00:47:46.240 --> 00:47:47.640
- So great people at Yale were like,


00:47:47.640 --> 00:47:49.160
hey, we're gonna generate, you know,


00:47:49.160 --> 00:47:54.160
3000 prompts on 200 databases with the SQL


00:47:54.160 --> 00:47:56.840
that should be the outcome of that.


00:47:56.840 --> 00:47:59.560
So it's a big test set so that different researchers


00:47:59.560 --> 00:48:02.640
working on text to SQL can compare their results.


00:48:02.640 --> 00:48:04.840
So for us, we're able to take that test set


00:48:04.840 --> 00:48:07.240
and some of our own test sets


00:48:07.240 --> 00:48:10.560
and run it at scale against OpenAI


00:48:10.560 --> 00:48:13.360
or against LLAMA or against different models.


00:48:13.360 --> 00:48:16.480
And by doing that, we're able to evaluate


00:48:16.480 --> 00:48:21.000
this particular combo of this prompt engineering methodology


00:48:21.000 --> 00:48:25.280
with this model generates 73% accuracy.


00:48:25.280 --> 00:48:28.760
And we have these reports we can compare fairly easily


00:48:28.760 --> 00:48:30.720
which prompts that, as I said before,


00:48:30.720 --> 00:48:33.600
were failing before are succeeding now and vice versa.


00:48:33.600 --> 00:48:37.040
So you're like, am I actually making progress here


00:48:37.040 --> 00:48:38.480
or going backwards?


00:48:38.480 --> 00:48:39.760
And if you try to do that on your own,


00:48:39.760 --> 00:48:41.180
like if you're crafting your prompt,


00:48:41.180 --> 00:48:43.720
just anecdotally and try on five or six things,


00:48:43.720 --> 00:48:45.480
like you quickly realize like,


00:48:45.480 --> 00:48:47.760
well shit, I'm gonna need to really test


00:48:47.760 --> 00:48:49.820
out of a much broader range of tests


00:48:49.820 --> 00:48:52.400
and then some regular methodology around that.


00:48:52.400 --> 00:48:53.400
So probably-- - Right, how do you


00:48:53.400 --> 00:48:54.800
remember and go back and go,


00:48:54.800 --> 00:48:56.500
this actually made it better, right?


00:48:56.500 --> 00:48:59.120
'Cause it's hard to keep all that in your mind, yeah.


00:48:59.120 --> 00:49:02.480
Yeah, and something interesting that I'm realizing


00:49:02.480 --> 00:49:04.480
to work on this stuff is like,


00:49:04.480 --> 00:49:05.920
everything is changing so fast, right?


00:49:05.920 --> 00:49:07.280
The models are changing fast,


00:49:07.280 --> 00:49:09.280
the prompting windows are changing fast,


00:49:09.280 --> 00:49:11.680
the vector databases, which is a way that


00:49:11.680 --> 00:49:14.560
organize and structure a context for your prompts,


00:49:14.560 --> 00:49:16.240
evolving extremely fast.


00:49:16.240 --> 00:49:19.200
So it feels like you're working on unsettled ground


00:49:19.200 --> 00:49:21.680
in a lot of ways, like a lot of stuff you're doing


00:49:21.680 --> 00:49:23.440
might be challenged by, you know,


00:49:23.440 --> 00:49:25.360
the Bart API came out last week,


00:49:25.360 --> 00:49:27.360
and maybe it's better at SQL generation,


00:49:27.360 --> 00:49:30.720
And then you got to throw everything that I did on OpenAI.


00:49:30.720 --> 00:49:32.520
But here's something you don't throw away.


00:49:32.520 --> 00:49:35.640
Your test library and your use cases, right?


00:49:35.640 --> 00:49:38.880
It's maybe is the thing, is the real asset here.


00:49:38.880 --> 00:49:39.960
The rest of the stuff is like,


00:49:39.960 --> 00:49:43.000
AI is moving so fast that all the mechanics


00:49:43.000 --> 00:49:46.040
of the engineering itself


00:49:46.040 --> 00:49:49.480
and the interface with the whatever model


00:49:49.480 --> 00:49:50.920
is the best at the time,


00:49:50.920 --> 00:49:52.360
you're probably going to have to throw away


00:49:52.360 --> 00:49:53.520
as this evolves quickly.


00:49:53.520 --> 00:49:56.960
But your test library is something really, really solid


00:49:56.960 --> 00:50:00.400
that you can perpetuate or keep improving


00:50:00.400 --> 00:50:03.240
and bringing along with you along the way.


00:50:03.240 --> 00:50:05.640
So it's kind of an interesting thought around that.


00:50:05.640 --> 00:50:07.760
- Yeah, let's talk through this example


00:50:07.760 --> 00:50:11.560
you have on Promptimize's GitHub read me here


00:50:11.560 --> 00:50:13.400
to make it a little concrete for people,


00:50:13.400 --> 00:50:15.640
like how do you actually write one of these tests?


00:50:15.640 --> 00:50:18.320
- Yeah, so there's different types of prompts,


00:50:18.320 --> 00:50:22.000
but yeah, what I wanted to get to was just like,


00:50:22.000 --> 00:50:25.400
what is the prompt and how do you evaluate it, right?


00:50:25.400 --> 00:50:27.800
And then behind the scene, we're going to be


00:50:27.800 --> 00:50:30.600
discovering all your prompts and running them


00:50:30.600 --> 00:50:32.600
and compiling results and reports,


00:50:32.600 --> 00:50:35.320
and doing analytics and making it easy to do analytics on it.


00:50:35.320 --> 00:50:38.520
The examples that we have here, and I'll try to be conscious


00:50:38.520 --> 00:50:41.160
of both the people who can read the code and people who don't,


00:50:41.160 --> 00:50:43.160
people who are just on audio.


00:50:43.160 --> 00:50:47.400
But here, from Proptimize, the prompt, we import a simple prompt,


00:50:47.400 --> 00:50:50.280
and then we bring some evals that are just like utility functions


00:50:50.280 --> 00:50:55.560
around evaluating the output of what comes back from the AI.


00:50:55.560 --> 00:50:57.920
And here the first prompt case in the model,


00:50:57.920 --> 00:51:02.280
here I just create an array or a list of prompt cases,


00:51:02.280 --> 00:51:04.640
and it's a prompt case like a test case.


00:51:04.640 --> 00:51:08.120
And with this prompt case, this very simple one,


00:51:08.120 --> 00:51:10.160
I say, "Hello there!"


00:51:10.160 --> 00:51:12.680
And then I evaluate that as says,


00:51:12.680 --> 00:51:16.040
either "Hi" or "Hello" in the output.


00:51:16.040 --> 00:51:19.360
So if any of the words exist and what comes back,


00:51:19.360 --> 00:51:21.960
I give it a one or a zero.


00:51:21.960 --> 00:51:23.460
Framework allows you to, you could say,


00:51:23.460 --> 00:51:24.960
"Oh, it has to have both these words,"


00:51:24.960 --> 00:51:27.660
or give the percentage of success


00:51:27.660 --> 00:51:30.460
based on the number of words from this list that it has.


00:51:30.460 --> 00:51:33.460
But, you know, that's the first case.


00:51:33.460 --> 00:51:35.860
The second one is like a little bit more complicated,


00:51:35.860 --> 00:51:40.360
but name the top 50 guitar players of all time, I guess,


00:51:40.360 --> 00:51:42.360
and I make sure that Frank Zappa is in the list


00:51:42.360 --> 00:51:44.360
because I'm a Frank Zappa fan here.


00:51:44.360 --> 00:51:46.960
But, you know, you could have a different,


00:51:46.960 --> 00:51:49.060
you could say, "Hey, I want to make sure that,


00:51:49.060 --> 00:51:52.500
you know, at least three out of five of these are in the list.


00:51:52.500 --> 00:51:57.300
Those are like very more like natural language, very simple tests to,


00:51:57.300 --> 00:52:00.300
you know, just so that's the hello world essentially.


00:52:00.300 --> 00:52:03.500
And then, you know, we're showing some examples of what's happening behind the scene.


00:52:03.500 --> 00:52:08.500
Well, we'll actually like call, you know, the underlying API, get the results,


00:52:08.500 --> 00:52:10.980
run your eval function and compile a report.


00:52:10.980 --> 00:52:12.300
What was the prompt?


00:52:12.300 --> 00:52:15.740
What was, oh, a bird just flew into my room.


00:52:15.740 --> 00:52:16.620
Inside?


00:52:16.620 --> 00:52:19.260
Yeah, that's gonna make the podcast interesting.


00:52:19.260 --> 00:52:21.340
Oh my goodness. Okay.


00:52:21.340 --> 00:52:23.580
That might be a first here.


00:52:23.580 --> 00:52:25.580
You're nuts.


00:52:25.580 --> 00:52:26.940
Oh, well, out of my room.


00:52:26.940 --> 00:52:31.660
Guess what? There's other people and that house, I'm just gonna close the door to my room.


00:52:31.660 --> 00:52:32.940
Deal with it later.


00:52:32.940 --> 00:52:35.340
All right. Well, that's a first.


00:52:35.340 --> 00:52:42.220
I've had a bat fly into my house once, but never a bird. So both are crazy.


00:52:42.220 --> 00:52:46.380
This is the first on the podcast. Out of eight years, we've never had a bird


00:52:46.380 --> 00:52:49.900
wild animal enter the studio of the guest.


00:52:49.900 --> 00:52:52.740
- Yes, well, welcome to my room.


00:52:52.740 --> 00:52:55.020
I live in Taos, so I guess that's something


00:52:55.020 --> 00:52:56.660
that's better than a bear, you know?


00:52:56.660 --> 00:52:57.700
I could have been more.


00:52:57.700 --> 00:52:58.780
- It is better than a bear.


00:52:58.780 --> 00:53:00.420
- All right, but yeah, so like,


00:53:00.420 --> 00:53:03.420
just like keep enumerating kind of what we're seeing


00:53:03.420 --> 00:53:07.660
visually here, you know, we'll keep a YAML file


00:53:07.660 --> 00:53:09.860
as the report output.


00:53:09.860 --> 00:53:13.220
So in Promptomize, you have your test case


00:53:13.220 --> 00:53:15.780
or your prompt cases, like test cases.


00:53:15.780 --> 00:53:17.660
you have an output report that says,


00:53:17.660 --> 00:53:20.740
for this, you know, prompt case, here's the key.


00:53:20.740 --> 00:53:22.200
Here's what the prompt that was actually


00:53:22.200 --> 00:53:24.240
the user input that came in.


00:53:24.240 --> 00:53:26.160
Here's what the prompt looked like.


00:53:26.160 --> 00:53:28.540
You know, what was the response,


00:53:28.540 --> 00:53:30.700
the raw response from the API?


00:53:30.700 --> 00:53:31.620
What are all the tasks?


00:53:31.620 --> 00:53:32.460
How long did it run?


00:53:32.460 --> 00:53:35.380
So a bunch of metadata and relevant information


00:53:35.380 --> 00:53:38.100
that we can use later to create these reports.


00:53:38.100 --> 00:53:40.920
Say like, was the score zero or one?


00:53:40.920 --> 00:53:43.140
So you get the whole output report here.


00:53:43.140 --> 00:53:43.980
- Yeah, okay.


00:53:43.980 --> 00:53:47.140
And then you also have a way to get like a report.


00:53:47.140 --> 00:53:49.180
I'm not sure, maybe I scrolled past it.


00:53:49.180 --> 00:53:51.580
Where it shows you how it did, right?


00:53:51.580 --> 00:53:53.060
I think that was in your-


00:53:53.060 --> 00:53:55.260
- I think at the blog post, you see a much more-


00:53:55.260 --> 00:53:56.820
- There you go, yeah.


00:53:56.820 --> 00:53:59.620
- So this one, we're running the spider dataset


00:53:59.620 --> 00:54:01.220
that I just, that I talked about.


00:54:01.220 --> 00:54:03.620
Remember, it's like the Yale generated text


00:54:03.620 --> 00:54:06.100
to SQL competition, you know, corpus.


00:54:06.100 --> 00:54:08.460
And so here we looked at, you know,


00:54:08.460 --> 00:54:10.620
my percentage of success is 70%.


00:54:10.620 --> 00:54:12.980
So, you know, here you say weight and score.


00:54:12.980 --> 00:54:18.180
So there's a way to say, this particular prompt case is 10 times more important than another one,


00:54:18.180 --> 00:54:23.140
right, so you can do a relative importance of weight of your different text cases.


00:54:23.140 --> 00:54:28.340
Now, one thing we didn't mention too, is like all these tests are generated programmatically too.


00:54:28.340 --> 00:54:32.100
So that it's the same philosophy behind, you know, airflow of like, you know,


00:54:32.100 --> 00:54:35.540
it's almost like a little DSL to write your test case.


00:54:35.540 --> 00:54:38.740
So you could, you know, you could read from a YAML file, for instance,


00:54:38.740 --> 00:54:40.820
in the case of what we do with Spyder SQL,


00:54:40.820 --> 00:54:43.700
there's a big JSON file of all the prompts and all the databases,


00:54:43.700 --> 00:54:48.300
and then we dynamically generate a thousand tests based on that.


00:54:48.300 --> 00:54:50.960
So you can do programmatic test definition,


00:54:50.960 --> 00:54:53.560
more and more dynamic if you want it to be,


00:54:53.560 --> 00:54:55.320
or you could do more static if you prefer that.


00:54:55.320 --> 00:54:59.220
So in this case, we're doing, we introduced this idea of a category too.


00:54:59.220 --> 00:55:01.860
So I mentioned there's some features in Promptomize,


00:55:01.860 --> 00:55:07.160
like categorizing your tests or weights, and things like that.


00:55:07.220 --> 00:55:10.820
So here we'll do some reporting on per category.


00:55:10.820 --> 00:55:12.620
What is the score per category?


00:55:12.620 --> 00:55:16.620
You can see which database is performing well or poorly again.


00:55:16.620 --> 00:55:19.260
So I could have another category that is large database,


00:55:19.260 --> 00:55:24.500
small databases and see what the score is and compare reports.


00:55:24.500 --> 00:55:29.220
>> It's pretty cool that it saves the test run to a file that then you can ask


00:55:29.220 --> 00:55:31.980
questions about and generate this report on


00:55:31.980 --> 00:55:35.140
and rather than just running it and passing or failing.


00:55:35.140 --> 00:55:38.340
Yeah, or like giving the output and then having to run it again.


00:55:38.340 --> 00:55:41.380
Yeah, there's some other features around if,


00:55:41.380 --> 00:55:43.380
so you can memoize the test.


00:55:43.380 --> 00:55:48.580
So because it has a report, if you exit off of it or restart it later,


00:55:48.580 --> 00:55:54.100
it won't rerun the same test if it's the same hash input.


00:55:54.100 --> 00:55:56.820
Even though with AI you might get a different answer.


00:55:56.820 --> 00:55:58.820
But at least in this case, it will say like,


00:55:58.820 --> 00:56:02.740
"Hey, I'm rerunning the same prompt."


00:56:02.740 --> 00:56:04.980
instead of like waiting five seconds for open AI


00:56:04.980 --> 00:56:07.340
and paying the tokens and paying the piper,


00:56:07.340 --> 00:56:09.940
you know, I'm just going to skip that.


00:56:09.940 --> 00:56:13.220
So there's some logic around skipping what's been done already.


00:56:13.220 --> 00:56:15.500
It's not just a couple of milliseconds to run it.


00:56:15.500 --> 00:56:17.420
It could be a while to get the answers.


00:56:17.420 --> 00:56:21.020
Yeah, also like your early libraries that I haven't written the sub,


00:56:21.020 --> 00:56:22.860
like the threading for it where you can say,


00:56:22.860 --> 00:56:24.860
oh, run it on eight threads.


00:56:24.860 --> 00:56:28.220
So with promptimize, I think,


00:56:28.220 --> 00:56:31.780
and you know, the blog post is probably more impactful


00:56:31.780 --> 00:56:33.820
than the Python project itself.


00:56:33.820 --> 00:56:37.180
If the Python project takes off and a bunch of people are using it to test


00:56:37.180 --> 00:56:39.860
prompts and contribute to it, it's great.


00:56:39.860 --> 00:56:43.380
But I think it's more like, okay, this is uncharted territory.


00:56:43.380 --> 00:56:47.260
I'm working with an AI type interface.


00:56:47.260 --> 00:56:51.900
And then it's more like, how do we, what's, how do we best do that as


00:56:51.900 --> 00:56:54.460
practitioners or as people building products?


00:56:54.460 --> 00:56:58.580
I think that's the big idea there, you know, then the test library, you


00:56:58.580 --> 00:56:59.620
could probably write your own.


00:56:59.620 --> 00:57:02.180
Like, I think for me, that was like a one or two week project.


00:57:02.180 --> 00:57:06.740
The, what I would like to say as like normally, if it wasn't for getting all


00:57:06.740 --> 00:57:11.140
the help from ChatGPT on, you know, it's like, Oh, I'm creative project.


00:57:11.140 --> 00:57:15.860
I'm setting up my setup.py, you know, set up tools is always a little bit of gas.


00:57:15.860 --> 00:57:19.780
And then I'm like, can you help me create like my setup.py and


00:57:19.780 --> 00:57:21.540
then, you know, generate some code.


00:57:21.540 --> 00:57:27.060
And I'm like, Oh, I want to make sure that PyPI is going to get my read me from GitHub.


00:57:27.060 --> 00:57:30.180
I forgot how to read the markdown and pass the stuff.


00:57:30.180 --> 00:57:31.300
Can you do that for me?


00:57:31.300 --> 00:57:34.300
And then ShedGBT generates this stuff very nicely.


00:57:34.300 --> 00:57:34.660
Right.


00:57:34.660 --> 00:57:40.900
Or I want to make sure I use my request requirements of TXT inside my dynamically


00:57:40.900 --> 00:57:42.980
building my setup tools integration.


00:57:42.980 --> 00:57:43.860
Can you do that for me?


00:57:43.860 --> 00:57:45.500
And it's just like, bam, bam, bam.


00:57:45.500 --> 00:57:47.100
Like all the repetitive stuff.


00:57:47.100 --> 00:57:48.140
I need a function.


00:57:48.140 --> 00:57:48.820
It's incredible.


00:57:48.820 --> 00:57:49.980
Go ahead.


00:57:49.980 --> 00:57:50.460
Yeah.


00:57:50.460 --> 00:57:52.860
I kind of, well, I just, I kind of want to close out the conversation with that.


00:57:52.860 --> 00:57:55.780
I do agree that the blog post is super powerful


00:57:55.780 --> 00:57:58.480
in how it kind of teaches you to think


00:57:58.480 --> 00:58:00.780
about how might you go about testing,


00:58:00.780 --> 00:58:03.440
integrating with an AI and these types of products, right?


00:58:03.440 --> 00:58:06.980
Much like TDD brought a way to think about


00:58:06.980 --> 00:58:08.940
how do we actually apply the concept just,


00:58:08.940 --> 00:58:10.160
well, I have things and I can test them


00:58:10.160 --> 00:58:11.860
with this assert thing.


00:58:11.860 --> 00:58:13.480
How should I actually go about building software, right?


00:58:13.480 --> 00:58:16.740
So this is kind of that for AI integrated software.


00:58:16.740 --> 00:58:18.980
So it's certainly worth people watching.


00:58:18.980 --> 00:58:19.980
Let's just close it out with,


00:58:19.980 --> 00:58:22.140
you know, you kind of touched on some of those things there.


00:58:22.140 --> 00:58:32.640
Like, you know, how do you recommend that people leverage things like ChatGPT to help them build, build their, their apps or how to use AI?


00:58:32.640 --> 00:58:34.140
Just kind of, oh my God, yeah.


00:58:34.140 --> 00:58:36.940
Like amp up your software development.


00:58:36.940 --> 00:58:37.740
100%.


00:58:37.740 --> 00:58:51.640
I mean, it's, it's been a lot of people report, you know, on Twitter, people used to Google, you know, all the problems that they had while writing, you know, writing code and using a lot of Stack Overflow.


00:58:51.640 --> 00:58:54.200
I don't know what the stats on Stack Overflow traffic,


00:58:54.200 --> 00:58:58.440
but once you try working with Chatterjee PT to do coding,


00:58:58.440 --> 00:59:01.960
you probably don't go back to those other flows of,


00:59:01.960 --> 00:59:05.680
I don't know, it's like putting your error message or stack trace into Google


00:59:05.680 --> 00:59:08.000
and then go on to a bunch of Stack Overflow link


00:59:08.000 --> 00:59:11.040
and try to make sense of what comes out.


00:59:11.040 --> 00:59:15.400
To me, it's been so much better to go just with Chatterjee PT,


00:59:15.400 --> 00:59:16.920
and there's a conversation there too.


00:59:16.920 --> 00:59:20.040
So say, for instance, if I'm in Proptomize, I need a function to say,


00:59:20.040 --> 00:59:23.220
Can you write, I wrote that function before, you know, but it's a, can you


00:59:23.220 --> 00:59:28.600
crawl a certain given folder and look for modules that contain objects of a


00:59:28.600 --> 00:59:31.620
certain class and then bring that back?


00:59:31.620 --> 00:59:35.340
And, you know, you have to use the import lib and she's a little bit


00:59:35.340 --> 00:59:36.600
pain in the ass to write this.


00:59:36.600 --> 00:59:39.380
So it writes, you know, a function that works pretty well.


00:59:39.380 --> 00:59:42.440
Then like, Oh yeah, I forgot to ask you to look into lists and


00:59:42.440 --> 00:59:43.840
dictionaries, can you do that too?


00:59:43.840 --> 00:59:45.540
Then, Oh, it does that in a second.


00:59:45.540 --> 00:59:48.260
It's like, I, you know, you didn't add like type hints and doc


00:59:48.260 --> 00:59:52.300
string and doc test and you write that too.


00:59:52.300 --> 00:59:53.620
I just bang, bang, bang,


00:59:53.620 --> 00:59:56.820
and just copy paste in your utils file and it works,


00:59:56.820 --> 00:59:58.820
and you save two hours.


00:59:58.820 --> 01:00:03.140
>> I think it would be really good at those things that are algorithmic.


01:00:03.140 --> 01:00:08.660
Now, it might be the thing that you would do on a whiteboard job interview test.


01:00:08.660 --> 01:00:11.700
It's just going to know that really, really solid.


01:00:11.700 --> 01:00:13.740
It actually, but it knows


01:00:13.740 --> 01:00:16.820
quite a bit about the other libraries and stuff that are out there.


01:00:16.820 --> 01:00:19.940
That's insane. Yeah, so one that I came across is


01:00:19.940 --> 01:00:22.420
I want to, I leverage something called link chain, which


01:00:22.420 --> 01:00:24.980
pointed to people getting interested in


01:00:24.980 --> 01:00:27.780
prompt engineering. There's a really good,


01:00:27.780 --> 01:00:30.100
well, the library link chain is really interesting.


01:00:30.100 --> 01:00:33.220
It's not perfect, it's news moving fast, but


01:00:33.220 --> 01:00:34.740
push people to check it out.


01:00:34.740 --> 01:00:37.220
Also like 41,000 stars, so very...


01:00:37.220 --> 01:00:38.660
I know that's right.


01:00:38.660 --> 01:00:39.780
And written in Python.


01:00:39.780 --> 01:00:42.260
Yes, you can do like, yeah, it's in Python too.


01:00:42.260 --> 01:00:46.100
You should talk to whoever is writing this


01:00:46.100 --> 01:00:47.300
or started this.


01:00:47.300 --> 01:00:49.620
But yeah, you can change some prompt to say like


01:00:49.620 --> 01:00:51.820
the output of a prompt will generate the next one.


01:00:51.820 --> 01:00:53.700
There's this idea of agents.


01:00:53.700 --> 01:00:58.300
There's this idea of estimating tokens before doing the request.


01:00:58.300 --> 01:01:02.420
There's a bunch of really cool things that it does.


01:01:02.420 --> 01:01:04.100
To me, the docs are not that comprehensive.


01:01:04.100 --> 01:01:05.900
There's someone else that created,


01:01:05.900 --> 01:01:10.220
if you Google "Lang chain cookbook",


01:01:10.220 --> 01:01:14.260
you'll find someone else that wrote what I thought was more


01:01:14.260 --> 01:01:16.460
comprehensive way to start.


01:01:16.460 --> 01:01:18.460
But this one has a YouTube video


01:01:18.460 --> 01:01:21.860
and an IP1B file introduces you to the concept


01:01:21.860 --> 01:01:23.100
in an interactive way.


01:01:23.100 --> 01:01:25.700
I thought that was really good.


01:01:25.700 --> 01:01:26.940
But yeah, so we were trying to,


01:01:26.940 --> 01:01:27.860
I was trying to use this.


01:01:27.860 --> 01:01:28.860
I was like, oh, Chad,


01:01:28.860 --> 01:01:30.100
can you generate a bunch of like


01:01:30.100 --> 01:01:30.940
land chain related stuff?


01:01:30.940 --> 01:01:32.380
I was like, I don't know of a project


01:01:32.380 --> 01:01:35.540
calling chain that was created after 2021.


01:01:35.540 --> 01:01:37.420
So I was like, I wish I could just say like,


01:01:37.420 --> 01:01:38.820
just go read the GitHub,


01:01:38.820 --> 01:01:40.740
you know, just read it all, read the docs.


01:01:40.740 --> 01:01:42.980
And then I'll ask you questions.


01:01:42.980 --> 01:01:46.900
And then Chajupiti is not that great at that currently,


01:01:46.900 --> 01:01:48.820
at learning things it doesn't know,


01:01:48.820 --> 01:01:50.500
for a reason we talked about.


01:01:50.500 --> 01:01:52.620
BARD is much more up to date.


01:01:52.620 --> 01:01:55.460
So you can always, for those projects,


01:01:55.460 --> 01:01:57.260
you know, Chajupiti might be better at Django


01:01:57.260 --> 01:01:59.380
'cause it's old and settled,


01:01:59.380 --> 01:02:01.020
and it's better at writing code overall,


01:02:01.020 --> 01:02:04.100
but BARD might be decent and pretty good for that.


01:02:04.100 --> 01:02:06.700
- Right, if you ask advice on how to do prompt-imized stuff,


01:02:06.700 --> 01:02:07.860
it's like, I don't know what that is.


01:02:07.860 --> 01:02:09.100
- Yeah, it's like, I've never heard of,


01:02:09.100 --> 01:02:11.180
it might elucidate to, I think if you go in,


01:02:11.180 --> 01:02:13.180
might make shit up. Like I've seen it,


01:02:13.180 --> 01:02:15.180
front of my install, it would be this


01:02:15.180 --> 01:02:17.180
and it just makes up stuff.


01:02:17.180 --> 01:02:19.180
So, not that great.


01:02:19.180 --> 01:02:21.180
But yeah, absolutely I encourage people


01:02:21.180 --> 01:02:23.180
to try, you know, for any


01:02:23.180 --> 01:02:25.180
subtask that you're trying to do


01:02:25.180 --> 01:02:27.180
to see if it can help you at it


01:02:27.180 --> 01:02:29.180
and maybe try a variation


01:02:29.180 --> 01:02:31.180
on the front and then, yeah, if it's not good


01:02:31.180 --> 01:02:33.180
at it, just do it the old way.


01:02:33.180 --> 01:02:35.180
But yeah, it might be better


01:02:35.180 --> 01:02:37.180
too for those familiar with the idea of like functional


01:02:37.180 --> 01:02:39.180
programming, where each function is


01:02:39.180 --> 01:02:42.220
is more deterministic and can be reasoned about


01:02:42.220 --> 01:02:44.220
and unit tested in isolation.


01:02:44.220 --> 01:02:46.300
A Charged GPT is gonna be better at that


01:02:46.300 --> 01:02:48.500
'cause it doesn't know about all your other packages


01:02:48.500 --> 01:02:49.340
and modules.


01:02:49.340 --> 01:02:51.520
So really great for the utils functions


01:02:51.520 --> 01:02:55.700
are very deterministic, functional, super great at that.


01:02:55.700 --> 01:02:59.500
Another thing is, and you tell me when we run out of time,


01:02:59.500 --> 01:03:03.260
another thing that I was really interesting too,


01:03:03.260 --> 01:03:05.380
bring some of the content and prompt them


01:03:05.380 --> 01:03:07.940
as they're writing the blog posts itself.


01:03:07.940 --> 01:03:11.460
things like, hey, I'm thinking about the difference of like the properties of


01:03:11.460 --> 01:03:14.660
test driven development as it applies for engineering.


01:03:14.660 --> 01:03:19.700
Uh, here's my blog post, but can you think of other differences between


01:03:19.700 --> 01:03:23.940
the two that are very core, you know, can you talk about the similarities


01:03:23.940 --> 01:03:27.860
and the differences and it would come up with like just really, really great


01:03:27.860 --> 01:03:33.300
ideas, right, brainstorming and very smart at mixing concepts.


01:03:33.300 --> 01:03:37.020
I do think one thing that not a great idea is you say, write this for me,


01:03:37.180 --> 01:03:41.980
But if you've got something in mind and you're going to say, give me some ideas or how should I go?


01:03:41.980 --> 01:03:43.500
Where should I go deeper into this?


01:03:43.500 --> 01:03:46.660
And then you use your own creativity to create that.


01:03:46.660 --> 01:03:48.380
That's a totally valid use.


01:03:48.380 --> 01:03:50.940
I wouldn't feel like, oh, I mean, this AI crap, right?


01:03:50.940 --> 01:03:54.140
It just it brought out some insights that you had forgot to think about.


01:03:54.140 --> 01:03:55.980
And now you now you are right.


01:03:55.980 --> 01:03:58.660
Or when it fails, it's just like I got it to fail.


01:03:58.660 --> 01:04:00.260
I is wrong and smarter than it.


01:04:00.260 --> 01:04:00.580
Do you like?


01:04:00.580 --> 01:04:05.580
Wait, is there something I can can I try to, you know, here's what it didn't get right.


01:04:05.580 --> 01:04:07.820
and why, like, what did I need to tell it?


01:04:07.820 --> 01:04:10.660
So you can go and edit your prompt or ask a follow up


01:04:10.660 --> 01:04:13.780
and generally it will do better and well.


01:04:13.780 --> 01:04:17.260
Yeah, I think also you can ask it to find bugs or security vulnerabilities.


01:04:17.260 --> 01:04:17.660
Yeah.


01:04:17.660 --> 01:04:20.940
Right? You know, like, here is my 30 line function.


01:04:20.940 --> 01:04:21.940
Do you see any bugs?


01:04:21.940 --> 01:04:25.380
Do you see any security vulnerabilities?


01:04:25.380 --> 01:04:29.100
Like, yeah, you're passing this straight to,


01:04:29.100 --> 01:04:31.780
you're concatenating the string in the SQL or something like that.


01:04:31.780 --> 01:04:34.540
Yeah, the regular stuff too.


01:04:34.540 --> 01:04:38.060
Or like, you know, I would say writing a good doc string,


01:04:38.060 --> 01:04:40.860
writing doc tests, writing unit tests,


01:04:40.860 --> 01:04:43.700
reviewing the logic, that kind of stuff.


01:04:43.700 --> 01:04:45.700
It does, type hints, right?


01:04:45.700 --> 01:04:49.900
If you're like me, like, I don't really like to write type hints upfront.


01:04:49.900 --> 01:04:53.340
But I'm like, can you just like sprinkle some type hints on top of that?


01:04:53.340 --> 01:04:55.100
Retrofit this thing for me.


01:04:55.100 --> 01:04:57.220
Yeah, that's it. Just make it that production grade.


01:04:57.220 --> 01:04:59.020
You know, one thing that's interesting too, of like,


01:04:59.020 --> 01:05:04.220
you know, you would think I'm a big TDD guy, like, I don't do test driven development.


01:05:04.220 --> 01:05:06.620
This is not my thing, I like to write code,


01:05:06.620 --> 01:05:09.660
I don't think of what I'm going to use the function for


01:05:09.660 --> 01:05:11.660
before I write it.


01:05:11.660 --> 01:05:17.100
But it's good at generating unit tests for a function too.


01:05:17.100 --> 01:05:20.140
And then I think what's interesting with Promptimize too


01:05:20.140 --> 01:05:22.460
is you might think you want deterministic,


01:05:22.460 --> 01:05:24.860
what I call prompt cases or test cases,


01:05:24.860 --> 01:05:29.420
but you can say I've written five or six of these,


01:05:29.420 --> 01:05:32.220
can you write variations on that theme too?


01:05:32.220 --> 01:05:38.460
So you can use it to generate test cases in the case of like TDD, but also the


01:05:38.460 --> 01:05:41.260
opposite, like for, for prompt demise, you can get it to generate stuff


01:05:41.260 --> 01:05:43.220
dynamically to itself.


01:05:43.220 --> 01:05:43.580
Yeah.


01:05:43.580 --> 01:05:45.060
It's, it's pretty amazing.


01:05:45.060 --> 01:05:45.540
It is.


01:05:45.540 --> 01:05:46.100
It is pretty neat.


01:05:46.100 --> 01:05:48.860
Let let's maybe close this out, but I'll ask you one more question.


01:05:48.860 --> 01:05:49.380
Okay.


01:05:49.380 --> 01:05:50.340
Can I do one more?


01:05:50.340 --> 01:05:51.140
Can I show one more?


01:05:51.140 --> 01:05:52.740
This is Python podcast.


01:05:52.740 --> 01:05:57.540
If you go on my repo on that repo for prompt demise under examples,


01:05:57.540 --> 01:05:59.060
there's one called Python exam.


01:05:59.060 --> 01:06:01.180
Yeah.


01:06:01.180 --> 01:06:01.660
Something like this.


01:06:01.660 --> 01:06:03.340
So let's start right here.


01:06:03.340 --> 01:06:05.060
So say here it says,


01:06:05.060 --> 01:06:08.740
so here I wrote a prompt that asks the bot


01:06:08.740 --> 01:06:10.980
to generate Python function,


01:06:10.980 --> 01:06:13.940
then I sandbox it and bring the function it wrote


01:06:13.940 --> 01:06:16.020
into the interpreter, and then I test it.


01:06:16.020 --> 01:06:17.980
So I say, write a function that tests


01:06:17.980 --> 01:06:19.500
if a number is a prime number,


01:06:19.500 --> 01:06:21.740
returns a boolean.


01:06:21.740 --> 01:06:25.820
And then I test, I have six state test cases for it.


01:06:25.820 --> 01:06:28.500
So write a function that finds the greatest common denominator


01:06:28.500 --> 01:06:30.340
of two numbers, right?


01:06:30.340 --> 01:06:33.620
Then behind the scene, we won't get to the class above.


01:06:33.620 --> 01:06:36.460
The class above basically interacts with it,


01:06:36.460 --> 01:06:38.860
gets the input, then runs the test,


01:06:38.860 --> 01:06:40.300
then compiles the results, right?


01:06:40.300 --> 01:06:44.940
So we could test how well 3.5 compares to 4.


01:06:44.940 --> 01:06:48.020
But I thought it was relevant for the Python folks


01:06:48.020 --> 01:06:50.220
on the line, so we're testing out


01:06:50.220 --> 01:06:52.100
what it is that writing Python function.


01:06:52.100 --> 01:06:54.820
- Write a function that generates the Fibonacci sequence.


01:06:54.820 --> 01:06:55.660
Yeah, very cool.


01:06:55.660 --> 01:06:57.260
- Up to a certain number of terms, right?


01:06:57.260 --> 01:06:58.540
So it's easy to test.


01:06:58.540 --> 01:06:59.260
So it's cool stuff.


01:06:59.260 --> 01:07:01.460
And what was your last question?


01:07:01.460 --> 01:07:05.380
Oh, I was going to say something like, see if I see how far we can push it.


01:07:05.380 --> 01:07:06.100
Um, right.


01:07:06.100 --> 01:07:13.540
A Python function to use requests and beautiful soup to scrape the


01:07:13.540 --> 01:07:17.520
titles of episodes of talk Python to me.


01:07:17.520 --> 01:07:18.740
Oh yeah.


01:07:18.740 --> 01:07:20.560
And then, yeah, it is.


01:07:20.560 --> 01:07:23.380
And if you don't have a, you know, one thing that's a pain in the butt for


01:07:23.380 --> 01:07:26.940
podcasts, people as to write the, like what, what all we talk about.


01:07:26.940 --> 01:07:29.860
So you use another AI to get the transcripts.


01:07:29.860 --> 01:07:32.740
It's like, can you write something that's going to leverage this library


01:07:32.740 --> 01:07:34.980
to transcript the library, summarize it,


01:07:34.980 --> 01:07:39.780
publish it back with SEO in mind.


01:07:39.780 --> 01:07:41.700
It's really quite amazing.


01:07:41.700 --> 01:07:43.140
It went through and said, "Okay, here's a function."


01:07:43.140 --> 01:07:46.780
And it knows talkbython.m/episodes/all,


01:07:46.780 --> 01:07:48.340
use age, get the title.


01:07:48.340 --> 01:07:50.660
And let's just finish this out, Max.


01:07:50.660 --> 01:07:53.380
I'll throw this into an interpreter, see if it runs.


01:07:53.380 --> 01:07:55.860
Interpreter and I'll see if I can get it to run.


01:07:55.860 --> 01:07:57.940
- And you know what's really interesting too,


01:07:57.940 --> 01:07:59.860
is like you can give it a random function,


01:07:59.860 --> 01:08:02.340
like you can write a function and say like,


01:08:02.340 --> 01:08:06.100
if you write a certain function that does certain things


01:08:06.100 --> 01:08:09.500
and you say, if I give this input to this function,


01:08:09.500 --> 01:08:11.220
what is it gonna come out of?


01:08:11.220 --> 01:08:13.020
And it doesn't have an interpreter,


01:08:13.020 --> 01:08:16.540
but it can interpret code like you and I do, right?


01:08:16.540 --> 01:08:18.820
Like an interview question of like,


01:08:18.820 --> 01:08:21.460
hey, here's a function, if I input a three as a value,


01:08:21.460 --> 01:08:23.100
what's gonna come, what's gonna return?


01:08:23.100 --> 01:08:25.540
So it's able to do it, follow the loops,


01:08:25.540 --> 01:08:29.260
follow the if statements and basically just do a logical.


01:08:29.260 --> 01:08:31.500
- Yeah, another thing I think would be really good


01:08:31.500 --> 01:08:34.700
is to say, here's a function, explain to me what it does.


01:08:34.700 --> 01:08:35.860
- Oh yeah, it's super great at that.


01:08:35.860 --> 01:08:37.700
It's great at that for SQL too.


01:08:37.700 --> 01:08:39.860
Here's a stupid long SQL query.


01:08:39.860 --> 01:08:40.980
Can you explain to me?


01:08:40.980 --> 01:08:43.300
No, it's like, the explanation is on,


01:08:43.300 --> 01:08:45.900
can you just summarize that in 300, 100 words?


01:08:45.900 --> 01:08:47.340
- Yeah, let's go step by step.


01:08:47.340 --> 01:08:48.780
Let's think step by step.


01:08:48.780 --> 01:08:50.180
What's this do?


01:08:50.180 --> 01:08:52.560
- Well, yeah, maybe a closing statement is like,


01:08:52.560 --> 01:08:54.780
this stuff is changing our world.


01:08:54.780 --> 01:08:56.740
Like for me, I'm interested in how it's changing,


01:08:56.740 --> 01:08:58.700
how we're building products, you know?


01:08:58.700 --> 01:09:02.900
But the core thing is that as a data practitioner,


01:09:02.900 --> 01:09:05.260
as a Python expert, as a programmer,


01:09:05.260 --> 01:09:09.020
it's really changing the way people work day after day


01:09:09.020 --> 01:09:12.700
faster than we all think.


01:09:12.700 --> 01:09:14.020
And across a broad, like, you know,


01:09:14.020 --> 01:09:15.060
you might understand pretty well,


01:09:15.060 --> 01:09:19.020
it's changing your daily workflow as a software engineer,


01:09:19.020 --> 01:09:22.420
but it's changing people's workflow to do chemistry


01:09:22.420 --> 01:09:27.420
or like in every field, there's a lot we can leverage here


01:09:27.420 --> 01:09:28.860
if you use it well.


01:09:28.860 --> 01:09:31.260
- Right, take this idea and apply it to,


01:09:31.260 --> 01:09:34.380
whatever vertical you wanna think of,


01:09:34.380 --> 01:09:36.260
it's doing the same thing there, right?


01:09:36.260 --> 01:09:37.460
- 100%. - Medicine, all over,


01:09:37.460 --> 01:09:39.540
yeah, 100%, 100%.


01:09:39.540 --> 01:09:43.020
All right, well, let's call it a wrap.


01:09:43.020 --> 01:09:45.380
I think we're out of time here.


01:09:45.380 --> 01:09:49.220
So really quick before we quit,


01:09:49.220 --> 01:09:50.580
PyPI package to recommend,


01:09:50.580 --> 01:09:52.980
maybe something AI related that you found recently,


01:09:52.980 --> 01:09:55.060
like all this thing's cool, people should check it out.


01:09:55.060 --> 01:09:56.940
- Promptimize, I think it would be,


01:09:56.940 --> 01:09:57.900
you know, something to check out.


01:09:57.900 --> 01:09:59.540
I think there's something called future tools


01:09:59.540 --> 01:10:01.140
that you could try to navigate there,


01:10:01.140 --> 01:10:04.500
but it shows like all of the AI powered tools


01:10:04.500 --> 01:10:07.180
that are coming out and it's hard to keep up.


01:10:07.180 --> 01:10:09.060
- Yeah, yeah, I think I have seen that, yeah.


01:10:09.060 --> 01:10:12.020
- And then if you wanna keep up on a daily


01:10:12.020 --> 01:10:16.020
with what's happening in AI, there's TLDR AI,


01:10:16.020 --> 01:10:19.740
they have like a DL with their relevant list for the day.


01:10:19.740 --> 01:10:22.260
I think that's a, it's, it's hard to stay on.


01:10:22.260 --> 01:10:25.260
I prefer like their weekly digestible.


01:10:25.260 --> 01:10:26.260
What's going on in AI.


01:10:26.260 --> 01:10:30.660
- It's too much of a, just a stream of information.


01:10:30.660 --> 01:10:32.300
- Yeah, it's just get it dizzying.


01:10:32.300 --> 01:10:34.340
And it's like, oh, this new model does this.


01:10:34.340 --> 01:10:35.700
Like I've got to change everything to that.


01:10:35.700 --> 01:10:40.180
And then something else, do you update the correct course


01:10:40.180 --> 01:10:42.900
too often and it's just like, you know, do not think


01:10:42.900 --> 01:10:45.780
because the foundation, the foundation is shifting


01:10:45.780 --> 01:10:47.660
too fast under you, so.


01:10:47.660 --> 01:10:48.740
- Yeah, absolutely.


01:10:48.740 --> 01:10:49.580
Well, very cool.


01:10:49.580 --> 01:10:52.820
And then final question, you write the Python code,


01:10:52.820 --> 01:10:54.320
what editor are you using these days?


01:10:54.320 --> 01:10:55.820
I'm a Vim user.


01:10:55.820 --> 01:10:57.860
Yeah, I know it's not the best.


01:10:57.860 --> 01:11:01.020
I know all the limitation, but it's like muscle memory.


01:11:01.020 --> 01:11:05.020
And I'm a UX guy now working on SuperSense.


01:11:05.020 --> 01:11:09.320
I do appreciate the development and all the new IDEs


01:11:09.320 --> 01:11:11.220
and the functionality that they have.


01:11:11.220 --> 01:11:12.920
I think it's amazing.


01:11:12.920 --> 01:11:15.120
It's just like, for me, it's all,


01:11:15.120 --> 01:11:17.720
like I know all my bash commands and big commands.


01:11:17.720 --> 01:11:20.720
>> Absolutely. All right. Well, Max, thanks for coming on the show,


01:11:20.720 --> 01:11:25.600
helping everyone explore this wild new frontier of AI and large language models.


01:11:25.600 --> 01:11:28.240
>> Yeah, well, we're exploring,


01:11:28.240 --> 01:11:32.720
but we're still relevant because I don't know how long we're going to be relevant for.


01:11:32.720 --> 01:11:36.880
>> Yeah, enjoy it while we can, get out there.


01:11:36.880 --> 01:11:40.680
Either control the robots or be controlled by them.


01:11:40.680 --> 01:11:44.360
Get on the right side of that. All right. Thanks again.


01:11:44.360 --> 01:11:48.360
This has been another episode of Talk Python to Me.


01:11:48.360 --> 01:11:52.360
Thank you to our sponsors. Be sure to check out what they're offering. It really helps support the show.


01:11:52.360 --> 01:11:56.360
The folks over at JetBrains encourage you to get work done


01:11:56.360 --> 01:12:00.360
with PyCharm. PyCharm Professional understands complex projects


01:12:00.360 --> 01:12:04.360
across multiple languages and technologies, so you can stay productive


01:12:04.360 --> 01:12:08.360
while you're writing Python code and other code like HTML or SQL.


01:12:08.360 --> 01:12:12.360
Download your free trial at talkpython.fm/donewithpycharm


01:12:12.360 --> 01:12:14.360
done with PyCharm.


01:12:14.360 --> 01:12:16.680
Listen to an episode of Compiler,


01:12:16.680 --> 01:12:18.800
an original podcast from Red Hat.


01:12:18.800 --> 01:12:21.400
Compiler unravels industry topics, trends,


01:12:21.400 --> 01:12:23.880
and things you've always wanted to know about tech


01:12:23.880 --> 01:12:26.320
through interviews with the people who know it best.


01:12:26.320 --> 01:12:30.800
Subscribe today by following talkpython.fm/compiler.


01:12:30.800 --> 01:12:32.120
Want to level up your Python?


01:12:32.120 --> 01:12:33.880
We have one of the largest catalogs


01:12:33.880 --> 01:12:36.240
of Python video courses over at Talk Python.


01:12:36.240 --> 01:12:38.280
Our content ranges from true beginners


01:12:38.280 --> 01:12:41.280
to deeply advanced topics like memory and async.


01:12:41.280 --> 01:12:43.940
And best of all, there's not a subscription in sight.


01:12:43.940 --> 01:12:46.980
Check it out for yourself at training.talkpython.fm.


01:12:46.980 --> 01:12:48.580
Be sure to subscribe to the show,


01:12:48.580 --> 01:12:51.600
open your favorite podcast app, and search for Python.


01:12:51.600 --> 01:12:52.980
We should be right at the top.


01:12:52.980 --> 01:12:55.860
You can also find the iTunes feed at /iTunes,


01:12:55.860 --> 01:12:58.020
the Google Play feed at /play,


01:12:58.020 --> 01:13:02.060
and the Direct RSS feed at /rss on talkpython.fm.


01:13:02.060 --> 01:13:05.580
We're live streaming most of our recordings these days.


01:13:05.580 --> 01:13:06.720
If you want to be part of the show


01:13:06.720 --> 01:13:08.980
and have your comments featured on the air,


01:13:08.980 --> 01:13:10.860
be sure to subscribe to our YouTube channel


01:13:10.860 --> 01:13:13.860
at talkpython.fm/youtube.


01:13:13.860 --> 01:13:15.280
This is your host, Michael Kennedy.


01:13:15.280 --> 01:13:16.440
Thanks so much for listening.


01:13:16.440 --> 01:13:17.680
I really appreciate it.


01:13:17.680 --> 01:13:19.900
Now get out there and write some Python code.


01:13:19.900 --> 01:13:22.480
(upbeat music)


01:13:22.480 --> 01:13:37.480
[Music]


01:13:37.480 --> 01:13:40.060
(upbeat music)


01:13:40.060 --> 01:13:50.060
[BLANK_AUDIO]

