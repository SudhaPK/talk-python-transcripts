00:00:00 - When you run your code in the cloud, how much do you know about where it runs?

00:00:04 I mean, the hardware it runs on and the data center it runs in.

00:00:08 There are just a couple of hyperscale cloud providers in the world.

00:00:11 This episode is a unique chance to get a deep look inside one of them, Microsoft Azure.

00:00:17 Azure is comprised of over 200 physical data centers with hundreds of thousands of servers in each one of those.

00:00:25 A look at how code runs on them is fascinating.

00:00:28 Our guide for this journey will be Mark Russinovich.

00:00:30 Mark is the CTO of Microsoft Azure and a technical fellow, Microsoft's senior most technical position.

00:00:37 He's also a bit of a programming hero of mine.

00:00:39 Even if you don't host your code in the cloud, I think you'll enjoy this conversation.

00:00:43 Let's dive in.

00:00:44 This is "Talk Python to Me," episode 445, recorded on-site at Microsoft Ignite in Seattle, November 16th, 2023.

00:00:53 (upbeat music)

00:00:58 Welcome to "Talk Python to Me," a weekly podcast on Python.

00:01:11 This is your host, Michael Kennedy.

00:01:12 Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.

00:01:20 Keep up with the show and listen to over seven years of past episodes @talkpython.fm.

00:01:25 We've started streaming most of our episodes live on YouTube.

00:01:29 Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:36 This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:42 Publish, share, and deploy all of your data projects that you're creating using Python.

00:01:46 Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Reports, Dashboards, and APIs.

00:01:52 Posit Connect supports all of them.

00:01:54 Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:02:00 And it's brought to you by the PyBytes Developer Mindset Program.

00:02:04 PyBytes' core mission is to help you break the vicious cycle of tutorial paralysis through developing real-world applications.

00:02:10 The PyBytes Developer Mindset Program will help you build the confidence you need to become a highly effective developer.

00:02:17 Check it out at talkpython.fm/pdm.

00:02:20 Mark, welcome to "Talk Python" with me.

00:02:23 - Thanks, thanks, Michael.

00:02:24 - Yeah, it's fantastic to have you here.

00:02:26 I've been a fan of your work for a really long time.

00:02:29 We're gonna have a really cool look inside of Azure.

00:02:31 And there's not very many hyperscale clouds in the world.

00:02:35 You can probably count 'em on your hands, right?

00:02:36 And so I think as developers, Python developers generally, we'll be really interested to just kind of get a sense of, when we run on the cloud, what exactly does that mean?

00:02:46 Because it's been a journey.

00:02:47 - Yeah, for sure.

00:02:48 - Yeah, before we dive into that, though, and some other cool things you're up to, just tell people a bit about yourself.

00:02:54 - Sure, I'm a CTO and technical fellow at Microsoft, CTO of Azure.

00:02:58 I've been in Azure since 2010.

00:02:59 Prior to that, I was in Windows.

00:03:01 I joined Microsoft in 2006 when it acquired my software company and my freeware website, Winternals and Sysinternals, respectively.

00:03:08 And since 2010, I've effectively been in the same role the entire time, which is overseeing technical strategy and architecture for the Azure platform.

00:03:16 - And the skill of that is quite something.

00:03:18 So it'll be great to get in that.

00:03:20 That's awesome.

00:03:21 You first came on my radar probably in the late '90s, early aughts, through the Sysinternals thing, not through Microsoft.

00:03:28 And you brought that up, so tell people a bit about Sysinternals.

00:03:30 It was like, if you wanted to see inside your, what your app is doing on Windows, you go to Sysinternals, right?

00:03:36 Tell us about that.

00:03:37 - Yeah, Sysinternals grew out of my just love of understanding the way things work.

00:03:40 And I was doing a lot of work on Windows internals, actually, in my PhD program, where I was trying to figure out how to get operating systems to be able to save their state and then come back in case of a failure.

00:03:54 So I learned the internals of Windows 3.1 and then Windows 95 and then Windows NT and started to think about cool ways that I could understand the way things worked underneath the hood.

00:04:05 So actually, the first Sysinternals tool was something called Control to Cap, which swaps the caps lock and key, the control key, because I came from a Unix background.

00:04:15 - And who needs caps lock?

00:04:15 - Yeah, exactly.

00:04:16 - No one should be that dumb.

00:04:17 - I'm not yelling at people very much.

00:04:20 So the second tool that I wrote was actually called NTFS DOS, to bring NTFS to DOS, but the native Windows NT tools that Bryce Cogswell, who I met in grad school, and I co-wrote together, were RegMon and FileMon.

00:04:34 They were like the originals.

00:04:35 And RegMon allowed you to watch registry activity, FileMon, file system activity.

00:04:39 We later merged them into Process Monitor after we joined Microsoft.

00:04:43 But we decided to make those tools available for free and so, and available, so we started the ntinternals.com website, which then, Microsoft's lawyers said, don't use NT, so we switched over to renaming it Sysinternals.

00:04:57 And then Bryce was like, hey, some of the tools that we've made, we should sell.

00:05:01 So we wrote a tool that would allow you to mount a dead NT system through a serial cable, as if it was a local drive on the recovery system.

00:05:08 And he said, if we make a rewrite version, we should sell.

00:05:11 So he started, he went and set up a credit account, you know, credit card account on eCommerce.

00:05:17 - Authorized.net or something like that, it was weird APIs, yeah.

00:05:20 - We started selling the software and that grew into what became Winternals, a commercial software company.

00:05:25 But Sysinternals and Winternals, like I said, were both acquired at the time.

00:05:29 I joined Microsoft in 2006.

00:05:31 But Bryce and I continued to work on Sysinternals.

00:05:33 He worked on them until he retired from Microsoft and retired just in general four years later.

00:05:39 And then I've continued.

00:05:40 I have now a couple people, three people working on Sysinternals engineering systems, keeping them healthy and the build pipelines working and then adding features to them.

00:05:49 And I still code on them and still add features.

00:05:52 Like just in a Zoomit release a few days ago, I added some, I blur and highlight to Zoomit.

00:05:57 - Yeah, people are doing presentations on Windows.

00:05:59 With Zoomit, you can say, let me quickly draw on the screen, not PowerPoint, but just whatever happens to be on your screen, which is really nice.

00:06:06 I love my Macs these days, but boy, I wish Zoomit existed on the Mac.

00:06:09 - People have asked for Zoomit for Mac and I'd like to make a Zoomit for Mac.

00:06:13 Now with Copilot, I wonder how good it is at writing Mac apps, because I don't want to spend all the time to learn how to write a Mac apps just to write Zoomit, but if Copilot can help, maybe it'll, you know, something that I can do in my spare time.

00:06:26 - I don't know if it'll do it, but it'll get you close.

00:06:27 It's crazy how these LLMs are writing code for us these days.

00:06:31 And we're going to talk a bit about maybe how some of those run and so on.

00:06:34 I mean, Azure is doing tons of stuff with large language models and you all have some, you know, we're here at the Microsoft Ignite conference.

00:06:42 You've got some big announcements.

00:06:43 I was such a fan still of Sysmon and I use that for all sorts of things still.

00:06:49 So super cool.

00:06:50 Now, before we jump in, I kind of want to talk about some of your big announcements because they really caught me off guard here.

00:06:56 I'm like, yes, this is exciting, but maybe since we're going to talk a decent amount about Azure, the internals of hardware, how our code runs, just give us a quick history of Azure.

00:07:05 You know, when did this whole thing get started?

00:07:06 - Azure started right as I joined Microsoft in 2006, there was a group of people, including Dave Cutler, one of the people that I've looked up to because Dave was the original architect behind the VMS operating system and then Windows NT, which is now underlying Windows.

00:07:21 He and some other people were just at the suggestion of Ray Ozzie.

00:07:26 This is back when services was a big thing and Ray sent the memo to the company, kind of echoing Bill Gates' internet memo saying, it's software and services now.

00:07:36 And they said, how can we build a data center scale type platform to make it easier for Microsoft to develop services?

00:07:43 And so this was called Project Red Dog, which was incubating for a while.

00:07:47 And then 2008, they publicly launched it 'cause Steve said, we need to make this available to third parties as well as Windows Azure.

00:07:54 And, sorry, 2008 they announced the preview of it.

00:07:57 2010, it commercially launched publicly in February and I joined in July.

00:08:02 And a few years later, with the rise of open source software and so many enterprise customers wanting to have Linux, we re-branded it Microsoft Azure.

00:08:11 And we also--

00:08:12 - Can I run Linux on Windows Azure?

00:08:14 I don't know if that makes any sense.

00:08:15 - Yeah, and one of the first things I'd done, worked on with Corey Sanders was being asked, hey, we've got platform as a service.

00:08:22 We have this thing called cloud services, this model for how you write apps.

00:08:25 But our enterprise customers were saying, I can't move my existing IT stuff to Azure because it just needs VMs.

00:08:33 And so the first thing we did was, hey, we should get IaaS capability in Azure.

00:08:38 And so in 2012, we launched the preview of IaaS for Azure.

00:08:42 And that's really when the business started to take off because enterprises then could, with minimal effort, start to move.

00:08:46 - Oh, well, that's like doing what we do in the--

00:08:49 - Yeah, in their--

00:08:50 - Our data center, but in your data center.

00:08:51 Yeah, exactly.

00:08:52 Now no one even thinks about it.

00:08:54 - Exactly.

00:08:54 So IaaS has continued to develop, PaaS has continued to develop.

00:08:58 Cloud services was designed in a world without containers.

00:09:01 Now we've got containerization, the rise of Kubernetes, and then application models on top of containers.

00:09:06 And so Azure's evolved.

00:09:08 It actually, I think, led some of that evolution of cloud-native computing up into containers and abstractions.

00:09:13 But it's been a long, long journey towards that.

00:09:15 I mean, I think one of the things is I've always believed that ultimately cloud should be about making it easy for developers to say, here's what I want, and then the cloud takes care of the rest.

00:09:25 And we're moving towards it relentlessly, that time when you'll really be able to do that.

00:09:29 - Yeah, so you don't have to know DevOps.

00:09:32 You don't have to know distributed architectures.

00:09:34 You just, give you guys a go.

00:09:37 Yeah, which is beautiful, it's beautiful.

00:09:39 Now, real quickly, just give us a scale.

00:09:41 Like, think of how many data centers, how many servers, how many miles, fiber.

00:09:47 It's kind of astonishing.

00:09:48 - It is pretty flabbergasting.

00:09:50 And the numbers continue to grow exponentially.

00:09:53 I'll just give you, 'cause I remember when I first started at Azure, I was asked to give a talk at the Azure All Hands about architecture and some of the announcements we had coming.

00:10:02 And the All Hands was two rooms with partition removed in our on-campus conference room meeting center.

00:10:11 Total about 500 people.

00:10:13 That was all of the Azure team in 2010.

00:10:15 And really, nobody outside the Azure team knew anything about Azure.

00:10:19 Really, the world didn't know about Azure.

00:10:19 - It was kind of a secret even inside, right?

00:10:21 - Inside, yeah.

00:10:22 So effectively, that was like most, at least half the people in the world that knew anything about Azure was in those two rooms.

00:10:28 And today, Scott Guthrie's organization, Cloud and AI, all of it's working on Azure.

00:10:34 And that's tens of thousands of people.

00:10:36 At least a good percentage, a majority percentage even, of the company is working directly on things that come under the Azure umbrella.

00:10:43 So it's come a long way from that perspective.

00:10:45 And you talked about physical scale.

00:10:47 Back then, when we originally launched Azure in two regions, it was like 40,000 servers, like 20,000 in one, 20,000 in the other.

00:10:54 - That's still a lot of servers.

00:10:55 - Yeah, that is kind of cloud scale back then.

00:10:57 Now, we are at millions of servers.

00:11:00 And when it comes to data centers, we've got 60 regions around the world, 60 plus regions.

00:11:05 And each of those consists of one, in many cases, multiple data centers.

00:11:10 And we're still building out.

00:11:12 We're launching a data, like two data centers every week, I think is the number that we're launching.

00:11:16 - Wow, that's crazy.

00:11:18 And these could be slotted into one of these regions or it could be something totally new, yeah.

00:11:22 Incredible.

00:11:23 This portion of Talk Python to Me is brought to you by Posit, the makers of Shiny, formerly RStudio, and especially Shiny for Python.

00:11:33 Let me ask you a question.

00:11:34 Are you building awesome things?

00:11:36 Of course you are.

00:11:37 You're a developer or a data scientist.

00:11:38 That's what we do.

00:11:39 And you should check out Posit Connect.

00:11:42 Posit Connect is a way for you to publish, share, and deploy all the data products that you're building using Python.

00:11:49 People ask me the same question all the time.

00:11:52 Michael, I have some cool data science project or notebook that I built.

00:11:55 How do I share it with my users, stakeholders, teammates?

00:11:58 Do I need to learn FastAPI or Flask or maybe Vue or React.js?

00:12:03 Hold on now.

00:12:04 Those are cool technologies and I'm sure you'd benefit from them, but maybe stay focused on the data project.

00:12:09 Let Posit Connect handle that side of things.

00:12:12 With Posit Connect, you can rapidly and securely deploy the things you build in Python.

00:12:16 Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quadro, Reports, Dashboards, and APIs.

00:12:23 Posit Connect supports all of them.

00:12:25 And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise requirements.

00:12:31 Make deployment the easiest step in your workflow with Posit Connect.

00:12:35 For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm/posit.

00:12:41 That's talkpython.fm/P-O-S-I-T.

00:12:45 The link is in your podcast player show notes.

00:12:47 Thank you to the team at Posit for supporting Talk Python.

00:12:50 So the big announcement that I wanted to ask you about, just before we run out of time, and then we'll dive into some of that, that sort of how does your code run story, is Azure Cobalt.

00:13:01 That's a new processor you guys announced.

00:13:03 And, you know, listeners know I'm a big fan of Apple Silicon and how it sort of changed the computing landscape for power and speed on like little laptops and stuff.

00:13:11 And this is kind of that idea, but for the data center, right?

00:13:14 Tell us about that.

00:13:15 - It is that idea.

00:13:16 I think having a processor that can be designed really with our specifications.

00:13:21 If you take a look at Intel and AMD processors, they're fantastic processors.

00:13:25 They're very versatile.

00:13:26 They're taking requirements from lots of different sources.

00:13:30 And so we're just, we're a voice, we're a significant voice when it comes to saying, we'd like your processors to do these things.

00:13:37 When we have our own, we've got the ability to just decide unilaterally what we'd like it to do based off of what we see and convert it or integrate it into our systems.

00:13:45 We can put it on SSEs and integrate it with memory and GPUs.

00:13:49 And so that is kind of the reason that we've done that verticalization for processors.

00:13:55 That's not to say that the other processors aren't going to be significant.

00:13:59 - It's gonna be probably a blend of these.

00:14:00 - It's gonna be a blend.

00:14:01 They'll have different capabilities that ours won't have.

00:14:03 There are customers that want the specific features that they've got or performance speeds and feeds that they've got, 'cause they're not all gonna look the same.

00:14:11 And so I think it's just better optionality for everybody.

00:14:14 - Well, I can tell you as somebody who tries to run Linux on that thing, it's hit and miss if there's even an ARM version available.

00:14:22 More often than not, there's not.

00:14:23 And so there's certainly not gonna be an insane rush to just drop everything, 'cause there's a lot of code that's written for x86.

00:14:31 - And optimized, that's the other thing too.

00:14:32 - For sure.

00:14:33 But yeah, so on my list here of things I was gonna ask you is, well, what about ARM and the data center?

00:14:37 - Yeah, that's ARM.

00:14:38 - No, exactly.

00:14:39 And I'm like, well, okay, so you guys beat me to the punch.

00:14:42 Exactly, so awesome.

00:14:44 Now, one of the things I wanted to kind of maybe have you go through for listeners that I think is just super interesting is sort of the evolution of the hardware, where our code runs.

00:14:53 You talked about it in some of your talks, like the data center generations, and what is that, like six or seven, maybe different variations.

00:15:00 And I'll kind of give you some prompts from them in all.

00:15:02 But one of the things that I was thinking about when I was looking at this is, do you have a bunch of small servers, or do you partition up really large servers, right?

00:15:12 What's the right flow for that?

00:15:14 - One of the things that you've seen since the start of cloud, back when we launched Azure, there was one server type.

00:15:21 We had different virtual machines offerings, but they were just all different sizes that could fit on that one server.

00:15:27 It was a 32-core Dell Opteron with, I think, 32 gig of RAM.

00:15:31 And so that was the server back then.

00:15:34 What we've seen is more workloads come to the cloud that have different requirements.

00:15:38 Some require large memory, some require more compute, some require GPUs, some require InfiniBand, back in networking for each high-performance computing.

00:15:46 And so there's been a drastic diversification of the server hardware in Azure that's being offered and current at any one point in time.

00:15:55 And I think you'll continue to see that.

00:15:57 So the old, you know, it's just a pizza box, it's a low-end commodity server, kind of that's the cloud vision back in 2010.

00:16:06 Now it's the cloud contains specialized servers for specific applications.

00:16:11 And when it comes to large servers, back in 2014, we started to introduce very large servers, the kind that, you know, people that were cloud purists back in 2010 would have been like, "No, don't allow that." >> It's all about the cheap and the--

00:16:24 >> Yeah, it's all about the cheap and scale up.

00:16:26 It's scale up servers for SAP workloads in memory database workloads.

00:16:30 So we introduced a machine that we nicknamed Godzilla, which had 512 gig of RAM in 2014, which was like an astonishing number.

00:16:40 And we've continued to, SAP workloads have gotten bigger and bigger and more has migrated to the cloud, created bigger and bigger and bigger and bigger machines.

00:16:48 In fact, I'm showing here at Ignite, the latest generation of the SAP scale up machines that we're offering.

00:16:53 It's not yet, they're not yet public, but I'm going to show a demo of one of them that I'm calling, nicknaming, Super Mega Godzilla Beast, 'cause we've gone through so many iterations.

00:17:03 So this one is super is the new--

00:17:05 >> Yeah, you're running low on adjectives here.

00:17:08 >> And I don't know what I'll come up with next.

00:17:09 But anyway, we're at Super Mega Godzilla Beast as the current generation, which has 1790 cores and 32 terabytes of RAM, 32 terabytes of RAM.

00:17:20 >> Incredible.

00:17:20 So do you do things to like pin VMs to certain cores so that they get better cache hits and stuff like that, rather than let it just kind of mosh around 1790 cores?

00:17:30 >> Yeah, that's especially important with NUMA architectures, where you've got memory that has different latencies to different sockets, 'cause you want to have the VM that's using certain cores on a socket have memory that is close to it, close to that socket.

00:17:44 So that's, and that's just part of Hyper-V scheduling, is doing that kind of assignment, which we have under the hood.

00:17:50 And again, it's like the control plane at the very top says launch a virtual machine of this size, of this skew.

00:17:56 Then there's a resource manager, the Azure allocator, that goes and figures out this is the best server to put that on, that has enough space, and will reduce, minimize fragmentation.

00:18:05 And places it on there, and then Hyper-V underneath is saying, okay, these are the cores to assign it to.

00:18:11 Here's the RAM to give it.

00:18:12 >> Excellent, and how much of that can you ask for?

00:18:15 >> You can ask for the whole machine.

00:18:15 >> You could, you could, if you--

00:18:16 >> You could, for a full machine, full server virtual machine sizes.

00:18:20 >> Wow, okay.

00:18:21 There's probably not too many of those in, but some people using them.

00:18:24 >> Like on the SSAP ones, because they're designed for SAP, I think for those kinds of, the current generations, I think we offer just two sizes, like either half of it, or the whole thing.

00:18:34 >> Incredible. >> Yeah.

00:18:35 >> Wow, okay.

00:18:36 How much of a chunk of a rack does that take?

00:18:38 >> It's basically--

00:18:39 >> It's a whole rack?

00:18:40 >> Yeah, pretty much, yeah.

00:18:41 >> Just top to bottom?

00:18:42 >> Yeah, it's like a 10 kilowatt server.

00:18:44 >> Yeah, a little power plant inside there.

00:18:46 As you kind of talk through the history of sort of how your code ran, it was more, more colo, as you said, like the more smaller, smaller ones.

00:18:53 And then, as you got bigger and bigger on some of this, you started working on things like, well, how do we let the servers run hotter, and have the air cool them, rather than more actively cooled?

00:19:04 And then it even gets to a, almost more, just remove big chunks of it, and let them fail, and then when enough of it has failed, take them out.

00:19:12 You want to kind of talk about--

00:19:13 >> Yeah, so, we're still, yeah, good question, 'cause we're still exploring this space towards higher efficiency, lower energy consumption, more sustainable.

00:19:22 One of the experiments that came out of Microsoft Research was Project Natick, which is taking a bunch of servers, putting it, or a rack of servers, putting it in a container that has nitrous oxide gas in it, so it's an inert gas, and dropping it into the ocean floor, and letting it be cooled ambiently through the water there.

00:19:40 >> Not water on the inside, but the outside of the container.

00:19:42 >> Outside of the container, yeah.

00:19:43 >> It's a giant heat sink, kind of.

00:19:44 >> Yeah, and there's potential benefits of that, and it's still something that might get revived at some point, but what we found coming out of that was, if the parts are in an inert environment, they have one-eighth the failure rates as ones that are in an air environment, and with particulate matter, and corrosive materials in the air.

00:20:02 We started exploring liquid cooling, both for that, as well as potential energy savings, and more sustainable cooling than air-cooled.

00:20:10 We've explored two-phase liquid immersion, we had a pilot running, there's some regulations that have changed around the kinds of fluids that have made us take a look at a different direction, so we--

00:20:20 >> Is that kind of like the, what you would get in an air conditioner, or some of the stuff they had replaced--

00:20:25 >> They're called the forever chemicals, or PFAS materials.

00:20:28 The ones we're using actually aren't, but the regulation is a little broad, and so we're just steering clear, and it might be revisited at some point, but we're also, they have been exploring liquid cooling, kind of traditional liquid cooling, cold plate cold, and some people listening, probably like me, are gamers, and have liquid-cooled GPUs, or CPUs at home in their gaming rigs, which allow them to get overclocked, and it's the same thing we're doing in our data centers.

00:20:52 In fact, one of the things Satya showed in the keynote was something called Sidekick, which is a cabinet that allows us to take liquid, cold plate liquid cooling, into an existing data center, air-cooled data center.

00:21:03 >> Right, okay.

00:21:04 >> Where Maya 100 AI accelerators are in the cabinet, sitting right next to it, and the cooling pipes are going into the Maya cabinet to cool the accelerators themselves.

00:21:16 >> So they mounted some big metal plate, and then the metal plate is liquid-cooled, or something like that, or?

00:21:21 >> Yeah, it's effectively that there's a plate on top of the processor, and then liquid is going through that.

00:21:25 So I'm gonna actually show pictures of the inside of the Maya system tomorrow in my AI innovation closing keynote.

00:21:32 But that is, I think the takeaway here is that at the scale we're at, and with the efficiency gains that you might get from even a few percentage, we're exploring everything at the same time.

00:21:42 Like single phase liquid immersion cooling, still exploring that, and then how to do cold plate more efficiently.

00:21:48 Also be showing something called microfluidics we're exploring, which is much more efficient than just pure liquid cold plate.

00:21:55 Which, a cold plate is just putting the plate, like you just said, right on top of the processor.

00:21:59 And so the water's taking the heat away.

00:22:03 But if we can put the liquid right into the processor, like-- >> Are we talking channels in the processor? >> Like channels around the processor, yeah.

00:22:11 Just flow it right on top of it.

00:22:13 And so that's something we're calling microfluidics.

00:22:15 I'll show that and talk a little bit about that tomorrow too.

00:22:18 Offers much more efficient cooling.

00:22:20 And it's not prime time yet, but it looks incredibly promising.

00:22:24 >> That looks awesome.

00:22:25 One of the things I saw in the opening keynote, I don't know if it fits into what you were just talking about or if it's also another things where actually had the whole motherboard submerged and then even just the entire computer just underwater.

00:22:37 >> So that was two phase liquid immersion cooling like I mentioned, is you're just dunking the whole thing in the dielectric fluid.

00:22:42 >> And you had it boil at a low temperature, I guess, 'cause the phase change is like extremely energy intense, aka heat exchange type of thing.

00:22:50 >> It's actually two phase because of the boiling.

00:22:52 It goes, it phase changes into gas and then condenses again back into liquid.

00:22:57 So that was the idea behind two phases.

00:23:00 >> I see, instead of just running a radiator, you almost condense it back somewhere else and then bring it back around.

00:23:05 Okay, cool.

00:23:07 So if we go and run our Python codes, whether it's pass or I has or whatever, what's the chance it's hitting that or is this kind of cutting edge stuff reserved for high energy AI training?

00:23:18 Is it more like if we ask ChatGPT, it's liquid cooled.

00:23:22 >> Our standard data centers right now are air cooled.

00:23:24 So it's air cooled servers.

00:23:25 This Maya part is liquid cooled.

00:23:27 So when that, in our first summer, we've got some of our own workloads now starting to leverage Maya.

00:23:32 >> Do you put on your own workloads first, just in case?

00:23:34 >> Yeah, well it's just to prove it.

00:23:36 >> Shake it out, yeah.

00:23:37 >> Yeah, it's good, I'm piloting it.

00:23:39 >> Yeah, 'cause you're not offering that up to the big customers just right away.

00:23:43 And so Maya, I don't know how many people know about this.

00:23:47 Either this is one of the GPU training systems you guys have.

00:23:51 And for those who don't know, OpenAI and ChatGPT run on Azure, which probably takes a couple of cores, a couple of GPUs to make happen.

00:23:59 You want to talk about that?

00:24:00 >> Right now our fleet, our large scale AI supercomputing fleet is made up of NVIDIA parts.

00:24:06 So the previous generation was, well the original generation that we trained GPT-3 on with OpenAI was NVIDIA V100s.

00:24:13 And then we introduced A100s, which is what GPT-4 was trained on.

00:24:16 >> And these are graphics cards, like 4080s or something, but specifically for AI, right?

00:24:21 Okay.

00:24:22 >> The current generation of supercomputer we're building for OpenAI training, their next generation of their model, that's NVIDIA H100 GPUs.

00:24:29 Then Maya is our own custom AI accelerator.

00:24:32 So it's not a GPU.

00:24:33 You know, one of the aspects of NVIDIA's parts has been their GPU base.

00:24:38 So they also can do texture mapping, for example.

00:24:42 But you don't need that if you're just doing pure AI workloads.

00:24:45 >> Back to that specialization, right?

00:24:46 Like, so if you could build it just for the one thing, maybe you'd build it slightly differently?

00:24:50 >> That's right.

00:24:51 >> Okay.

00:24:52 >> So Maya is just designed purely for matrix operations used in, in fact, low precision matrix operations used for AI training and inference.

00:24:59 And so that is the specialized part that we've created called Maya 100, the first generation of that.

00:25:06 >> This portion of Talk Python to Me is brought to you by the PyBytes Python Developer Mindset Program.

00:25:14 It's run by my two friends and frequent guests, Bob Delderbos and Julian Sequeira.

00:25:19 And instead of me telling you about it, let's hear them describe their program.

00:25:23 >> Happy New Year.

00:25:24 As we step into 2024, it's time to reflect.

00:25:28 Think back to last year.

00:25:29 What did you achieve with Python?

00:25:31 If you're feeling like you haven't made the progress you wanted and procrastination got the best of you, it's not too late.

00:25:37 This year can be different.

00:25:38 This year can be your year of Python mastery.

00:25:42 At PyBytes, we understand the journey of learning Python.

00:25:46 Our coaching program is tailor-made to help you break through barriers and truly excel.

00:25:52 Don't let another year slip by with unmet goals.

00:25:55 Join us at PyBytes and let's make 2024 the year you conquer Python.

00:26:00 Check out PDM Today, our flagship coaching program, and let's chat about your Python journey.

00:26:06 >> Apply for the Python Developer Mindset today.

00:26:09 It's quick and free to apply.

00:26:12 The link is in your podcast player show notes.

00:26:14 Thanks to PyBytes for sponsoring the show.

00:26:16 If I think of some of the stuff presented at the opening keynote and stuff here, I think the word AI was said a record number of times.

00:26:27 >> Yeah, I think there was a topic there where I wasn't a part of.

00:26:30 >> So how much is this changing things for you guys?

00:26:32 Like 12 months ago or something, Chachi P. appeared on the scene and yeah.

00:26:37 >> It's literally changing everything.

00:26:38 You know, Jensen was saying this is the biggest thing since the internet.

00:26:41 >> Yeah, Jensen being the CEO of Embedded.

00:26:43 >> Yeah, who was on stage with Chachi at the keynote.

00:26:46 It is changing everything.

00:26:48 It's changing not just the product offerings.

00:26:51 So the way that we have to integrate AI into the products using Copilot, it's changing the way we develop the products as well and the way that we run our systems inside already.

00:27:00 So for example, incident management, we've got Copilot built, you know, our own Copilot internally built into that.

00:27:06 So somebody that's responding to an issue in our production systems can say, okay, so what's going on?

00:27:12 What's the, what happened with this?

00:27:13 Show me the graph of this, you know, just be able to use human language to get caught up in what's going on.

00:27:18 >> People tell me it's just statistics, just prediction.

00:27:20 I, boy, it doesn't feel like, it doesn't feel like prediction.

00:27:23 >> People that say that, I think, are missing the scale of the statistics.

00:27:28 And because if you--

00:27:29 >> We're probably predicting a little bit, like thinking about what are you gonna say next, what were you meaning?

00:27:33 >> Exactly, that's what we are, we're statistical.

00:27:34 >> Yeah, yeah.

00:27:35 >> So it's just once you get statistics at a large enough scale that you start to see something that looks like what we call intelligence.

00:27:42 >> It's really incredible.

00:27:43 I'm starting to use it to just write my Git commit logs for me, you know, push a button and it says, oh, you added error handling to the background task, so in case this fails, you'll be more resilient and it'll keep running like, that's better than I could have gotten.

00:27:55 (laughing)

00:27:56 Just thought that might crash, you know, and you just push the button and it's just--

00:28:00 >> It's magic, it's magical.

00:28:01 >> Yeah, it really is.

00:28:02 >> It's called Copilot for a reason because we're not at the point yet where you can just let it do what it does autonomously.

00:28:07 You need to check its work.

00:28:09 Like, you need to look at it and say, oops, you know, that time it screwed it up, it didn't get it quite right, or I need to add more context to this than it had, or it extracted, but as far as accelerating work, it's just a game changer.

00:28:21 >> Yeah, it really, really is.

00:28:22 So before we run out of time, I want to ask you just a couple more things, bit of a diversion.

00:28:26 So Python, we saw Python appear in the keynote.

00:28:29 They were showing off, I can't remember who, it wasn't Satya, it was whoever followed him, saying, look, we want to show off our new sharing of the insanely large GPUs for machine learning.

00:28:39 So let's just pull up some Python and a Jupyter Notebook and we'll just check that out.

00:28:42 And you're like, wait, where are we again?

00:28:44 Really interesting.

00:28:45 So you said you're using a little bit of Python yourself.

00:28:48 Like, what's Python look like in your world?

00:28:49 >> The reason why I'm using Python is I took a sabbatical this summer, and so I was like, I'm going to do some AI research.

00:28:55 So I got connected with an AI researcher.

00:28:56 In fact, I'm going to talk about this at my keynote tomorrow, some of the work that came out of it.

00:29:01 The, obviously, AI is completely Python.

00:29:04 >> Yeah, almost entirely, yeah.

00:29:05 >> So I spent the whole summer, and I still am spending my time in Python, Jupyter Notebooks, and then Python scripts when you want to do some run for final result.

00:29:15 I hadn't used really Python before, other than in passing.

00:29:18 I mean, it's a very large, it's very easy to pick up.

00:29:20 >> There's a t-shirt that says, I learned Python, it was a good weekend.

00:29:23 >> Yeah. >> It's a bit of a joke, but it's a good joke, you know?

00:29:25 >> I think that's what makes it so powerful, is that it's so easy to pick up.

00:29:28 But what's made it even easier for me to pick it up, I'd say that I'm a mediocre Python programmer, but I'm using Copilot.

00:29:35 >> Yeah.

00:29:36 >> And it's made me an expert Python coder.

00:29:37 >> It really has.

00:29:38 How do I do this?

00:29:39 >> I don't go to Stack Overflow for questions.

00:29:43 I haven't had to get a book on Python.

00:29:45 I basically just either ask Copilot explicitly, like, how do I do this, or write me this, or I put it in the function, or in the comment, and it gets it done for me.

00:29:54 >> Yeah.

00:29:55 >> And occasionally, I'll have to go hand edit it and figure out what's going on, but for the most part, it is writing almost all my code.

00:30:01 >> Yeah.

00:30:02 >> So my goal is, how can I just not have it write everything for me?

00:30:05 So that has kind of become the way that I program in Python, and I think Python and AI, the knowledge of Copilot for Python, because OpenAI, obviously, for their own purposes, has made GPT-4 and GPT-35 before it really know Python and Python--

00:30:21 >> I hadn't really thought of that connection, but of course, they wanted to answer Python questions, I'm sure.

00:30:25 >> When it comes to seeing what AI can do for programming, Python is at the forefront of that.

00:30:31 >> What was your impression of it?

00:30:33 I mean, I'm sure you've probably seen it before, but what's your impression of working in it, coming from a curly brace semicolon type language like C++ or something, 'cause they drop a bunch of parentheses, they have these tab space, these four spaces, rules and stuff.

00:30:46 >> You know, it's the YAML versus JSON.

00:30:48 >> It is kind of that debate, isn't it?

00:30:50 >> I mean, I've gotten used to it.

00:30:52 It's not a big deal.

00:30:53 I find it's less verbose than C.

00:30:56 There's less typing.

00:30:57 >> Less symbol noise, you just kind of get the essence, yeah.

00:31:00 >> Yeah.

00:31:01 >> Yeah, I kind of had that experience as well, coming from a C-based language.

00:31:03 I'm like, wow, this is really weird.

00:31:04 And then after I went back to C#, I'm like, this is also weird.

00:31:07 And I kind of like the clarity over here, so now what do I do with life?

00:31:11 >> And then you go back and like, semicolons are annoying now.

00:31:14 >> Yes, exactly.

00:31:15 I'm like, I thought they were needed.

00:31:16 They're not needed, what's going on?

00:31:18 >> Awesome, oh, that's really cool.

00:31:19 Another thing that I think maybe people would really enjoy hearing a bit about, and I'm a big fan of, you wrote a three-part series of novels about computer hackers called Zero Day.

00:31:31 Really good.

00:31:32 >> Thanks.

00:31:33 >> I read all of them back when they came out.

00:31:34 And so much of this computer mystery stuff is like, oh, they're using VB6, I'm going to get their IP address.

00:31:40 You're like, wait, what?

00:31:41 I mean, those words are meaningful, but the sense is not, right?

00:31:45 And your books are like a lot of sort of spy stuff, but also a lot of really cool, legit, reasonably possible computer stuff.

00:31:53 Yeah, tell people a quick bit about that, if they want to catch up.

00:31:56 >> I love thrillers growing up, techno thrillers.

00:31:59 I read Andromeda Strain when I was in seventh grade, and I was like, this book is so cool because it's like I'm learning science plus--

00:32:06 >> Yeah, exactly.

00:32:07 >> You know, it's really exciting.

00:32:09 So I've always wanted to write one, and then coming into the late 1990s, when you started to see some of these large-scale viruses, I was just thinking, this is such a powerful weapon for somebody to cause destruction.

00:32:22 And then 9/11 happened, and I'm like, all right, logical next step is leveraging a cyber weapon to do something with the same goals.

00:32:28 And so that's what led me to write Zero Day, which is taking that idea of using a cyber weapon for terrorism.

00:32:36 And then I was like, oh, that book was really well-received, I had a lot of fun doing it, so let me write the next one.

00:32:41 And I wanted to continue in this theme with the same characters, Darrell Hoggan and Jeff Akin, and say, what's something else?

00:32:48 What's another cyber security angle that I can take a look at in the second one?

00:32:53 So the second one was state-sponsored cyber espionage.

00:32:56 And actually, the ironic thing is, I'd already had Iran in the story, I'd already had China in the story, I had people trying to figure out how to get Iran a nuclear weapon, and then Stuxnet happened, right when I was still writing the book.

00:33:08 And I'm like, okay, this is a story part of my plot.

00:33:11 - Yeah, lining it up for you.

00:33:12 - So I had to change the book a little to acknowledge Stuxnet happening.

00:33:16 And then the third one was about insider threats, which I think is one of the toughest threats to deal with.

00:33:20 In this case, it was a long-range plot from some people that wanted to compromise a stock exchange and kind of a mixture of high-frequency trading and insider threat with cyber security systems, was the third one called Rogue Code.

00:33:33 - Yeah, so they were all really good, I really enjoyed it.

00:33:35 Were you a fan of Mr. Robot?

00:33:36 Did you ever watch that series?

00:33:37 - I did, I love that series.

00:33:39 - Yeah, oh my gosh.

00:33:40 Again, it seemed pretty plausible.

00:33:42 So I really like that.

00:33:44 Imagine a lot of people out there listening have seen Mr. Robot as well.

00:33:47 If they want that kind of idea, but in just a series, they can binge.

00:33:50 Yeah, cool.

00:33:51 Maybe we should wrap up our chat here with just a quick of some of the future things.

00:33:55 You talked about rapidly deploying some of these data centers and some of these Balar systems.

00:34:01 Maybe just give us a sense of like, even like disaggregated rack architecture.

00:34:05 Do you have, instead of having a GPU alongside a server, like a rack of GPUs and then optical connections to a rack of servers.

00:34:13 Like give us a sense of some of the stuff where it's going.

00:34:16 - Yeah, so that's some of the stuff that we're exploring.

00:34:18 Like I mentioned, we're taking a look at lots of different ways to re-architect the data center to be more efficient.

00:34:23 And one of the ways that you get efficient is by, in reduced fragmentation, is by having larger pools to allocate resources from.

00:34:30 If you think about allocating a virtual machine on a server, how much RAM can you give it at most?

00:34:34 Well, as much as sitting on the server.

00:34:36 How many GPUs can you attach to it?

00:34:38 Well, as most as are attached to that server.

00:34:40 - Right, how many PCI slots you got.

00:34:42 - But if you think about, I've got a large resource pool.

00:34:44 It's a whole group of GPUs that I could be able to give it as many GPUs as--

00:34:49 - What, 50?

00:34:50 - Yeah, you ask for 50, exactly.

00:34:52 The benefits pulling for a resource allocation are that you reduce fragmentation and you get more flexibility.

00:34:57 So we've been trying to explore how we can do this.

00:35:00 From rack scale desegregation of saying, there's a whole bunch of SSDs at the top of the rack, then there's a bunch of GPUs, and then there's a bunch of CPU cores, and let's just compose the system dynamically.

00:35:11 There's a bunch of challenges.

00:35:13 From a resiliency perspective, like how do you prevent one failure of the GPU part of the system bringing down the whole rack, for example.

00:35:21 There's latency and bandwidth challenges.

00:35:24 Like how do you, when you're sitting there on the PCI bus, you get a whole bunch of bandwidth and you get very low latency.

00:35:30 If you're going across the rack, you might have the same bandwidth, you might have lower bandwidth, just because you can't deliver that much bandwidth out of the GPUs.

00:35:38 - All the systems are optimized to make assumptions about these numbers.

00:35:42 - Exactly.

00:35:42 And your latency is gonna be higher.

00:35:44 And so some workloads can't tolerate their latency.

00:35:45 So we've been exploring desegregated memory, desegregated GPUs.

00:35:49 I've shown demos of both of them.

00:35:50 We're still exploring those.

00:35:52 We're not, you know, it's not--

00:35:53 - Which is harder.

00:35:54 - Yeah.

00:35:55 Desegregated memory or GPUs?

00:35:56 - I would guess memory, but I have zero experience.

00:35:59 - Memory is challenging because there are certain GPU workloads that aren't so latency sensitive.

00:36:04 Like AI training.

00:36:05 - Sure.

00:36:06 Like a batch job sort of thing.

00:36:07 - Yeah, a batch job thing.

00:36:08 But when it comes to memory, you almost always see the latency.

00:36:13 And so what we think we can do is get remote memory down to NUMA, you know, speaking of non-uniform memory architecture, latency down to that level.

00:36:21 And a lot of applications can tolerate that.

00:36:23 - Okay.

00:36:24 - And so we have a memory tiering where you've got class memory that's on the system and then remote memory, which is like NUMA latency.

00:36:29 - Kind of like a L2 cache, but like a bigger idea of it.

00:36:32 Interesting.

00:36:33 All right, well, very, very cool.

00:36:35 I think you guys are doing such neat stuff.

00:36:37 And you know, really, when you see these hyperscale clouds, I think a lot of what people see is the insane dashboard of choices.

00:36:47 Like do I do routing?

00:36:49 Do I do firewalls?

00:36:50 Do I do VPCs?

00:36:52 Do I do like paths, IaaS?

00:36:54 What do I do?

00:36:54 But oftentimes don't really think about like, well, you're getting a slice of this giant server and you know, maybe someday he'll live under the ocean or whatever, right?

00:37:03 So it was really cool to get that look.

00:37:05 - What you're seeing is the cloud, it started with a few basic building blocks and then we started to explore lots of different directions of creating lots of different PaaS services and PaaS services for compute and then different data offerings.

00:37:17 I think the space, again, coming to the workload, you get this, is it high, do you need key value store?

00:37:23 Do you need a vectorized database?

00:37:24 Do you need, and do you need any of those to be extreme performance?

00:37:28 'Cause then if you need extreme performance, go for the design for purpose vectorized database.

00:37:34 If you want key value with vectorization, but it's okay if the vectorization isn't the fastest possible, you know, you can go use this other offer.

00:37:43 So that's why the list of options has continued to expand is just because every workload says, I need this and that's the most important thing to me.

00:37:51 And the other one says, no, I need this, that's the most important thing.

00:37:54 And others are like, I don't care.

00:37:55 - As it becomes the mainframe of the world, right?

00:37:58 There's a lot of different types of apps running on it.

00:38:00 - Yeah. - Yeah, awesome.

00:38:02 All right, Mark, final call to action.

00:38:03 People maybe wanna learn more about some of the stuff we saw here, see some pictures, but, or maybe also just do more with Azure.

00:38:09 What do you say?

00:38:09 - A couple of things.

00:38:10 One is, I've been doing a series of Azure innovation talks at Build and Ignite sessions.

00:38:14 So go back to the last Build and you'll see the most recent one of those.

00:38:17 And then at this Ignite, I'm doing one that's just looking at AI related innovation.

00:38:22 That's on Friday, tomorrow here at Ignite, and it'll be available on demand.

00:38:27 - Yeah, I'll grab the links to some of those and put them in the show notes for people.

00:38:30 - Excellent.

00:38:30 - Well, thanks so much for being on the show.

00:38:31 - All right, thanks for having me, Michael.

00:38:32 - Yeah, it's great.

00:38:34 This has been another episode of Talk Python to Me.

00:38:37 Thank you to our sponsors.

00:38:38 Be sure to check out what they're offering.

00:38:40 It really helps support the show.

00:38:41 This episode is sponsored by Posit Connect from the makers of Shiny.

00:38:46 Publish, share, and deploy all of your data projects that you're creating using Python.

00:38:51 Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quatro, Reports, Dashboards, and APIs.

00:38:58 Posit Connect supports all of them.

00:39:00 Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:39:05 Are you ready to level up your Python career?

00:39:10 And could you use a little bit of personal and individualized guidance to do so?

00:39:15 Check out the PyBytes Python Developer Mindset Program at talkpython.fm/pdm.

00:39:22 Want to level up your Python?

00:39:24 We have one of the largest catalogs of Python video courses over at Talk Python.

00:39:28 Our content ranges from true beginners to deeply advanced topics like memory and async.

00:39:33 And best of all, there's not a subscription in sight.

00:39:36 Check it out for yourself at training.talkpython.fm.

00:39:39 Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

00:39:44 We should be right at the top.

00:39:45 You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.

00:39:55 We're live streaming most of our recordings these days.

00:39:57 If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:40:06 This is your host, Michael Kennedy.

00:40:07 Thanks so much for listening.

00:40:09 I really appreciate it.

00:40:10 Now get out there and write some Python code.

00:40:12 (upbeat music)

00:40:30 [MUSIC]

