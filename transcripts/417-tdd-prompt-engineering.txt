00:00:00 Large language models and chat-based AIs are kind of mind-blowing at the moment.

00:00:04 Many of us are playing with them for working on code or just as a fun alternative to search, but others of us are building applications with AI at the core.

00:00:14 And when doing that, the slight unpredictable nature and probabilistic style of LLMs makes writing and testing Python code very tricky.

00:00:23 Interpromptimize, from Maxine Bocheman, and Preset.

00:00:28 It's a framework for non-deterministic testing of LLMs inside of our applications.

00:00:33 Let's dive inside the AIs with Max. This is Talk Python To Me, episode 417, recorded May 22nd, 2023.

00:00:54 Welcome to Talk Python.me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:59 Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org. Be careful with impersonating accounts on other instances, there are many. Keep up with the show and listen to over seven years of past episodes at talkpython.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:28 This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.

00:01:34 Download your free trial of PyCharm Professional at talkpython.fm/done-with-pycharm.

00:01:41 And it's brought to you by The Compiler Podcast from Red Hat.

00:01:45 to an episode of their podcast to demystify the tech industry over at talkbython.fm/compiler.

00:01:53 Max welcome to Talk Python to Me.

00:01:54 >> Well it's good to be back on the show and now I know it's live too so no mistakes.

00:01:59 I'm going to try to not say anything outrageous.

00:02:01 >> People get the unfiltered versions though.

00:02:04 >> Absolutely, absolutely.

00:02:05 I love it when people come check out the live show.

00:02:08 >> Welcome back.

00:02:09 It's been a little while since you were on the show.

00:02:11 About since September.

00:02:13 We talked about supersets.

00:02:15 We also talked a little bit about Airflow, some of the stuff that you've been working on.

00:02:19 And now we're kind of circling back through this data side of things, but trying to bring AI into the whole story.

00:02:26 So pretty cool project I'm looking forward to talking to you about.

00:02:29 - Awesome, I'm excited to talk about it too.

00:02:31 And these things are related in many ways.

00:02:34 Like one common foundation is Python.

00:02:38 Like a lot of these projects are in Python.

00:02:40 They're data related.

00:02:41 And here, problemize and prompt engineering and integrating AI is related in a way that we're building some AI features into super set right now and into presets.

00:02:52 So it ties things together.

00:02:55 - Yeah, I can certainly see a lot of synergy here.

00:02:57 Before we dive into it, it hasn't been that long since you were on the show, but give us a quick update, just a bit about your background for people who don't know you.

00:03:05 - Yeah, so my career is a career like maybe 20 or so years in data building, doing data engineering, doing, try to make useful data useful for organizations.

00:03:18 over the past decade or so, I've been very involved in open source.

00:03:22 I started Apache Airflow 2014.

00:03:25 So for those not familiar with Airflow, though, it's pretty well known now.

00:03:28 It's used at, I heard like, I think it's like tens of thousands, I think above a hundred thousand companies are using Apache Airflow, which is kind of insane.

00:03:37 to think about. It's like you started a little project. So for me, I started this project at Airbnb, and it really took off. And I think it's just like great, called project community fit, like, we'll really need it needed that was the right abstraction for people at the time, and still today, and it just, it just really took off. So I was working on orchestration. And then, then I was like, I just love things that are visual and interactive. So there was no great open source BI tool out there business intelligence. So this old data, data dashboarding, exploration, SQL IDE.

00:04:10 So it's a playground for people trying to understand and visualize and explore data.

00:04:15 So I started working on Apache Superset in 2000, it was like 15 or 16 at Airbnb too.

00:04:20 And we also brought that to the Apache Software Foundation.

00:04:23 So again, I had a very, very popular open source project that's used in like tens of thousands, maybe a hundred thousand organizations or so.

00:04:32 And a great, like today, like it has become a super great open source or alternatives to Tableau, Looker, like all those business intelligence tool is very viable for organizations.

00:04:43 And then quick plugs, I had pre-set, I had a company I started, I'm also an entrepreneur, I started a company a little bit more than four years ago around Apache Superset, and the idea is to bring Superset to the masses.

00:04:56 So it's really like hosted, manage, state of the art Apache Superset for everyone with some bells and whistles.

00:05:03 So the best Superset you can run, There's a free version too.

00:05:07 So you can go and play and try it, today gets started in five minutes.

00:05:11 So it's a little bit of a commercial pointer, but also very relevant to what I've been doing, personally over the past like four years.

00:05:18 - Sure, it's some of the inspiration for some of the things we're gonna talk about as well and trying to bring some of the AI craziness back to products, right?

00:05:26 From an engineering perspective, not just a, hey, look, I asked what basketball team was gonna win this year and it gave me this answer, right?

00:05:36 - It's like, and I, caveat, I don't know anything that happened since 2021.

00:05:40 So AI, or AI specifically, Bart is a little bit better at that, but it's like, you know, the last thing I read off of the internet was in fall 2021.

00:05:49 Wow, it makes things a little bit challenging.

00:05:52 But yeah, so we're looking, we're building, you know, AI features into preset, you know, as a commercial open source company, we need to build some differentiators too from supersets.

00:06:00 So we contribute a huge amount, Like maybe 58% of the work we do at Preset is contributed back to Superset, but we're looking to build on differentiators.

00:06:09 And we feel like AI is a great kind of commercial differentiator too, on top of Superset that makes people even more interested to come and run Preset too.

00:06:20 - Yeah, excellent.

00:06:21 And people, you say they were popular projects, like Airflow has 30,000 stars, Apache Superset has 50,000 stars, which puts it on par with Django and Flask for people's sort of mental models out there, which is, I would say, it's pretty well known.

00:06:37 So awesome.

00:06:39 - Yeah, stars are kind of vanity metric in some ways, right?

00:06:42 Not necessarily usefulness or value delivered, but it's a proxy for popularity and hype, you know?

00:06:49 So it gives a good sense.

00:06:50 And I think like at 50,000 stars, if you look at, you know, it's probably in the top 100 of GitHub projects.

00:06:57 If you remove the, in the top 100, there's a lot of documentations and guides and things that are not really open source projects.

00:07:05 It's probably like top 100 open source project-ish, you know, in both cases, which--

00:07:11 - Right.

00:07:12 - Some of them, it doesn't matter.

00:07:13 - Oh, like it's like, you start to, oh no, like how, like whether it's gonna take off and how, and you know, it's like, wow, it's just nice to see that.

00:07:19 - Yeah, absolutely.

00:07:20 I mean, on one hand, it's nice, but it doesn't necessarily make it better.

00:07:24 But it does mean there's a lot of people using it, there's a lot of polish, there's a lot of PRs and stuff that have been submitted to make it work, a lot of things that can plug into, right?

00:07:34 So there's certainly a value for having a project popular versus unpopular.

00:07:38 - Oh my God, yes.

00:07:39 And I would say one thing is, all the dark, like call it like secondary assets outside of the core projects documentation.

00:07:46 And there will be a lot of like use cases and testimonials and reviews and people bending the framework in various ways and forks and plugins.

00:07:56 Another thing too, that people, I think, don't really understand the value of an in-software and open source is, or I'm sure people understand the value, but it's not talked about as just a whole battle-tested thing.

00:08:07 Like when something is run at thousands of organizations in production for a long time, there's a lot of things that happen in a software that are very, very valuable for the incremental organization adopting it.

00:08:20 >> Well, let's talk large language models for a second.

00:08:25 So AI means different things to different people, right?

00:08:28 They kind of get carved off as they find some kind of productive productized use, right?

00:08:34 AI is this general term and like, oh, machine learning is now a thing that we've done or computer vision is a thing we've done.

00:08:40 And the large language models are starting to find their own special space.

00:08:45 So maybe we could talk a bit about a couple of examples just so people get a sense.

00:08:51 To me, ChatGPT seems like the most well-known.

00:08:54 What do you think?

00:08:55 >> Yeah. I'll say if you think about what is a large language model, and what are some of the leaps there?

00:09:02 I'm not an expert, so I'm going to try to put my foot in my mouth.

00:09:06 But some things that I think are interesting, a large language model is a big neural network that is trained on a big corpus of text.

00:09:15 I think one of the big leaps we've seen is unsupervised learning.

00:09:19 Really often, like in machine learning in the past, or pre-LLMs, we would have very specific training set and outcomes we were looking for.

00:09:30 And then the training data would have to be really structured.

00:09:33 Here what we're doing with large language models is feeding a lot of, a huge corpus of text, and what the large language model is trying to do or resolve is to chain words, right?

00:09:43 So it's trying to predict the next word, which seems like you would be able to put words together that kind of makes sense, but like you wouldn't think that consciousness, not just like consciousness, but intelligence would come out of that. But somehow it does, right? Like if you chain, it's like if you say, you know, Hapthi Dhati sits on a, it's really clear it's going to be wall, you know, the next word. But if you push this idea much further with a very large corpus of like human knowledge, somehow there's some really amazing stuff that does happen. All these are distinguished models and I think that realization around happened at around you know, at GPT-3, 3.5 getting pretty good and then at 4 like, oh my god, this stuff can really kind of seems like it can think or be smart and be very helpful. Yeah, the thing that I think impresses me the most about these is they seem and people can tell me it's statistics and you know, I'll believe them but it seems like they have an understanding of the context of what they're talking about more than just predicting like Humpty Dumpty sat on the what?

00:10:48 It sat on the wall, right? Obviously, that's what was likely to come next when you see those that set of words. But there's an example that I like to play with, which I copied. I'll give it a little thing. I'll say, hey, here's a program, Python program. I'm going to ask you questions about it. Let's call it Arrow. And it's like this is a highly nested program that function that tests whether something's a platypus. I saw this example somewhere and I thought, okay, this is is pretty cool, but it has this, if it's a mammal, then if it has fur, then if it has a beak, then if it has a tail, and you can just do stuff that's really interesting, like I'll ask it to--

00:11:25 - Is bird or something like that, or is like--

00:11:27 - Yeah, yeah, yeah, rewrite it, write it using guarding clauses to be not nested, right?

00:11:34 - Oh, yeah.

00:11:36 - Right, and it'll say, sure, here we go, and instead of being if this, then nest if this, then if this, it'll do, if not, if not return false, right?

00:11:44 Which is really cool.

00:11:45 And that's kind of a pretty interesting one.

00:11:47 But like, this is the one, this is the example that I think is crazy.

00:11:51 It's rewrite arrow to test for crocodiles.

00:11:56 - Using that, it's like what people would call a one shot, a few shot example of like, hey, here's an example of the kind of stuff I might want.

00:12:05 There's some different ways to do that.

00:12:07 But it's a pattern in prop engineering where you'll say you have a zero shot, one shot, a few shot examples we get into.

00:12:13 But it does feel like it understands the code, right?

00:12:16 Like what you're trying to do.

00:12:18 - Right, just for people listening, it said, okay, here's a function isCrocodile.

00:12:22 If not self.isReptile, if not self.hasScales, these are false examples, right?

00:12:27 And if it has four legs and a long snout, it can swim, right?

00:12:29 Like it rewrote the little tests and stuff, right?

00:12:32 In a way, that seems really unlikely that it's just predicting likelihood 'cause it's never seen anything like this really, which is really, it's pretty mind blowing I think.

00:12:42 - Or it had, like it read the entire internet and all of GitHub and like had steps to say it has seen some of the things.

00:12:49 I think that's mind boggling, it's just like, like when you think about what it did there is it read the entire conversation so far, your input prompt, and it has like a system prompt ahead of time that says, you know, your ChatGPT, try to be helpful to people and here's a bunch of things you should or should not say, and unbiased, you try to be concise and good and virtuous.

00:13:12 And if you want to find all sorts of jailbreaks out of that, but I can, all it does from that point on is I try to predict the next word, which is kind of insane that it gets to, you know, the amount of structure that we see.

00:13:25 Right, right.

00:13:25 That's a lot of structure there.

00:13:27 Right.

00:13:27 And so pretty impressive.

00:13:28 And ChatGPT is starting to grow.

00:13:31 You know, if you've got version four and you can start using some of the plugins, it's going to keep going crazy there.

00:13:34 Other examples are simply a just released lemur, which is a large language model, but really focused on transcribing speech, which I think is kind of cool.

00:13:44 Microsoft reduced Microsoft security, released Microsoft security copilot, which is a large language model to talk about things like Nginx misconfiguration and stuff like that.

00:13:55 There's just a lot of stuff out there that's coming along here.

00:14:00 A lot of thousands of hours of learning type of thing.

00:14:03 On the open source front too, there's this whole ethical thing, like should everyone and anyone have access to open source models doing that?

00:14:11 Well, we don't really understand.

00:14:13 And we probably shouldn't get into the ethics part of the debate here because that's a whole series of episodes we probably won't want to get into.

00:14:21 But what's interesting is Databricks came up with a model for what's called phasebook, came up with one called LAM, or the open source, or the weights.

00:14:30 So you have the model topology with pre-trained weights.

00:14:34 In some cases, there's open source corpus of training that are also coming out and are also open source.

00:14:41 And I mean, it's like, and these these open source models are somewhat competitive or increasingly competitive with GPT-4.

00:14:49 Yeah, which is kind of crazy.

00:14:52 And some of them don't are where GPT-4 has limitations.

00:14:57 They break through these limitations.

00:14:58 So one thing that's really important as a current limitation of the GPT models and LLMs is the prompt window, the token prompt window.

00:15:09 So basically when you ask a question, it's been trained and has machine learn with data up to, I think in the case of GPT-3.5 or 4, it's the corpus of training goes all the way to fall 2021.

00:15:23 So if you ask, "Who is the current president of the United States?" It just doesn't know, or it will tell you, "As of 2021, it is this person." But if you're trying to do tasks, like what I've been working on, we'll probably get into later in the conversation, is trying to generate SQL, it doesn't know your table.

00:15:40 So you have to say, "Hey, here's all the tables in my database.

00:15:42 Now can you generate SQL that does X on top of it?" And that context window is limited and increasing.

00:15:51 But some of these open source models have different types of limitations.

00:15:54 They might have like 2x, 4x, 5x, 10x, you know, 10x limitation that GPT-4 might have.

00:16:00 This portion of Talk Python to Me is brought to you by JetBrains, who encourage you to get work done with PyCharm.

00:16:09 PyCharm Professional is the complete IDE that supports all major Python workflows, including full stack development.

00:16:16 That's front-end JavaScript, Python backend and data support, as well as data science workflows with Jupyter.

00:16:23 PyCharm just works out of the box.

00:16:25 Some editors provide their functionality through piecemeal add-ins that you put together from a variety of sources.

00:16:32 PyCharm is ready to go from minute one.

00:16:35 And PyCharm thrives on complexity.

00:16:38 The biggest selling point for me personally is that PyCharm understands the code structure of my entire project, even across languages such as Python and SQL and HTML.

00:16:48 If you see your editor completing statements just because the word appears elsewhere in the file, but it's not actually relevant to that code block, that should make you really nervous.

00:16:59 I've been a happy paying customer of PyCharm for years.

00:17:02 Hardly a workday passes that I'm not deep inside PyCharm working on projects here at Talk Python.

00:17:09 What tool is more important to your productivity than your code editor?

00:17:13 You deserve one that works the best.

00:17:15 So download your free trial of PyCharm professional today at talkpython.fm/donewithpycharm and get work done.

00:17:23 That link is in your podcast player show notes.

00:17:26 Thank you to PyCharm from JetBrains for sponsoring the show and keeping Talk Python going strong.

00:17:33 It's interesting to ask questions, but it's more interesting from a software developer perspective of, can I teach it a little bit more about what my app needs to know or what what my app structure is, right?

00:17:46 In your case, I want to use Superset to ask the database questions.

00:17:52 But if I'm gonna bring in AI, it needs to understand the database structure so that when I say, help me do a query to do this thing, it needs to know what the heck to do, right?

00:18:03 - The table, it needs to know, so there's just stuff it knows and the stuff it can't know.

00:18:08 And some of it goes, is related to the fact that whether this information is public on the internet, whether it has happened to be trained against it.

00:18:15 And then if it's a private, there's just no hope that it would know about, you know, your own internal documents or your database structures. In our case, it speaks SQL very, very well.

00:18:26 So as we get into this example, like how to get GPT to generate good SQL in the context of a tool like SuperSep or SQL Lab, which is our SQL IDE.

00:18:35 So it knows how to speak SQL super well.

00:18:38 It knows the different dialects of SQL very, very well.

00:18:41 It knows its functions, its dates functions, which a lot of the SQL, and like the engineers only call it like, yeah, I can never remember like what Postgres date this function is.

00:18:51 But GPT or GPT models just, it knows SQL and knows the dialects and knows the mechanics of SQL.

00:18:57 It understands data modeling, foreign keys, drawings, primary, all this stuff it understands.

00:19:02 It knows nothing about your specific database, the schema names and the table names, the column names that they might be able to use.

00:19:10 that's where we need to start providing some context.

00:19:13 And this context window is limited.

00:19:15 So it's like, how do you use that context well, or as well as possible?

00:19:21 And that's the field and some of the ideas behind those prompt crafting and prompt engineering, which we can get into once we get there, maybe we're there already.

00:19:31 - Yeah, yeah, well, yeah, I think where I see this stuff going is from this general purpose knowledge, starting to bring in more private or personal or internal type of information, right?

00:19:44 Like our data about our customers is like structured like this in a table.

00:19:48 And here's what we know about them.

00:19:49 Now let us ask questions about our company and our thing, right?

00:19:53 And it's like starting to make inroads in that direction, I think.

00:19:56 Yeah, what, you know, one thing to know about is, there's different approaches to teach or provide that context.

00:20:04 So one would be to build your own model from scratch, right?

00:20:08 And that's pretty prohibitive.

00:20:10 So you'd have to find the right corpus.

00:20:12 And instead of starting with a model that knows SQL and needs to know your table and context, you have to start from zero and very prohibitive.

00:20:20 Another one is you start from a base model at some point of some kind.

00:20:24 There's a topology, so there's different layers and number of neurons and it knows some things.

00:20:30 And then you load up some weights that are open source.

00:20:32 and then you say, I'm going to tune this model to teach it my database schema as in basically my own corpus of data.

00:20:39 So it could be your data dictionaries, could be your internal documents, it could be your GitHub code, your dbt projects.

00:20:46 If you have one of your Airflow DAGs, be like, I'm going to dump all this stuff in the model that will get baked into their own network itself.

00:20:54 That's doable pretty primitive in this era.

00:20:57 If you have the challenge that we have at Preset, which is we have multiple customers with different schemas, we can't have spillover.

00:21:04 So you have to train a model for each one of our customers and serve a model for each one of our customers.

00:21:09 So it's still pretty prohibitive.

00:21:11 And a lot of people fall back on this third or fourth method that I would call prompt engineering, which is I'm going to use the base model, the OpenAI API, or just an API on LLM.

00:21:23 And then I will, if no SQL already, I'll just say, hey, here's a bunch of tables that you might want to use.

00:21:28 can you generate SQL on top of it?

00:21:30 So then that's just a big request with a lot of context.

00:21:34 Then we have to start thinking about maximizing the use of that context window to pass the information that's most relevant within the limits allowed by the specific model.

00:21:45 - Right, and that starts to get into reproducibility, accuracy, and just those limitations, which is kind of an engineering type of thing, right?

00:21:55 - Yeah.

00:21:55 - And then, you know, maybe a topic too, And this conversation is based on that recent blog post and the flow, just going back to the flow of that blog post, we started by establishing the premise that everyone is trying to bring AI into their product today.

00:22:10 Thousands of product builders are currently exploring ways to harness the power of AI in the products and experiences they create.

00:22:17 That's the premise for us with Text-to-SQL and SQL Lab as part of the SuperSet preset.

00:22:22 But I don't know, if you think of any product, any startup, any SaaS product you use.

00:22:27 If you work at HubSpot today, you're trying to figure out how to leverage AI to build, you know, sales chatbots or SDR chatbots.

00:22:36 So everyone everywhere is trying to figure that out.

00:22:39 The challenge is, I guess, very probabilistic in a different interface to anything we know.

00:22:45 Like, you know, engineers would be like, "Oh, let's look at an API and leverage it." And APIs are very, very deterministic in general.

00:22:53 AI is kind of a wild beast to tame.

00:22:57 You ask, first the interface is language not code, and then what comes back is like semi-probabilistic in nature.

00:23:05 And it could change underneath you.

00:23:07 It's a little bit like web scraping in that regard.

00:23:09 That like, it does the same, it does the same, and then, you know, something out there changed, not your code.

00:23:14 And then a potentially different behavior comes back, right?

00:23:18 Because they may have trained another couple of years, refined the model, switch the model, change the default temperature, all these things.

00:23:24 Yeah, there's a lot that can happen there.

00:23:26 One thing I noticed, like starting to work with what I would call prompt crafting, which is, you know, you work with GPT and you craft different prompt with putting emphasis in a place or another, or changing the order of things, or just changing a word, right?

00:23:42 Just say like, important exclamation point, capitalize the words, you know, the reserved words in SQL, and then just the fact that you put import exclamation point will make it do it or not do it, changing from a model to another.

00:23:56 So one thing that's great is the model, at least at OpenAI, they are immutable, as far as I know.

00:24:04 But if you use GPT-3.5 Turbo, for instance, that's just one train model, I believe that that is immutable.

00:24:13 The chatbot on top of it might get fine-tuned and change over time, but the model is supposed to be static.

00:24:19 You mentioned temperature, it'd be kind of interesting to just mention for those who are not familiar with that.

00:24:24 So when you interact with AI, one of the core parameters is temperature, and I think it's a value from zero to one, or I'm not sure how exactly you pass it, but it basically defines how creative you want to let the AI be.

00:24:42 Like if you put it to zero, you're going to have something more deterministic.

00:24:47 So asking the same question should lead to a similar or the same answer.

00:24:51 Though not in my experience, it feels like it should, but it doesn't.

00:24:54 But then if you put a higher, it will get more creative.

00:24:57 Talk more about how that actually seem to work behind the scene.

00:25:02 >> Yeah. Well, that variability seems to show up more in the image-based ones.

00:25:07 So for example, this article, this blog post that you wrote, you have this image here and you said, "Oh, and I made this image from mid-journey.

00:25:14 I've also got some examples of a couple that I did.

00:25:18 Where did I stick them?

00:25:19 Somewhere. Here we go.

00:25:21 Where I asked just for YouTube thumbnails, I asked Midjourney for a radio astronomy example that I can use.

00:25:27 Because here's one that's not encumbered by some licensing, but still looks cool and is representative.

00:25:34 There it's like massive difference.

00:25:37 I'm not sure how much difference I've seen.

00:25:40 I know it will make some, but I haven't seen as dramatic of a difference on chat.

00:25:44 - Well, on TNT, yeah. - Yeah.

00:25:47 I'm not sure exactly how they introduce the variability on the generative images AI.

00:25:53 I know it's like this multi-dimensional space with a lot of words and a lot of images in there.

00:25:59 And then it's probably like where the location point of that, they randomize that point in that multi-dimensional space.

00:26:08 For Chadgpd, it's pretty easy to reason about, and I might be wrong on this, again, I'm not an expert, But you know how the way it works is it writes, it takes the prompt and then it comes up with the next word sequentially.

00:26:20 So for each word for the next word, so Humpty Dumpty sat on a, it might be wall at 99%, but like there might be 1% of, you know, a sense or something like that.

00:26:32 And if you, you up the temperature, there's, it's more likely to pick the non-first word in that probability list, they probably do in a weighted way.

00:26:44 Like it's possible that I take a second or third word randomly, and then of course, it's going to get a tree or decision tree.

00:26:50 Once it picks a word, the next word is also changes.

00:26:53 So as you up that, it goes down path that sends it into more creative or different.

00:26:59 >> Right. Yeah, a little butterfly effect.

00:27:02 It makes a different choice here and then it sends it down through the graph.

00:27:07 Interesting. So one thing that you really pointed out here, and I think is maybe worth touching on a bit is this idea of prompt engineering. There's even places like learn prompting.org that like try to teach you how to talk to these things. And and you make a strong distinction between prompt crafting or just talking to the AI versus like really trying to put an engineering focus on it. You want to Yeah, I think it's a super important differentiation, but one that I'm proposing, right?

00:27:36 So I don't think that people have settled as to what is one or what is the other.

00:27:40 I think I saw a Reddit post recently that was like, "Prompt engineering is just a load of crap." Like, you know, anyone can go because they thought their understanding of prompt engineering was like, "Oh, you know, you fine tune or you craft your prompt and you say like, You are an expert AI working on, you know, creating molecules.

00:28:00 Now, can you do this?

00:28:01 And then, you know, by doing that, you might get a better outcome.

00:28:05 Or one really interesting thing that people have been doing in trunk crafting that seemed to have like huge impact on there's been paper written on this specific just hint or craft tweak is let's proceed step by step.

00:28:21 So basically, whatever the question you're asking, specifically around like more mathematicals or like things that require more systematic step-by-step thinking, the whole like just like let's think, let's expose this or let's go about it step by step makes it much better. So here you might be able to, well, so you know, if you had, if you had an example where ChatGPT-3 failed or ChatGPT-4 failed, you could just say, Colin, let's go step by step and it might succeed that time around.

00:28:52 - Maybe you can get it to help you understand instead of just get the answer, right?

00:28:58 Like factor this polynomial into its primary, you know, solutions or roots or whatever.

00:29:04 And you're like, okay, show me, don't just show me the answer, show me step by step so I could understand and try to learn from what you've done, right?

00:29:11 - Yeah, I mean, if you think about how, the way that it's trying to come up with a new word, if all it does is a language-based answer to a mathematical question, Like how many days are there between this date and that date?

00:29:24 There's no, that specific example might not exist or it's kind of complicated for it to go about it.

00:29:30 But if you say, let's think step by steps, okay, there's this many months, this month's duration is this long, there's this many days since the beginning of that month, it might get it right that, you know, that time around.

00:29:42 - Right, or if it fails, you could pick up part way along where it has some more.

00:29:46 - Yeah, you know, and then you could trace, I mean, just you too, like, I think, you know, one thing is like you should be extremely careful as like taking for granted that it's right all the time, you know, so that means like it also helps you review its process and where it might be wrong.

00:30:00 But back to crafting versus engineering.

00:30:02 So crafting would be the process that I think is more attached to a use ChatGPT every day, the same way that, you know, we've been trained at Googling, you know, over the past like two decades, you know, use quotes, use plus and minus, and you know which keywords to use intuitively, you know where it's going to get confused or not.

00:30:23 So I think prompt crafting is a different version of that that's just more worthy.

00:30:28 And if you're working with the AI to try to assist you, write your blog post or to try to assist you in any task really, just to be smart about how you bring the context, how you tell it to proceed, goes a very, very long way.

00:30:42 So that's what I call prompt crafting, call it like one-off cases.

00:30:47 what people do when they're interacting with, with the large language model.

00:30:50 I think so.

00:30:51 Right.

00:30:51 Like it's not evident for a lot of people will are exploring the edge of where it fails and they love to see it fail.

00:30:58 And, and then they, they don't think about like, Oh, what could I have told it to get the answer I was actually looking for?

00:31:04 Like, I got you wrong.

00:31:06 You know, it's as if I had that actor in a conversation.

00:31:09 I like, ah, you're wrong.

00:31:10 And I told you so, you know, so I think there's a lot of that online, but I think for all these examples that I've seen, I'm really tempted to take the prompt that they had and then give it an instruction or two or more and then figure out how to get it to come up with the right thing.

00:31:23 So prompt crafting, super important skill.

00:31:26 You know, you could probably get a boost of, for most knowledge information workers, you'll get a boost of 50% to 10X for a lot of the tasks you do every day if you use AI well.

00:31:36 So it's great personal skill to have.

00:31:38 - This portion of the Talk Python is sponsored by the compiler podcast from Red Hat.

00:31:44 Just like you, I'm a big fan of podcasts, and I'm happy to share a new one from a highly respected open source company, Compiler, an original podcast from Red Hat.

00:31:53 Do you want to stay on top of tech without dedicating tons of time to it?

00:31:57 Compiler presents perspectives, topics, and insights from the tech industry, free from jargon and judgment.

00:32:02 They want to discover where technology is headed beyond the headlines and create a place for new IT professionals to learn, grow, and thrive.

00:32:09 Compiler helps people break through the barriers and challenges turning code into community at all levels of the enterprise.

00:32:16 One recent and interesting episode is there, the Great Stack Debate.

00:32:19 I love love love talking to people about how they architect their code, the tradeoffs and conventions they chose, and the costs, challenges, and smiles that result.

00:32:29 This Great Stack Debate episode is like that.

00:32:31 Check it out and see if software is more like an onion, or more like lasagna, or maybe even more complicated than that.

00:32:37 It's the first episode in Compiler's series on software stacks.

00:32:41 more about compiler at talkpython.fm/compiler. The link is in your podcast player show notes.

00:32:47 And yes, you could just go search for compiler and subscribe to it, but follow that link and click on your player's icon to add it. That way they know you came from us. Our thanks to the compiler podcast for keeping this podcast going strong.

00:33:01 Prop engineering, in my case, I'm like, you're building something, you're using an AI as an as an API behind the scene, you wanna pass it a bunch of relevant contexts, really specify what you wanna get out of it.

00:33:17 Maybe you even wanna get a structured output, right?

00:33:20 You might wanna get a JSON blob out of it.

00:33:22 And you say, return a JSON blob with the following format, so it's more structured.

00:33:27 So then to give all these instructions, is this idea of providing few shots too, you might be storing context in a vector database.

00:33:34 I don't know if we're gonna get to that today, But there are ways to kind of structure and organize your potential embeddings or the things you want to pass as context.

00:33:43 So there's a lot here.

00:33:45 I think somewhere too, I talked about prompt engineering.

00:33:47 If we'd scroll in the blog posts, like what is in prompt engineering, it will list the kind of things.

00:33:54 - Yeah, here you go.

00:33:56 - Oh yeah, right here.

00:33:57 The definition of this is Chad's GPT's version of it.

00:34:00 So when you do prompt engineering, you can add context, which that means that you're gonna have to retrieve context maybe from a database, from a user session, from your Redux store if you're in the front end, right?

00:34:11 So you're gonna go and fetch the context that's relevant in context with the application, at least while building products.

00:34:17 Specifying an answer format, you could just say, yes, I just want a yes or no, a Boolean, I want a JSON blob with not only the answer, but your confidence on that answer or something like that.

00:34:27 Limiting scope, asking for pros and cons, incorporating verification or sourcing.

00:34:33 So that's more, you know, if you iterate on a prompt, you're going to be rigorous about is this prompt better than the previous prompt I had?

00:34:40 Like if I pass five rows of sample data while doing text to SQL, does it do better than if I, or does it do more poorly than if I pass 10 rows of sample data or provide a certain amount of column level statistics?

00:34:54 So prompt engineering is not just from crafting.

00:34:56 It is like bringing maybe the scientific method to it, bring some engineering of like fetching the right context and organizing it well and then measuring the outcome.

00:35:06 Right, exactly.

00:35:07 Something that comes out, you can measure and say this is 10% better by my metric than it was before with this additional data.

00:35:14 Right. That's that's a big difference.

00:35:16 Right. And then there's so many things moving, right?

00:35:18 Like and everything is changing so fast in the space.

00:35:22 You're like, oh, well, GPT5 is out or GPT4 Turbo is half the price and then just came out.

00:35:28 Now I'm just going to move to that.

00:35:29 They're like, wait, is that performing better?

00:35:32 Or, you know, like, what are the trade-offs?

00:35:34 Or even I'm going to add, I'm going to move this section, you know, asking for a certain JSON format above this other section.

00:35:41 I'm going to write important exclamation point do X.

00:35:45 Does that improve my result?

00:35:47 Or does that mess it up?

00:35:49 And which one of my test case, perhaps, that succeeded before fails now, and which one failed before succeeds now?

00:35:55 So you can be like, is that a better or worse iteration towards Michael?

00:36:01 Right, right.

00:36:02 Kind of training, bringing this unit testing TDD mindset.

00:36:06 Yes.

00:36:07 Yeah, so that's what we're getting deeper into the blog post, right?

00:36:10 So the blog post, I'm talking about bringing this TDD, the test-driven development mindset to prompt engineering, right?

00:36:19 And there's a lot of things that are in common.

00:36:22 you can take and apply, kind of transfer just over.

00:36:25 There are some things to that breakdown that are fundamentally different between testing a prompt or working with AI and working with, you know, just a bit more deterministic code testing type framework to get into that.

00:36:38 - Yeah, yeah, for sure.

00:36:40 So you called out a couple of reasons of why TDD is important for prompt engineering.

00:36:45 Maybe we could run through those.

00:36:47 - Yeah, so, you know, the first thing is the AI model is not a deterministic things, or you use a modern API or a GraphQL REST API, the format of what you ask is extremely clear, and then the format of what you get back is defined by a schema.

00:37:06 It's like very deterministic, pretty guaranteed that you do the same request, so you get the same output-ish, or at least format.

00:37:13 With AI, that's not the case, right?

00:37:15 So it's much more unpredictable and probabilistic by nature.

00:37:20 The second one is handling complexity.

00:37:22 So AI systems are complex, black boxy, kind of unpredictable too.

00:37:26 So embrace that and assume that you might get something really creative out of there for better or for worse.

00:37:33 And then reducing risk, like you're shipping product, you know, if you're shipping product, writing product, you don't want to see any sort of like bias or weird thing like the AI could go crazy.

00:37:47 And yeah, there are examples of AI is going crazy before like Tay.

00:37:52 Do you remember Microsoft Tay?

00:37:54 I don't know that one, but I know of other examples.

00:37:56 If you want it.

00:37:57 Yeah.

00:37:57 I mean, it came out and it was like this, this sort of just, you know, I'm here to learn from you internet and people just turned it into a racist and made it do all sorts of horrible things.

00:38:06 And they had to shut it down a couple of days later because it just, it's like, Whoa, it met the internet and the internet is mean.

00:38:12 So, that's not great.

00:38:14 - Yeah, train it on 4chan or let it, you know, go crawl 4chan and read it.

00:38:19 It's not always gonna be nice.

00:38:21 - So bad, right?

00:38:22 I mean, you don't entirely control what's gonna come out of those things.

00:38:26 And so, it's a little more predictable, right?

00:38:29 - And it's not even like, you don't entirely control.

00:38:31 It's like, I think, yeah, like basically, you know, control might be a complete illusion.

00:38:36 Like even the people who work at OpenAI don't fully understand what's happening in there.

00:38:41 (laughing)

00:38:42 Yeah, like, well, it read a bunch of stuff and it's predicting the next word and it gets most things right.

00:38:49 By the way, like they do a lot around this idea of like, not necessarily TDD, but there's a whole eval framework so you can submit your evaluation functions to open AI.

00:38:58 And as they train the next version of things, they include that in what their evaluation system or the new thing.

00:39:05 So say, if I wanted to go and contribute back a bunch of our text to SQL type use cases as they call evals, then they would take that into consideration when they train their next models.

00:39:16 All right, so going down the list, reducing risk, right?

00:39:19 So you're integrating some of that beast that's not fully tamed into your product.

00:39:23 So you probably want to make sure it's tamed enough to live inside your product.

00:39:28 Continuous improvements, that should have been maybe the first one in the list is you're iterating on your prompts, you're trying to figure out a best context, you're trying different model versions, maybe you're trying some open source models or the latest GPT cheaper and greater thing.

00:39:45 So you want to make sure that as you iterate, you're getting to the actual outcomes that you want systematically.

00:39:52 And performance measurement too, like how long does it take? How much does it cost?

00:39:56 You kind of need to have a handle on that.

00:40:00 The new model might be 3% better on your corpus of tests, but it might be six times the price. Like, do you want, are you okay?

00:40:08 Right, right. Or just slower from a user perspective.

00:40:11 Yeah.

00:40:11 Time to interaction.

00:40:12 You know, that's one thing with AI we're realizing now is a lot of the prompts on four will be like, you know, two to one to seven seconds, which in the Google era, you know, there's been some really great papers out of Google early on that prove that, you know, it's like 100 milliseconds as an impact on user behaviors.

00:40:32 And I'll say, right.

00:40:34 Yeah, people give up on checkout flows or whatever, and go into the next part of your site on a measurably on a hundred millisecond blocks, right?

00:40:41 When you're talking, well, here's 7,000, you know, you're 70 of those.

00:40:44 That's going to have an effect.

00:40:46 Potentially.

00:40:46 Oh, it has, has been proven and very intricate and usage pattern session duration session outcomes, right.

00:40:54 And you know, a second is a mountain.

00:40:56 if today, like we were at this AB test, Google between like whatever millisecond it's at now, like just one second or half a second.

00:41:04 that the results coming out of that A/B test would show very, very different behaviors.

00:41:08 - Wow.

00:41:09 - I think there's some, no quote me on it, there's some really great papers, you know, written on TTI and just time-share interaction and the way it influenced user behavior.

00:41:17 So we're still, you know, in the AI world, it has to, if you're gonna wait two to seven seconds for your prompt to come back, it's gotta add some real important value to what's happening.

00:41:27 - Yeah, it does.

00:41:28 I think it's interesting that it's presented as a chat.

00:41:30 I think that gives people a little bit of a pause, like, oh, it's talking to me, So let's let it think for a second rather than it's a website that's supposed to give me an answer.

00:41:38 - Yeah, compared to then I guess your basis for comparison is a human, not a website or not comparing against Google.

00:41:46 So that's great.

00:41:47 - Yeah, I asked it a really hard question.

00:41:48 Give it some time, right?

00:41:49 Like that's not normally how we think about these things.

00:41:51 Okay, so you have a kind of a workflow from this engineering, building a product, testing like an AI inside of your product.

00:41:59 You wanna walk us through your workflow here?

00:42:01 Yeah, and you know, if you I think I looked at TDD, you know, and and the originally what is the normal like TDD type workflow.

00:42:09 And I just adapted this little diagram to prop engineering, right?

00:42:15 Because the whole idea of the blog post is to bring prop engine like TDD mindset to prop engineering.

00:42:20 So this is where I went.

00:42:22 But yeah, the workflow is like, okay, define the use case and desired AI behavior.

00:42:27 what are you trying to solve with AI?

00:42:28 In my case, the example that I'll use and try to reuse throughout this presentation is throughout this conversation is text to SQL.

00:42:39 So like we're trying to, where the user from, what are they schema gets, get the AI to generate gets good useful SQL, find the right tables and columns to use that kind of stuff, create test cases.

00:42:50 So it's like, okay, if I have this database and I have this from to give me my top five salary per department on this HR dataset, there's a fairly deterministic output to that.

00:43:01 You could say the SQL is not necessarily deterministic.

00:43:04 There's different ways to write that SQL.

00:43:06 There's a deterministic data frame or results set that might come up.

00:43:09 >> There is a right answer of the top five salaries.

00:43:12 >> That's right.

00:43:13 >> You can see like, am I getting, ultimately get that.

00:43:15 >> It's great if it is deterministic, because you can test this.

00:43:19 If you're trying to use AI to say write an essay about Nepalian, but apart, second conquest, you know, in less than 500 words, it's not as deterministic and it's hard to test whether the AI is good or not.

00:43:36 So you might need human evaluators.

00:43:38 But I would say in most AI products, or people are trying to bring AI to their product, in many cases more deterministic.

00:43:46 Another example of like more deterministic would say like, oh, getting, if you say getting AI to write Python functions. It's like, oh, write a function that, you know, returns if a, if a number is prime, yes or no, like that you can get the function and test it in a deterministic kind of way. So anyways, just pointing out, it's better, you're only going to be able to have a TDD mindset if you have like somewhat deterministic, you know, outcome to the, - Right. - want to use the AI for. Then create So you create a prompt generator, so that would be your first version, and that takes the SQL example.

00:44:21 It's given the 20 tables in this database, and there's columns and table names and data types and sample data.

00:44:29 Generate SQL that answers the following user prompt, and then the user prompt would say something like, "Department by top five salary per department." And then we're getting, for people that are not on the visual stream, not YouTube, but on just audio, we're getting into the loop here where it's like, evaluate the results, refine the tests, refine the prompts, and then start over.

00:44:50 And probably compile the results, keep track of the results so that you can compare.

00:44:55 Not just like, "Oh, are you 3% better on your test cases?" But also, "Which tests that used to fail succeed now?" "Which tests that used to fail now?" And then once you're happy with the level of success you have, you can integrate the prompt into product or maybe upgrade?

00:45:15 - Ship it. - Yeah, ship it.

00:45:17 - Ship it. So I think it's probably a good time to jump over to your framework for this because PyTest and other testing frameworks in Python are great, but they're pretty low level compared to these types of questions you're trying to answer, right?

00:45:32 Like, how has this improved over time for, you know, I was doing 83% right, right?

00:45:38 PyTest asserts a true or a false.

00:45:40 It doesn't assert that 83% is...

00:45:43 - Yeah, and it's a part of CI.

00:45:45 Like if any of your PyTest fail, you're probably gonna not CI, not allow CI, not even merge the PR, right?

00:45:54 So one thing that's different between test-driven development and unit testing and prompt engineering is that the outcome is probabilistic.

00:46:02 It's not true or false.

00:46:03 It might just be like zero or one, right?

00:46:06 where our spectrum fails.

00:46:09 For a specific test, you're like, "If it gets this column, but not this other column, you succeed at 50%." So it's non-binary.

00:46:18 It's also, you don't need perfection to ship.

00:46:21 You just need better than the previous version or good enough to start with.

00:46:25 So the mindset is, so there's a bunch of differences.

00:46:28 And for those interested, we won't get into the blog post.

00:46:32 I think I list out the things that are different between the two.

00:46:35 I think it's a little bit above this.

00:46:37 But, you know, the first thing I want to say is like the level of ambition of this project versus say an Airflow and SuperSet is like very low, right?

00:46:44 So it's maybe more similar to a test, a unit test library, and no discredit to the great awesome unit test libraries out there, but you would think those are fairly simple and straightforward.

00:46:58 Just the information architecture of a PyTest is probably simpler than the information architecture of a Django, for instance, right?

00:47:05 is just like a different thing. And here, the level of ambition is much more much, you know, for this is is fairly simple. So promptimize is something that I created, which is a toolkit to help people write to evaluate and score and understand while they iterate on their while doing prompt engineering. So in this case, I think I talked about the use case of preset, which is a we have a a big corpus that luckily was contributed by, I forgot which university, but a bunch of PhD people did a text to SQL contest.

00:47:42 - I think it was Yale.

00:47:43 - Yale, yeah.

00:47:44 - I think it was Yale, yeah.

00:47:46 - So great people at Yale were like, hey, we're gonna generate, you know, 3000 prompts on 200 databases with the SQL that should be the outcome of that.

00:47:56 So it's a big test set so that different researchers working on text to SQL can compare their results.

00:48:02 So for us, we're able to take that test set and some of our own test sets and run it at scale against OpenAI or against LLAMA or against different models.

00:48:13 And by doing that, we're able to evaluate this particular combo of this prompt engineering methodology with this model generates 73% accuracy.

00:48:25 And we have these reports we can compare fairly easily which prompts that, as I said before, were failing before are succeeding now and vice versa.

00:48:33 So you're like, am I actually making progress here or going backwards?

00:48:38 And if you try to do that on your own, like if you're crafting your prompt, just anecdotally and try on five or six things, like you quickly realize like, well shit, I'm gonna need to really test out of a much broader range of tests and then some regular methodology around that.

00:48:52 So probably-- - Right, how do you remember and go back and go, this actually made it better, right?

00:48:56 'Cause it's hard to keep all that in your mind, yeah.

00:48:59 Yeah, and something interesting that I'm realizing to work on this stuff is like, everything is changing so fast, right?

00:49:05 The models are changing fast, the prompting windows are changing fast, the vector databases, which is a way that organize and structure a context for your prompts, evolving extremely fast.

00:49:16 So it feels like you're working on unsettled ground in a lot of ways, like a lot of stuff you're doing might be challenged by, you know, the Bart API came out last week, and maybe it's better at SQL generation, And then you got to throw everything that I did on OpenAI.

00:49:30 But here's something you don't throw away.

00:49:32 Your test library and your use cases, right?

00:49:35 It's maybe is the thing, is the real asset here.

00:49:38 The rest of the stuff is like, AI is moving so fast that all the mechanics of the engineering itself and the interface with the whatever model is the best at the time, you're probably going to have to throw away as this evolves quickly.

00:49:53 But your test library is something really, really solid that you can perpetuate or keep improving and bringing along with you along the way.

00:50:03 So it's kind of an interesting thought around that.

00:50:05 - Yeah, let's talk through this example you have on Promptimize's GitHub read me here to make it a little concrete for people, like how do you actually write one of these tests?

00:50:15 - Yeah, so there's different types of prompts, but yeah, what I wanted to get to was just like, what is the prompt and how do you evaluate it, right?

00:50:25 And then behind the scene, we're going to be discovering all your prompts and running them and compiling results and reports, and doing analytics and making it easy to do analytics on it.

00:50:35 The examples that we have here, and I'll try to be conscious of both the people who can read the code and people who don't, people who are just on audio.

00:50:43 But here, from Proptimize, the prompt, we import a simple prompt, and then we bring some evals that are just like utility functions around evaluating the output of what comes back from the AI.

00:50:55 And here the first prompt case in the model, here I just create an array or a list of prompt cases, and it's a prompt case like a test case.

00:51:04 And with this prompt case, this very simple one, I say, "Hello there!" And then I evaluate that as says, either "Hi" or "Hello" in the output.

00:51:16 So if any of the words exist and what comes back, I give it a one or a zero.

00:51:21 Framework allows you to, you could say, "Oh, it has to have both these words," or give the percentage of success based on the number of words from this list that it has.

00:51:30 But, you know, that's the first case.

00:51:33 The second one is like a little bit more complicated, but name the top 50 guitar players of all time, I guess, and I make sure that Frank Zappa is in the list because I'm a Frank Zappa fan here.

00:51:44 But, you know, you could have a different, you could say, "Hey, I want to make sure that, you know, at least three out of five of these are in the list.

00:51:52 Those are like very more like natural language, very simple tests to, you know, just so that's the hello world essentially.

00:52:00 And then, you know, we're showing some examples of what's happening behind the scene.

00:52:03 Well, we'll actually like call, you know, the underlying API, get the results, run your eval function and compile a report.

00:52:10 What was the prompt?

00:52:12 What was, oh, a bird just flew into my room.

00:52:15 Inside?

00:52:16 Yeah, that's gonna make the podcast interesting.

00:52:19 Oh my goodness. Okay.

00:52:21 That might be a first here.

00:52:23 You're nuts.

00:52:25 Oh, well, out of my room.

00:52:26 Guess what? There's other people and that house, I'm just gonna close the door to my room.

00:52:31 Deal with it later.

00:52:32 All right. Well, that's a first.

00:52:35 I've had a bat fly into my house once, but never a bird. So both are crazy.

00:52:42 This is the first on the podcast. Out of eight years, we've never had a bird wild animal enter the studio of the guest.

00:52:49 - Yes, well, welcome to my room.

00:52:52 I live in Taos, so I guess that's something that's better than a bear, you know?

00:52:56 I could have been more.

00:52:57 - It is better than a bear.

00:52:58 - All right, but yeah, so like, just like keep enumerating kind of what we're seeing visually here, you know, we'll keep a YAML file as the report output.

00:53:09 So in Promptomize, you have your test case or your prompt cases, like test cases.

00:53:15 you have an output report that says, for this, you know, prompt case, here's the key.

00:53:20 Here's what the prompt that was actually the user input that came in.

00:53:24 Here's what the prompt looked like.

00:53:26 You know, what was the response, the raw response from the API?

00:53:30 What are all the tasks?

00:53:31 How long did it run?

00:53:32 So a bunch of metadata and relevant information that we can use later to create these reports.

00:53:38 Say like, was the score zero or one?

00:53:40 So you get the whole output report here.

00:53:43 - Yeah, okay.

00:53:43 And then you also have a way to get like a report.

00:53:47 I'm not sure, maybe I scrolled past it.

00:53:49 Where it shows you how it did, right?

00:53:51 I think that was in your-

00:53:53 - I think at the blog post, you see a much more-

00:53:55 - There you go, yeah.

00:53:56 - So this one, we're running the spider dataset that I just, that I talked about.

00:54:01 Remember, it's like the Yale generated text to SQL competition, you know, corpus.

00:54:06 And so here we looked at, you know, my percentage of success is 70%.

00:54:10 So, you know, here you say weight and score.

00:54:12 So there's a way to say, this particular prompt case is 10 times more important than another one, right, so you can do a relative importance of weight of your different text cases.

00:54:23 Now, one thing we didn't mention too, is like all these tests are generated programmatically too.

00:54:28 So that it's the same philosophy behind, you know, airflow of like, you know, it's almost like a little DSL to write your test case.

00:54:35 So you could, you know, you could read from a YAML file, for instance, in the case of what we do with Spyder SQL, there's a big JSON file of all the prompts and all the databases, and then we dynamically generate a thousand tests based on that.

00:54:48 So you can do programmatic test definition, more and more dynamic if you want it to be, or you could do more static if you prefer that.

00:54:55 So in this case, we're doing, we introduced this idea of a category too.

00:54:59 So I mentioned there's some features in Promptomize, like categorizing your tests or weights, and things like that.

00:55:07 So here we'll do some reporting on per category.

00:55:10 What is the score per category?

00:55:12 You can see which database is performing well or poorly again.

00:55:16 So I could have another category that is large database, small databases and see what the score is and compare reports.

00:55:24 >> It's pretty cool that it saves the test run to a file that then you can ask questions about and generate this report on and rather than just running it and passing or failing.

00:55:35 Yeah, or like giving the output and then having to run it again.

00:55:38 Yeah, there's some other features around if, so you can memoize the test.

00:55:43 So because it has a report, if you exit off of it or restart it later, it won't rerun the same test if it's the same hash input.

00:55:54 Even though with AI you might get a different answer.

00:55:56 But at least in this case, it will say like, "Hey, I'm rerunning the same prompt." instead of like waiting five seconds for open AI and paying the tokens and paying the piper, you know, I'm just going to skip that.

00:56:09 So there's some logic around skipping what's been done already.

00:56:13 It's not just a couple of milliseconds to run it.

00:56:15 It could be a while to get the answers.

00:56:17 Yeah, also like your early libraries that I haven't written the sub, like the threading for it where you can say, oh, run it on eight threads.

00:56:24 So with promptimize, I think, and you know, the blog post is probably more impactful than the Python project itself.

00:56:33 If the Python project takes off and a bunch of people are using it to test prompts and contribute to it, it's great.

00:56:39 But I think it's more like, okay, this is uncharted territory.

00:56:43 I'm working with an AI type interface.

00:56:47 And then it's more like, how do we, what's, how do we best do that as practitioners or as people building products?

00:56:54 I think that's the big idea there, you know, then the test library, you could probably write your own.

00:56:59 Like, I think for me, that was like a one or two week project.

00:57:02 The, what I would like to say as like normally, if it wasn't for getting all the help from ChatGPT on, you know, it's like, Oh, I'm creative project.

00:57:11 I'm setting up my setup.py, you know, set up tools is always a little bit of gas.

00:57:15 And then I'm like, can you help me create like my setup.py and then, you know, generate some code.

00:57:21 And I'm like, Oh, I want to make sure that PyPI is going to get my read me from GitHub.

00:57:27 I forgot how to read the markdown and pass the stuff.

00:57:30 Can you do that for me?

00:57:31 And then ShedGBT generates this stuff very nicely.

00:57:34 Right.

00:57:34 Or I want to make sure I use my request requirements of TXT inside my dynamically building my setup tools integration.

00:57:42 Can you do that for me?

00:57:43 And it's just like, bam, bam, bam.

00:57:45 Like all the repetitive stuff.

00:57:47 I need a function.

00:57:48 It's incredible.

00:57:48 Go ahead.

00:57:49 Yeah.

00:57:50 I kind of, well, I just, I kind of want to close out the conversation with that.

00:57:52 I do agree that the blog post is super powerful in how it kind of teaches you to think about how might you go about testing, integrating with an AI and these types of products, right?

00:58:03 Much like TDD brought a way to think about how do we actually apply the concept just, well, I have things and I can test them with this assert thing.

00:58:11 How should I actually go about building software, right?

00:58:13 So this is kind of that for AI integrated software.

00:58:16 So it's certainly worth people watching.

00:58:18 Let's just close it out with, you know, you kind of touched on some of those things there.

00:58:22 Like, you know, how do you recommend that people leverage things like ChatGPT to help them build, build their, their apps or how to use AI?

00:58:32 Just kind of, oh my God, yeah.

00:58:34 Like amp up your software development.

00:58:36 100%.

00:58:37 I mean, it's, it's been a lot of people report, you know, on Twitter, people used to Google, you know, all the problems that they had while writing, you know, writing code and using a lot of Stack Overflow.

00:58:51 I don't know what the stats on Stack Overflow traffic, but once you try working with Chatterjee PT to do coding, you probably don't go back to those other flows of, I don't know, it's like putting your error message or stack trace into Google and then go on to a bunch of Stack Overflow link and try to make sense of what comes out.

00:59:11 To me, it's been so much better to go just with Chatterjee PT, and there's a conversation there too.

00:59:16 So say, for instance, if I'm in Proptomize, I need a function to say, Can you write, I wrote that function before, you know, but it's a, can you crawl a certain given folder and look for modules that contain objects of a certain class and then bring that back?

00:59:31 And, you know, you have to use the import lib and she's a little bit pain in the ass to write this.

00:59:36 So it writes, you know, a function that works pretty well.

00:59:39 Then like, Oh yeah, I forgot to ask you to look into lists and dictionaries, can you do that too?

00:59:43 Then, Oh, it does that in a second.

00:59:45 It's like, I, you know, you didn't add like type hints and doc string and doc test and you write that too.

00:59:52 I just bang, bang, bang, and just copy paste in your utils file and it works, and you save two hours.

00:59:58 >> I think it would be really good at those things that are algorithmic.

01:00:03 Now, it might be the thing that you would do on a whiteboard job interview test.

01:00:08 It's just going to know that really, really solid.

01:00:11 It actually, but it knows quite a bit about the other libraries and stuff that are out there.

01:00:16 That's insane. Yeah, so one that I came across is I want to, I leverage something called link chain, which pointed to people getting interested in prompt engineering. There's a really good, well, the library link chain is really interesting.

01:00:30 It's not perfect, it's news moving fast, but push people to check it out.

01:00:34 Also like 41,000 stars, so very...

01:00:37 I know that's right.

01:00:38 And written in Python.

01:00:39 Yes, you can do like, yeah, it's in Python too.

01:00:42 You should talk to whoever is writing this or started this.

01:00:47 But yeah, you can change some prompt to say like the output of a prompt will generate the next one.

01:00:51 There's this idea of agents.

01:00:53 There's this idea of estimating tokens before doing the request.

01:00:58 There's a bunch of really cool things that it does.

01:01:02 To me, the docs are not that comprehensive.

01:01:04 There's someone else that created, if you Google "Lang chain cookbook", you'll find someone else that wrote what I thought was more comprehensive way to start.

01:01:16 But this one has a YouTube video and an IP1B file introduces you to the concept in an interactive way.

01:01:23 I thought that was really good.

01:01:25 But yeah, so we were trying to, I was trying to use this.

01:01:27 I was like, oh, Chad, can you generate a bunch of like land chain related stuff?

01:01:30 I was like, I don't know of a project calling chain that was created after 2021.

01:01:35 So I was like, I wish I could just say like, just go read the GitHub, you know, just read it all, read the docs.

01:01:40 And then I'll ask you questions.

01:01:42 And then Chajupiti is not that great at that currently, at learning things it doesn't know, for a reason we talked about.

01:01:50 BARD is much more up to date.

01:01:52 So you can always, for those projects, you know, Chajupiti might be better at Django 'cause it's old and settled, and it's better at writing code overall, but BARD might be decent and pretty good for that.

01:02:04 - Right, if you ask advice on how to do prompt-imized stuff, it's like, I don't know what that is.

01:02:07 - Yeah, it's like, I've never heard of, it might elucidate to, I think if you go in, might make shit up. Like I've seen it, front of my install, it would be this and it just makes up stuff.

01:02:17 So, not that great.

01:02:19 But yeah, absolutely I encourage people to try, you know, for any subtask that you're trying to do to see if it can help you at it and maybe try a variation on the front and then, yeah, if it's not good at it, just do it the old way.

01:02:33 But yeah, it might be better too for those familiar with the idea of like functional programming, where each function is is more deterministic and can be reasoned about and unit tested in isolation.

01:02:44 A Charged GPT is gonna be better at that 'cause it doesn't know about all your other packages and modules.

01:02:49 So really great for the utils functions are very deterministic, functional, super great at that.

01:02:55 Another thing is, and you tell me when we run out of time, another thing that I was really interesting too, bring some of the content and prompt them as they're writing the blog posts itself.

01:03:07 things like, hey, I'm thinking about the difference of like the properties of test driven development as it applies for engineering.

01:03:14 here's my blog post, but can you think of other differences between the two that are very core, you know, can you talk about the similarities and the differences and it would come up with like just really, really great ideas, right, brainstorming and very smart at mixing concepts.

01:03:33 I do think one thing that not a great idea is you say, write this for me, But if you've got something in mind and you're going to say, give me some ideas or how should I go?

01:03:41 Where should I go deeper into this?

01:03:43 And then you use your own creativity to create that.

01:03:46 That's a totally valid use.

01:03:48 I wouldn't feel like, oh, I mean, this AI crap, right?

01:03:50 It just it brought out some insights that you had forgot to think about.

01:03:54 And now you now you are right.

01:03:55 Or when it fails, it's just like I got it to fail.

01:03:58 I is wrong and smarter than it.

01:04:00 Do you like?

01:04:00 Wait, is there something I can can I try to, you know, here's what it didn't get right.

01:04:05 and why, like, what did I need to tell it?

01:04:07 So you can go and edit your prompt or ask a follow up and generally it will do better and well.

01:04:13 Yeah, I think also you can ask it to find bugs or security vulnerabilities.

01:04:17 Yeah.

01:04:17 Right? You know, like, here is my 30 line function.

01:04:20 Do you see any bugs?

01:04:21 Do you see any security vulnerabilities?

01:04:25 Like, yeah, you're passing this straight to, you're concatenating the string in the SQL or something like that.

01:04:31 Yeah, the regular stuff too.

01:04:34 Or like, you know, I would say writing a good doc string, writing doc tests, writing unit tests, reviewing the logic, that kind of stuff.

01:04:43 It does, type hints, right?

01:04:45 If you're like me, like, I don't really like to write type hints upfront.

01:04:49 But I'm like, can you just like sprinkle some type hints on top of that?

01:04:53 Retrofit this thing for me.

01:04:55 Yeah, that's it. Just make it that production grade.

01:04:57 You know, one thing that's interesting too, of like, you know, you would think I'm a big TDD guy, like, I don't do test driven development.

01:05:04 This is not my thing, I like to write code, I don't think of what I'm going to use the function for before I write it.

01:05:11 But it's good at generating unit tests for a function too.

01:05:17 And then I think what's interesting with Promptimize too is you might think you want deterministic, what I call prompt cases or test cases, but you can say I've written five or six of these, can you write variations on that theme too?

01:05:32 So you can use it to generate test cases in the case of like TDD, but also the opposite, like for, for prompt demise, you can get it to generate stuff dynamically to itself.

01:05:43 Yeah.

01:05:43 It's, it's pretty amazing.

01:05:45 It is.

01:05:45 It is pretty neat.

01:05:46 Let let's maybe close this out, but I'll ask you one more question.

01:05:48 Okay.

01:05:49 Can I do one more?

01:05:50 Can I show one more?

01:05:51 This is Python podcast.

01:05:52 If you go on my repo on that repo for prompt demise under examples, there's one called Python exam.

01:05:59 Yeah.

01:06:01 Something like this.

01:06:01 So let's start right here.

01:06:03 So say here it says, so here I wrote a prompt that asks the bot to generate Python function, then I sandbox it and bring the function it wrote into the interpreter, and then I test it.

01:06:16 So I say, write a function that tests if a number is a prime number, returns a boolean.

01:06:21 And then I test, I have six state test cases for it.

01:06:25 So write a function that finds the greatest common denominator of two numbers, right?

01:06:30 Then behind the scene, we won't get to the class above.

01:06:33 The class above basically interacts with it, gets the input, then runs the test, then compiles the results, right?

01:06:40 So we could test how well 3.5 compares to 4.

01:06:44 But I thought it was relevant for the Python folks on the line, so we're testing out what it is that writing Python function.

01:06:52 - Write a function that generates the Fibonacci sequence.

01:06:54 Yeah, very cool.

01:06:55 - Up to a certain number of terms, right?

01:06:57 So it's easy to test.

01:06:58 So it's cool stuff.

01:06:59 And what was your last question?

01:07:01 Oh, I was going to say something like, see if I see how far we can push it.

01:07:05 right.

01:07:06 A Python function to use requests and beautiful soup to scrape the titles of episodes of talk Python to me.

01:07:17 Oh yeah.

01:07:18 And then, yeah, it is.

01:07:20 And if you don't have a, you know, one thing that's a pain in the butt for podcasts, people as to write the, like what, what all we talk about.

01:07:26 So you use another AI to get the transcripts.

01:07:29 It's like, can you write something that's going to leverage this library to transcript the library, summarize it, publish it back with SEO in mind.

01:07:39 It's really quite amazing.

01:07:41 It went through and said, "Okay, here's a function." And it knows talkbython.m/episodes/all, use age, get the title.

01:07:48 And let's just finish this out, Max.

01:07:50 I'll throw this into an interpreter, see if it runs.

01:07:53 Interpreter and I'll see if I can get it to run.

01:07:55 - And you know what's really interesting too, is like you can give it a random function, like you can write a function and say like, if you write a certain function that does certain things and you say, if I give this input to this function, what is it gonna come out of?

01:08:11 And it doesn't have an interpreter, but it can interpret code like you and I do, right?

01:08:16 Like an interview question of like, hey, here's a function, if I input a three as a value, what's gonna come, what's gonna return?

01:08:23 So it's able to do it, follow the loops, follow the if statements and basically just do a logical.

01:08:29 - Yeah, another thing I think would be really good is to say, here's a function, explain to me what it does.

01:08:34 - Oh yeah, it's super great at that.

01:08:35 It's great at that for SQL too.

01:08:37 Here's a stupid long SQL query.

01:08:39 Can you explain to me?

01:08:40 No, it's like, the explanation is on, can you just summarize that in 300, 100 words?

01:08:45 - Yeah, let's go step by step.

01:08:47 Let's think step by step.

01:08:48 What's this do?

01:08:50 - Well, yeah, maybe a closing statement is like, this stuff is changing our world.

01:08:54 Like for me, I'm interested in how it's changing, how we're building products, you know?

01:08:58 But the core thing is that as a data practitioner, as a Python expert, as a programmer, it's really changing the way people work day after day faster than we all think.

01:09:12 And across a broad, like, you know, you might understand pretty well, it's changing your daily workflow as a software engineer, but it's changing people's workflow to do chemistry or like in every field, there's a lot we can leverage here if you use it well.

01:09:28 - Right, take this idea and apply it to, whatever vertical you wanna think of, it's doing the same thing there, right?

01:09:36 - 100%. - Medicine, all over, yeah, 100%, 100%.

01:09:39 All right, well, let's call it a wrap.

01:09:43 I think we're out of time here.

01:09:45 So really quick before we quit, PyPI package to recommend, maybe something AI related that you found recently, like all this thing's cool, people should check it out.

01:09:55 - Promptimize, I think it would be, you know, something to check out.

01:09:57 I think there's something called future tools that you could try to navigate there, but it shows like all of the AI powered tools that are coming out and it's hard to keep up.

01:10:07 - Yeah, yeah, I think I have seen that, yeah.

01:10:09 - And then if you wanna keep up on a daily with what's happening in AI, there's TLDR AI, they have like a DL with their relevant list for the day.

01:10:19 I think that's a, it's, it's hard to stay on.

01:10:22 I prefer like their weekly digestible.

01:10:25 What's going on in AI.

01:10:26 - It's too much of a, just a stream of information.

01:10:30 - Yeah, it's just get it dizzying.

01:10:32 And it's like, oh, this new model does this.

01:10:34 Like I've got to change everything to that.

01:10:35 And then something else, do you update the correct course too often and it's just like, you know, do not think because the foundation, the foundation is shifting too fast under you, so.

01:10:47 - Yeah, absolutely.

01:10:48 Well, very cool.

01:10:49 And then final question, you write the Python code, what editor are you using these days?

01:10:54 I'm a Vim user.

01:10:55 Yeah, I know it's not the best.

01:10:57 I know all the limitation, but it's like muscle memory.

01:11:01 And I'm a UX guy now working on SuperSense.

01:11:05 I do appreciate the development and all the new IDEs and the functionality that they have.

01:11:11 I think it's amazing.

01:11:12 It's just like, for me, it's all, like I know all my bash commands and big commands.

01:11:17 >> Absolutely. All right. Well, Max, thanks for coming on the show, helping everyone explore this wild new frontier of AI and large language models.

01:11:25 >> Yeah, well, we're exploring, but we're still relevant because I don't know how long we're going to be relevant for.

01:11:32 >> Yeah, enjoy it while we can, get out there.

01:11:36 Either control the robots or be controlled by them.

01:11:40 Get on the right side of that. All right. Thanks again.

01:11:44 This has been another episode of Talk Python to Me.

01:11:48 Thank you to our sponsors. Be sure to check out what they're offering. It really helps support the show.

01:11:52 The folks over at JetBrains encourage you to get work done with PyCharm. PyCharm Professional understands complex projects across multiple languages and technologies, so you can stay productive while you're writing Python code and other code like HTML or SQL.

01:12:08 Download your free trial at talkpython.fm/donewithpycharm done with PyCharm.

01:12:14 Listen to an episode of Compiler, an original podcast from Red Hat.

01:12:18 Compiler unravels industry topics, trends, and things you've always wanted to know about tech through interviews with the people who know it best.

01:12:26 Subscribe today by following talkpython.fm/compiler.

01:12:30 Want to level up your Python?

01:12:32 We have one of the largest catalogs of Python video courses over at Talk Python.

01:12:36 Our content ranges from true beginners to deeply advanced topics like memory and async.

01:12:41 And best of all, there's not a subscription in sight.

01:12:43 Check it out for yourself at training.talkpython.fm.

01:12:46 Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

01:12:51 We should be right at the top.

01:12:52 You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.

01:13:02 We're live streaming most of our recordings these days.

01:13:05 If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

01:13:13 This is your host, Michael Kennedy.

01:13:15 Thanks so much for listening.

01:13:16 I really appreciate it.

01:13:17 Now get out there and write some Python code.

01:13:19 (upbeat music)

01:13:22 [Music]

01:13:37 (upbeat music)

01:13:40 [BLANK_AUDIO]

