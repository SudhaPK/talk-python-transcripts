00:00:00 When you think about processing tabular data in Python, what library comes to mind?

00:00:05 Pandas, I guess.

00:00:06 But there are other libraries out there, and Polars is one of the more exciting new ones.

00:00:11 It's built in Rust, embraces parallelism, and can be 10 to 20 times faster than Pandas out of the box.

00:00:17 We have Polars creator, Richie Vink, here to give us a look at this exciting new DataFrame library.

00:00:23 history. This is Talk Python to Me, episode 402, recorded January 29th, 2023.

00:00:30 Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:48 Follow me on Mastodon where I'm @mkennedy and follow the podcast using @talkpython, both on bastodon.org.

00:00:56 Be careful with impersonating accounts on other instances, there are many.

00:00:59 Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:05 We've started streaming most of our episodes live on YouTube.

00:01:09 Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.

00:01:17 This episode is brought to you by TaiPi.

00:01:19 TaiPi is here to take on the challenge of rapidly transforming a bare algorithm in Python into a full-fledged decision support system for end users.

00:01:27 Check them out at talkpython.fm/taipi, T-A-I-P-Y.

00:01:32 And it's also brought to you by User Interviews.

00:01:35 Earn extra income for sharing your software developer opinion.

00:01:38 Head over to talkpython.fm/userinterviews to participate today.

00:01:43 Hey, Richie.

00:01:45 Welcome to Talk Python to Me.

00:01:46 Hello, Michael.

00:01:47 I feel like maybe I should rename my podcast, talk rust to me or something.

00:01:52 I don't know.

00:01:52 Rust is taken over as, as the low level part of, of how do we make Python go fast?

00:01:59 There's some kind of synergy with rust.

00:02:01 What's going on there?

00:02:02 Yeah, there is.

00:02:02 I'd say Python always already was low level languages that succeeded that made Python a success.

00:02:09 I mean, like NumPy, Pandas, everything that was reasonable fast was so because of C or Cyton, which is also C, but Rust, different from C, Rust has made low level programming a lot more fun to use and a lot more safe.

00:02:23 And especially if you regard multi-threaded programming, parallel programming, current programming, it is a lot easier in Rust.

00:02:31 There's a lot of possibilities.

00:02:33 Yeah.

00:02:34 My understanding, I've only given a cursory look to Rust, just sort of scan some examples and we're going to see some examples of code in a little bit, actually related to Polar's, It's kind of a low level language.

00:02:46 It's not as simple as Python, maybe a JavaScript, but it's, it is easier than C, C++, not just in the syntax, but you know, it has, it does better memory tracking for you and the concurrency especially.

00:02:59 Right?

00:03:00 Yeah.

00:03:01 Well, so Russ has got a, brings a whole new thing to the table, which is called ownership and a borrower checker.

00:03:05 And Russ is really strict.

00:03:07 There are things that Russ you cannot do in C or C++, because at a time there can only be one owner of a piece of memory and other people can you can lend out this piece of memory to other users but then they cannot mutate.

00:03:19 So it can be only one owner which is able to mutate something and this restriction make Rust a really hard language to learn but once you once it's clicked once you went over that that steep learning curve it becomes a lot easier because it doesn't allow you things that you could do in C and C++ but those things were also things you shouldn't do in C and C++ because they probably led to set all set to memory issues.

00:03:44 And this border checker also makes writing concurrent programming safe.

00:03:49 You can have many threads reading a variable all they want.

00:03:53 They can read concurrently.

00:03:54 It's when you have writers and readers that this whole thread safety, critical section, take your locks or the locks re-entrant, all of that really difficult stuff comes in.

00:04:04 And so, yeah, it sounds like an important key to making that.

00:04:08 And the same word checker also knows when memory has to be freed and not.

00:04:13 But it doesn't have to, unlike in Go or Java, where you have a garbage collector, it doesn't have to do garbage collection and it doesn't have to do reference counting by Python.

00:04:22 It does so by just statically. So at compile time, it knows when something is out of scope and not used anymore. And this is real power. I guess the takeaway for listeners who are wondering, you know, why is Rust seemingly taking over so much of the job that C And variations of C, right?

00:04:38 Like you said, Cython have traditionally played in Python.

00:04:41 It's easier to write modern, faster, safer code.

00:04:44 Yeah.

00:04:45 Okay.

00:04:45 Probably more fun too, right?

00:04:47 Yeah, definitely.

00:04:48 And it's a, it's a language which has got its tools, right?

00:04:51 So it's got a package manager, which is really great to use.

00:04:54 It's got a real great WL, which is similar to the PyPI index.

00:04:58 Feels like a modern language.

00:04:59 Yeah.

00:04:59 Builds low level, more low level code.

00:05:02 You can also write high level stuff like REST APIs, which is, I will say also for high level stuff, I like to write it in Rust because of the safety guarantees and also the correctness guarantees.

00:05:14 If my program compiles in Rust, I'm much more certain it is correct than when I write my Python program, which is dynamic and types are not enforced.

00:05:23 So it's always a bit fraying on that side.

00:05:26 Python is great to use, but it's harder to write correct code in Python.

00:05:30 Yeah.

00:05:30 And you can optionally write very loose code, or you could opt into things like type hints and even my PI, and then you get closer to the static languages.

00:05:40 Right.

00:05:41 Are you a fan of, um, on typing?

00:05:43 Definitely.

00:05:44 But because they're optional, they are as strong as the weakest link.

00:05:47 So one library, which you use, if it doesn't do the start, correct.

00:05:52 Or it doesn't do it.

00:05:53 Yeah.

00:05:53 It breaks.

00:05:54 It's, it's quite brittle because it's optional.

00:05:56 I hope we get something really enforces it and really can check it.

00:06:00 I don't know if it's possible because of the dynamic nature of Python.

00:06:04 Python can do so many things, job dynamically.

00:06:07 And technically we just cannot know from, I don't know, how far it can go.

00:06:12 But yeah, in Power BI as well, we use MyPy type prints, which prevent us from having a lot of bugs most of the way.

00:06:20 The IDE experience much nicer.

00:06:22 Yeah.

00:06:23 My prints are great.

00:06:24 help you also think about your library.

00:06:26 I think we really see a shift in modern Python and Python 10 years ago, where it was more dynamic and dynamic, the dynamic, I remember, I thought it depends on Python were more seen as a strength than currently I believe.

00:06:39 Yeah, I, I totally agree.

00:06:41 And I feel like when type pens first came out, you know, this was, yes.

00:06:44 Wow.

00:06:45 At this point, kind of early Python three, but it didn't feel like it at the time.

00:06:49 You know, Python three had been out for quite a while when type pens were introduced, I feel like that was Python three, four, but anyway, that was, put it maybe six years into the life cycle of Python three, but still, I feel like a lot of people were suspicious of that at the moment.

00:07:03 You know, they're like, Oh, what is this weird thing?

00:07:06 We're not really sure we want to put these types into our Python.

00:07:09 And now a lot less, there's a lot less of those reactions.

00:07:13 I feel.

00:07:13 Yeah.

00:07:14 Yeah.

00:07:14 I see Python having two, probably more, but I often see Python as the really fun, nice to your own, Duck tape language where I can, my princess and took the notebook, I can just hack away and try interactively what happens and for such code, pipelines don't matter, but once I write more of a library or product or tool, then pipelines were really great.

00:07:36 I believe they came about the Dropbox really needed them.

00:07:39 They have a huge pipeline.

00:07:40 It's really tough for me to get out.

00:07:43 I'm not really sure.

00:07:44 Yeah.

00:07:44 And I heard some guy who has something to do with Python used to work there.

00:07:47 Yeah.

00:07:47 Yeah.

00:07:47 I think even at that time.

00:07:50 All right.

00:07:51 So a bit of a diversion from how I often start the show.

00:07:54 So let's just circle back real quick and get your story.

00:07:56 How'd you get into programming and Python and Rust as well?

00:07:59 I suppose.

00:08:00 I got into programming.

00:08:01 I just wanted to learn programming.

00:08:02 A friend of mine who did, who programmed a lot of PHP said, learn Python.

00:08:07 Like that.

00:08:07 Or give me an interactive website where I could do some, some puzzles and I really got hooked to it.

00:08:14 It was a fun summer.

00:08:16 And, uh, programming a lot.

00:08:18 I started automating.

00:08:19 I, my job was civil engineer at the moment that I'd started.

00:08:22 It was a lot of mundane tasks, repetitive, and I just found ways to automate my job.

00:08:27 And eventually I was doing that for a year or three or, and then I got into data science and I switched jobs.

00:08:34 Uh, I became a data scientist and later a data engineer.

00:08:37 Yeah.

00:08:38 So then was Python.

00:08:39 Mostly I've always been looking for more languages, playing with Haskell, playing with Go, playing with Dubstrip, or just playing with Scala.

00:08:49 And then I found Rust.

00:08:51 And Rust really, really, like you learn a lot about how computers work.

00:08:56 Yeah.

00:08:57 Yeah.

00:08:57 So I had a new renaissance of the first experience with Python.

00:09:00 And now this summer at Rust, I've been doing a lot of other projects, like writing and interpreter, I don't know, a lot of projects and always getting one of those hobby projects just to use Rust more.

00:09:12 Now it's, uh, it's got quite the following and we're going to definitely dive into that, but let me pull it up.

00:09:18 It does right here.

00:09:19 13,000 get up stars.

00:09:21 That's a good number of people using that project.

00:09:25 Yeah.

00:09:25 Yeah.

00:09:25 Crazy.

00:09:25 Isn't it?

00:09:26 Yeah, it is.

00:09:27 It's it's, um, on GitHub stars, it's the fastest where I will data to agree.

00:09:31 Wow.

00:09:32 Incredible.

00:09:32 It, um, you must be really proud of that.

00:09:35 Yeah.

00:09:35 Yeah.

00:09:36 If you would have told me this two years ago, I would never be, but it was, it happen slow enough so you can get accustomed to that.

00:09:43 Yeah.

00:09:43 That's cool.

00:09:44 Kind of like being a parent.

00:09:46 The challenges that the kids are, are small, they they're, they're intense, but there are only a few things they need when they're small and you grow, you kind of grow with it.

00:09:53 So a couple of thoughts, one, you had the inverse style of learning to program that I think a lot of computer science people do, and certainly that I did, that could also just be that I learned it a long time ago, but when I learned programming, it was, I'm going to learn C and C plus plus.

00:10:10 And then you're kind of allowed to learn the easier languages, but you will learn your pointers.

00:10:15 You'll have your void star star, and you're going to like it.

00:10:18 You're going to understand what a pointer to a pointer means.

00:10:21 I mean, you start inside of the most complex, closest to the machine.

00:10:27 You work your way out.

00:10:28 You kind of took this opposite, like let me learn Python, where it's much more high level.

00:10:32 It's much-- if you choose to be, often say, very much more away from the hardware and the ideas of memories and threads and all that.

00:10:40 And then you went to rust.

00:10:41 So was it kind of an intense experience where you're like, Oh my gosh, this is intense, or had you studied enough languages by then to become comfortable?

00:10:49 Well, yeah, yeah, no.

00:10:50 So the going from high level to low level, I think it makes natural sense.

00:10:55 You're learning yourself.

00:10:56 There's no professor telling me you learn your pointers.

00:10:59 Yeah.

00:11:00 I think this also helped a lot because at that point you're really custom programming to algorithms.

00:11:06 Yeah.

00:11:06 So you can, I believe you should learn one thing, one new thing at a time, and that you can really own that knowledge later on.

00:11:14 But Rusts, I wouldn't say you should learn Rusts as a first language.

00:11:17 It would be really terrible.

00:11:19 That would be terrible.

00:11:22 But other languages also don't help you much because the PowerChecker, it's quite unique.

00:11:26 It doesn't let you do things you can do in other languages.

00:11:29 So what you learn there, the languages that allow you to do that, they just purge because So you can do a lot of things with it.

00:11:37 And you can do a lot of things with it.

00:11:38 And you can do a lot of things with it.

00:11:39 And you can do a lot of things with it.

00:11:40 And you can do a lot of things with it.

00:11:41 And you can do a lot of things with it.

00:11:42 And you can do a lot of things with it.

00:11:43 And you can do a lot of things with it.

00:11:44 And you can do a lot of things with it.

00:11:45 And you can do a lot of things with it.

00:11:46 And you can do a lot of things with it.

00:11:47 And you can do a lot of things with it.

00:11:48 And you can do a lot of things with it.

00:11:49 And you can do a lot of things with it.

00:11:50 And you can do a lot of things with it.

00:11:51 And you can do a lot of things with it.

00:11:52 is really clear who owns the memory, how deep your nesting is.

00:11:56 It's always one D deeper.

00:11:58 Most of the times it's, it's not that complicated.

00:12:01 You, you make things really bad and really isn't easy to reason about.

00:12:05 And in the beginning of project seems okay, but over-constraining, but when I mean software will become complex and complicated, and then you're happy to compile a notch to this.

00:12:16 Yeah, absolutely.

00:12:17 In this direction.

00:12:18 It seems like a better way, honestly.

00:12:20 You get a sense of programming in a more simple language that doesn't ask so many low-level concepts of you.

00:12:28 And then you can add on these new ones.

00:12:30 So I feel like a lot of how we teach programming, how people learn programming is a little bit backwards, to be honest.

00:12:36 Anyway, enough on that.

00:12:37 So you were a civil engineer for a while, and then you became a data scientist, and now you've created this library.

00:12:43 Still working as a data scientist now?

00:12:45 - No, no.

00:12:45 I got sponsored two years ago, put two days a week and yeah, just to use the time to get a partner.

00:12:53 And currently I thought all my day jobs, you know, going full time or there's, I'm trying to live on sponsorships, which is not really working well enough at this time.

00:13:03 I hope to start a foundation and get some proper sponsorship.

00:13:07 Yeah.

00:13:07 That'd be great.

00:13:08 Yeah.

00:13:08 That's awesome.

00:13:10 It's still awesome that you're able to do that, even if you know, you still need it to grow a little bit.

00:13:14 Yeah.

00:13:15 we'll have you on a podcast and let other people know out there who, who maybe are using your library, maybe they can, you know, put a little sponsorship and get up sponsors.

00:13:23 I feel like get up sponsors really made it a lot easier for people to, to support.

00:13:29 Cause there used to be like PayPal donate buttons and other, other things like that.

00:13:34 And one, those are not really recurring.

00:13:37 And two, you've got to go find someplace and put your credit card.

00:13:40 Many of us already have a credit card registered at GitHub.

00:13:44 It's just a matter of checking the box and monthly, it'll just go, you know, it's kind of like the app store versus buying independent apps.

00:13:49 It just cuts down a lot of friction.

00:13:51 I feel like it's been really positive mostly for open source.

00:13:55 Yeah.

00:13:55 I think it's good to, as a way to say, thank you with them.

00:13:59 It doesn't need not to pay the bills.

00:14:00 I think for most people, it isn't, but, um, I hope we get there.

00:14:04 Big companies who use it should give a bit more, a bit more back.

00:14:08 I mean, you have a lot of money.

00:14:09 I agree.

00:14:10 It's really, really ridiculous that there are banks and VC funded companies and things like that, that have not necessarily in terms of the VC ones, but definitely in terms of financial and other large companies that make billions and billions of dollars in profit on top of open source technology.

00:14:27 And many of them don't give anything back, which is, it's not criminal because the licenses allow it, but it's, it's certainly borders on immoral to say, we made all this money and not at all support the people who are really building the foundations that we build upon.

00:14:43 Most of my sponsors are developers.

00:14:44 Yeah.

00:14:44 Yeah.

00:14:45 So, uh, yeah, we'll, let's hope it changes.

00:14:48 I don't know.

00:14:49 Yeah.

00:14:49 Well, I'll continue to beat that drum.

00:14:52 This portion of talk Python to me is brought to you by Type I Type I is the next generation open source Python application builder with Type I you can turn data and AI algorithms into full web apps in no time.

00:15:07 Here's how it works.

00:15:08 You start with a bare algorithm written in Python.

00:15:11 You then use TypeEye's innovative tool set that enables Python developers to build interactive end-user applications quickly.

00:15:18 There's a visual designer to develop highly interactive GUIs ready for production, and for inbound data streams, you can program against the TypeEye Core layer as well.

00:15:27 TypeEye Core provides intelligent pipeline management, data caching and scenario and cycle management facilities.

00:15:33 That's it, you'll have transformed a bare algorithm into a full-fledged decision support system for end users.

00:15:40 Type-I is pure Python and open source, and you install it with a simple pip install type-I.

00:15:45 For large organizations that need fine-grained control and authorization around their data, there is a paid Type-I Enterprise Edition, but the Type-I core and GUI described above is completely free to use.

00:15:56 Learn more and get started by visiting talk, by Thon dot FM slash type high.

00:16:01 That's T a I P Y the links in your show notes.

00:16:04 Thank you to type high for sponsoring the show.

00:16:06 Let's talk about your project.

00:16:10 So Polars and the RS is for rust.

00:16:14 I imagine at the end, but tell us about the name Polars, like Polar bear.

00:16:18 But Polars.

00:16:19 Yeah.

00:16:19 So I started writing a data from library and initially it was only for, for us.

00:16:24 And I wanted to give a week to the pandas project, but I wanted a beer that was better, faster.

00:16:29 I don't know, stronger.

00:16:30 So luckily a Panda beer is the most practical beer.

00:16:35 So I had a few to choose, to choose.

00:16:38 But the Polaris has the RRF.

00:16:41 So that's a lucky coincidence.

00:16:42 Yeah.

00:16:43 Yeah.

00:16:44 So the subtitle here is lightning fast, but you can't get it.

00:16:48 You can't get it.

00:16:48 You can't get it.

00:16:49 But you can get it.

00:16:50 And it's a, it's a, it's a really, really good beer.

00:16:52 So the subtitle here is lightning fast data frame library for rust and Python.

00:16:57 And you have two APIs that people can use.

00:17:00 We'll get to dive into those.

00:17:01 Yeah.

00:17:02 Cause we read an angle and Rusts, it's a complete data from library and Rusts thing you can expose that to many fun bits, but we have this already front end and Rusts Python, no JS.

00:17:11 R is coming up and normal JavaScript is coming up and Ruby.

00:17:16 There's also a polar to group.

00:17:18 So, how interesting.

00:17:20 So for the JavaScript one, are you going to use web assembly?

00:17:23 Yeah.

00:17:23 Right.

00:17:23 Which is pretty straightforward because Rust comes from Mozilla web assembly, I believe also originated.

00:17:29 They kind of originated as a somewhat tied together story.

00:17:33 Yeah.

00:17:33 C++ C can compile to web assembly.

00:17:36 It's not really straightforward because the web assembly virtual machine isn't like your normal OS.

00:17:41 So there are a lot of things harder, but we're, we are working on it.

00:17:44 Okay.

00:17:45 Well, that's pretty interesting, but for now you got Python and you've got Rust.

00:17:49 And that's great.

00:17:50 Let's, I think a lot of people listening, myself included, when I started looking into this immediately go to, it's like pandas, but rust, you know, it's like pandas, but instead of C at the bottom, it's, it's rust at the bottom.

00:18:04 And that's somewhat true, but mostly not true.

00:18:07 So let's start with you telling us, you know, how is this like pandas and how is it different from pandas?

00:18:13 Yeah.

00:18:13 So it's not like pandas.

00:18:16 I think it's different on two ways.

00:18:18 So we have the API and we have the page.

00:18:21 And which one should I start with?

00:18:23 Bottom up?

00:18:23 That's I think bottom up.

00:18:25 Yeah.

00:18:25 Yeah.

00:18:25 Bottom up.

00:18:26 Sure.

00:18:26 All right.

00:18:26 So that was my critique from pandas.

00:18:29 And if they didn't start bottom up, they do whatever was there already with work for that purpose.

00:18:36 And pandas built on NumPy.

00:18:39 And NumPy is a great library.

00:18:40 It's, well, it's built for numerical processing and not for relational processing, relational data is completely different.

00:18:47 You have string data, nested data, and this data is probably as put as Python object in those NumPy arrays.

00:18:54 And if you know anything about, about memory, then in this array, you have pointer with where each Python object is somewhere else, so if you traverse this memory, every pointer you emit must look it up somewhere else, but memory is not a cache, but cache miss, which is a 200x slowdown per element you traverse.

00:19:11 Yeah.

00:19:11 So for people listening, what you're saying the 200x slowdown is, the L1, L2, L3 caches, which all have different speeds and stuff, but the caches that are near the CPU versus main memory.

00:19:24 It's like two to 400 times slower, not aging off a disk or something. It's really different, right? It's really a big deal.

00:19:30 It's a big deal. It's terribly slow. It also, Python has a gill. It also blocks multi-threading.

00:19:36 If you want to read the string, you cannot do this on different threads.

00:19:40 If you want to modify the string, there's only one thread that connects this five-finger.

00:19:44 So they also didn't take into account anything from databases.

00:19:49 So databases are facing from the 1950s.

00:19:53 There's been a lot of research in databases and how we do things fast, write a query and then optimize this query because the user that uses your library is not the expert.

00:20:03 It doesn't write optimized query.

00:20:04 No, but we have a lot of information so we can optimize this query and execute this in the most, in a very efficient way.

00:20:11 Well, that's an interesting idea.

00:20:13 Yeah, and Pongos just executes it and gives you what you ask.

00:20:17 What you ask is probably not...

00:20:18 Yeah, that's interesting because as programmers, when I have my Python hat on, I want my code to run exactly as I wrote it.

00:20:26 I don't want it to get clever and change it.

00:20:30 If I said do a loop, do a loop.

00:20:31 If I said put it in a dictionary, put it in a dictionary.

00:20:35 But when I write a database query, be that against Postgres with relational or MongoDB, There's a query planner and the query planner looks at all the different steps.

00:20:45 Should we do the filter first?

00:20:47 Can we use an index?

00:20:48 Can we use a compo, which index should we choose?

00:20:51 All of those things, right?

00:20:52 And so what you tell it and what happens, you don't tell it how to do finding the data, the database, you just give it, here's kind of the expressions that I need, the, the, the predicates that I need you to work with.

00:21:04 And then you figure it out.

00:21:06 You're smart.

00:21:06 You're the database.

00:21:07 So one of the differences I got from reading what you've got here so far is it looks like, I don't know if it goes as far as this database stuff that we're talking about, but there's a way for it to build up the code it's supposed to run.

00:21:20 And it can decide things like, you know, these two things could go in parallel or things along those lines.

00:21:25 Right?

00:21:26 Yeah.

00:21:26 Yeah.

00:21:26 Well, it is actually very similar.

00:21:29 It is a vectorized query engine.

00:21:30 And you can, the only thing that doesn't make us a database is that we don't have any, we don't bother with, uh, with file structures.

00:21:38 And we've right.

00:21:38 Like the persistence and transactions and all that.

00:21:41 So we have different kinds of late databases, you have OLAP and OLTP transactional modeling, which works often on one, but if you do a rest API query and you modify one user ID, then your transactional, and if you're doing OLAP, that's more analytical and then you do large aggregations of large old tables.

00:21:59 And then you need to process all the data and those different databases So you can see that we have a lot of different things that we can do.

00:22:06 And I think that's a really cool feature.

00:22:08 And I think that's a really cool feature.

00:22:09 And I think that's a really cool feature.

00:22:10 And I think that's a really cool feature.

00:22:11 And I think that's a really cool feature.

00:22:12 And I think that's a really cool feature.

00:22:13 And I think that's a really cool feature.

00:22:14 And I think that's a really cool feature.

00:22:15 And I think that's a really cool feature.

00:22:16 And I think that's a really cool feature.

00:22:17 And I think that's a really cool feature.

00:22:18 And I think that's a really cool feature.

00:22:19 And I think that's a really cool feature.

00:22:20 And I think that's a really cool feature.

00:22:21 And I think that's a really cool feature.

00:22:22 And I think that's a really cool feature.

00:22:23 And then we write down the whole algorithm, how to get a copy.

00:22:26 You could just say, get me a healthy Nintendo.

00:22:28 I'd like some sugar and then let the query engine decide how to test it.

00:22:34 And that's more declarative.

00:22:35 You describe the end result.

00:22:38 And as it turns out, this is also very readable because you declare what you want and the intent is readable in the query.

00:22:45 And if you're doing more procedural programming, you describe what you're doing and the intent often needs to come from comments.

00:22:52 And then we have a lot of things that we can do with the API.

00:22:54 So we have a lot of things that we can do with the API.

00:22:57 And then we have a lot of things that we can do with the API.

00:22:59 And then we have a lot of things that we can do with the API.

00:23:02 And then we have a lot of things that we can do with the API.

00:23:04 And then we have a lot of things that we can do with the API.

00:23:06 And then we have a lot of things that we can do with the API.

00:23:08 And then we have a lot of things that we can do with the API.

00:23:10 And then we have a lot of things that we can do with the API.

00:23:12 And then we have a lot of things that we can do with the API.

00:23:14 And then we have a lot of things that we can do with the API.

00:23:16 And then we have a lot of things that we can do with the API.

00:23:18 exposed on this API and then we noticed how bad it was for writing fast data.

00:23:23 On this API it just isn't really for this declarative analyzing of what the user wants to do.

00:23:30 So we just cut it off and took the freedom to design an API that makes most sense.

00:23:34 Well, that's interesting.

00:23:36 I didn't realize that you had started trying to be closer to pandas than you ended up.

00:23:40 Yeah.

00:23:41 Well, it was very short-list I must say.

00:23:43 It was painful.

00:23:44 Yeah. And that's not necessarily saying pandas are bad. I don't think it's approaching the problem differently and it has different goals. Right. Yeah.

00:23:52 So maybe we could look at an example of some of the code that we're talking about, I guess also one of the other differences there is much of this has to do with what you would call, I guess you refer to them as lazy APIs or streaming APIs, kind of like a generator.

00:24:08 Yeah. So what you think about a join, for instance, in pandas, if you would right at join and then take only do and only want to first 100 rows with that result, then it would first do the join.

00:24:20 And then that might produce 1 million, 10 million rows.

00:24:23 And then you take only 100 of them and then you have materialized a million, but you take only a fraction of that.

00:24:29 And by having that lazy, you can, can optimize for the whole query at a time and just see how we do this.

00:24:35 Join, but we only need 100 rows.

00:24:36 So that's how we materialize.

00:24:38 That gets you more realistic.

00:24:40 That's really cool.

00:24:41 I didn't realize it had so many similarities to databases, but yeah, it makes a lot of sense.

00:24:46 All right, let's look at maybe a super simple example you've got on polar.rs.

00:24:53 What country is rs?

00:24:54 I always love how different countries that often have nothing to do with domain names get grabbed because they have a cool ending like Libya that was .ly for a while.

00:25:04 You know, it still is, but like it was used frequently like bit.ly and stuff.

00:25:07 Do you know what rs is?

00:25:08 - I believe it's Serbia.

00:25:10 - Serbia, okay, cool. - I'm not sure.

00:25:12 - Yeah, yeah, very cool.

00:25:13 All right, so polar.rs, like polar.rs.

00:25:17 Over here, you've got on the homepage here, the landing page, and then through the documentation as well, you've got a lot of places where you're like, show me the Rust API or show me the Python API.

00:25:26 People can come and check out the Rust code.

00:25:29 It's a little bit longer because it's that kind of language, but it's not terribly more complex.

00:25:35 But maybe talk us through this little example here on the homepage in Python, just to give people a sense of what the API looks like.

00:25:42 Yeah. So we started with a scanned CSV, which is a lazy read, which is, so read CSV tells what to do.

00:25:50 And then it reads the CSV and you get the data frame.

00:25:52 And a scanned CSV, we start a computation graph.

00:25:56 It's called a lazy frame.

00:25:57 And a lazy frame is actually just, it holds, it remembers the steps of the operations you want to do.

00:26:02 Then it tells it, oh, large, what it looks at this, this very plan and optimize it, and we'll think of how to execute it.

00:26:09 And we have different engines, so you can have an engine that's more specialized for data that doesn't fit into memory and an engine that's more specialized for data that does fit into memory.

00:26:17 So we start with a scan and then we do a doc filter and we want to use verbs.

00:26:23 Verbs, that's the declarative part.

00:26:26 In pandas we often do indexes or a, and those indexes are ambiguous in my opinion, because you can, you can pass in a NumPy array with booleans, but you can also pass in a NumPy array with integers.

00:26:38 So you can do slicing, you can also pass in a list of strings, and then you do column selection.

00:26:44 So it has three functions.

00:26:46 One thing that I find really interesting about Pandas is it's so incredible, and people who are very good with Pandas, they can just make it fly.

00:26:55 They can make it really write expressions that are super powerful, but it's not obvious that you should have been able to do that before you see it.

00:27:02 You know, there's a lot of not quite magic, but stuff that doesn't seem to come really straight out of the API directly.

00:27:10 You pass in some sort of a Boolean expression that involves a vector and some other test into the brackets.

00:27:19 I wait, how do I know I can do that?

00:27:21 Whereas this, your API is a lot more of a fluent API where you say, PD would say PL, PL.scan, CSV.filter.groupby.aggregate.collect, And it kind of just flows together.

00:27:35 Does that mean that the editors and IDEs can be more helpful suggesting what happens at each step?

00:27:41 Yes, we are really strict on type.

00:27:43 So we also only return a single type from a method and we only a dot filter just expects a Boolean expression that produces a Boolean, not only integer and not a string.

00:27:54 So we want our methods from reading or code.

00:27:58 You should be able to understand what should go in.

00:28:00 That's really important to me.

00:28:02 It should be unambiguous.

00:28:04 It should be consistent and you, your knowledge of the API should expand to different parts of the API.

00:28:08 And that's where I think we'll talk about this later, but that's where expressions are going to be coming over.

00:28:14 This portion of talk Python to me is brought to you by user interviews.

00:28:19 As a developer, how often do you find yourself talking back to products and services that you use?

00:28:25 Sometimes it may be frustration over how it's working poorly.

00:28:29 And if they just did such and such, it would work better and it's easy to do.

00:28:35 Other times it might be delight.

00:28:36 Wow, they auto-filled that section for me.

00:28:39 How did they even do that?

00:28:40 Wonderful.

00:28:41 Thanks.

00:28:42 While this verbalization might be great to get the thoughts out of your head, did you know that you can earn money for your feedback on real products?

00:28:49 User interviews connects researchers with professionals that want to participate in research studies.

00:28:55 There is a high demand for developers to share their opinions on products being created for developers.

00:29:00 Aside from the extra cash, you'll talk to people building products in your space.

00:29:04 You will not only learn about new tools being created, but you'll also shape the future of the products that we all use.

00:29:11 It's completely free to sign up and you can apply to your first study in under five minutes.

00:29:16 The average study pays over $60.

00:29:18 However, many studies specifically interested in developers pay several hundreds of dollars for a one-on-one interview. Are you ready to earn extra income from sharing your expert opinion?

00:29:29 Head over to talkpython.fm/userinterviews to participate today. The link is in your podcast player show notes. Thank you to User Interviews for supporting the show.

00:29:38 I just derailed you a little bit here as you were describing this. So you start out with scanning a CSV, which is sort of creating and kicking off a data frame equivalent here.

00:29:53 And then you, a lazy frame.

00:29:55 Okay.

00:29:55 And then you say a dot filter and you give it an expression like this column is greater than five, right.

00:30:00 Or some expression that we would understand in Python.

00:30:03 And that's the filter statement.

00:30:05 Right?

00:30:05 Yeah.

00:30:05 And then we follow it a group by argument and then an aggregation where we say, okay, take all columns and some of them.

00:30:12 And this again, as an expression, these are really easy expressions.

00:30:15 And then we take this lazy frame and we materialize it into a data framework called connect.

00:30:21 And collect means, okay, all those steps you recorded, now you can do your magic, query optimizer, get all the stuff.

00:30:29 And what this will do here, it will recognize that, okay, we've taken the iris.csv, which got different columns.

00:30:35 And now in this case, it won't.

00:30:37 So if you would have finished with a semect, where we only selected two columns, it would have recognized, oh, we don't need all those columns in the cc file.

00:30:46 We only take the ones we need.

00:30:47 it will do, it will push the filter, the predicate down to the scan.

00:30:51 So during the reading of the CSV, we will take this predicate.

00:30:55 We say, okay, where the sample length is larger than five, the rows that don't match the predicate will not be materialized.

00:31:01 So if you have a really large CSV file, if we really, let's say you have a CSV file with tens of gigabytes, but your, your predicate only selects 5% of that.

00:31:11 Then you only materialize 5% of the 10 gigabytes.

00:31:14 Yeah.

00:31:14 So 500 megs instead of 10 gigabytes or something like that.

00:31:17 or 200 megs, whatever it is, quite a bit less.

00:31:20 That's really interesting.

00:31:22 And this is all part of the benefits of what we're talking about with the lazy, lazy frames, lazy APIs and and building up all of the steps before you say go.

00:31:32 Because in Pandas, you would say read CSV.

00:31:34 So, okay, it's going to read the CSV.

00:31:36 Now what?

00:31:36 - Yes. - Right?

00:31:37 And then you apply your filter if that's the order you want to do it in.

00:31:40 And then you group and then and so on and so on, right?

00:31:42 - Right. - It's interesting in that it does allow more database like behavior behind the scenes.

00:31:48 Yeah, yeah. And yet, in my opinion, the data frame is should be seen as a table in a database.

00:31:54 It's the final view of computation. Like you can see it as a materialized view.

00:31:59 It's we have some data on this and we want to get it into another table, which we would feed into our machine learning model or whatever. And we do a lot of operations on them before we get So I wouldn't see a data frame as a data.

00:32:16 It's not only a data structure, it's not only a list or a dictionary.

00:32:20 There are lots of steps before we get into those tables eventually.

00:32:25 Right. So here's an interesting challenge.

00:32:28 There's a lot of visualization libraries.

00:32:32 There are a lot of other data science libraries that know and expect Pandas data frames.

00:32:39 So like, okay, what you do is you send me the Pandas data frame here, where we're going to patch pandas so that if you call this function on the data frame, it's going to do this thing. And they may say, "Richie, fantastic job you've done here in Polars, but my stuff is already all built around pandas, so I'm not going to use this." But it's worth pointing out there's some cool pandas integration, right?

00:32:58 Yeah. So Polars doesn't want to do plotting. I don't think it should be in the data frame.

00:33:04 Maybe another library can do it on top of Polars, just that you like it.

00:33:09 It shouldn't be a part of it in my opinion, but often when you do plotting, you're plotting the number of rows will not be billions.

00:33:16 I mean, there's no plotting engine that can deal with that.

00:33:19 So you will be reducing your, your big data sets to something small, and then you can send it to the plot.

00:33:24 Yeah.

00:33:25 There's hardly a monitor that has enough pixels to show you that.

00:33:29 Right.

00:33:29 Right.

00:33:29 So yeah.

00:33:30 Yeah.

00:33:31 You can call it two pandas and then we transform our Polish data frame to pandas, and then you can integrate with, I could learn with, and we often find that progressively rewriting some fondos into polars already is cheaper than keeping it in fondos.

00:33:45 If you do a, if you call from fondos, polars do a join in polars and then back to fondos, we probably made up for those double copies.

00:33:52 Fondos does a lot of internal copies.

00:33:54 If you do a reset index, copies all data.

00:33:56 If you do, there are a lot of internal copies in fondos which aren't listed, so I wouldn't worry about an explicit copy in the end of your ETL to go to plotting when the data is already.

00:34:07 - Right, right.

00:34:08 So let's look at the benchmarks 'Cause it sounds like to a large degree, even if you do have to do this conversion in the end, many times, it still might even be quicker.

00:34:17 So you've got some benchmarks over here, and you compared, I'm gonna need some good vision for this one, you compared Polar's, Panda's, Dask, and then two things which are too small for me to read.

00:34:28 Tell us what you compared.

00:34:29 - Moding and Vax.

00:34:30 - Moding and Vax, okay.

00:34:31 And for people listening, you go out here and look at these benchmarks, they're linked right off the homepage.

00:34:38 There's like a little tiny purple thing and a whole bunch of really tall bar graphs at the rest and the little tiny thing that you can kind of miss.

00:34:46 If you don't look carefully, that's the time it takes for.

00:34:49 Polars.

00:34:50 And then all the others are up there in like 60 seconds, a hundred seconds.

00:34:54 And then Polars is like quarter of a second.

00:34:57 So, you know, it's, it's easy to miss it in the graph, but the quick takeaway here, I think is this some fast stuff.

00:35:03 Yeah.

00:35:03 Yeah.

00:35:03 We're often orders of magnitude faster than pandas.

00:35:06 So it's not uncommon to hear it.

00:35:08 then 20x storage files, especially if you write proper pandas and proper portals, it's probably 20x if we deal with I/O as well.

00:35:17 So what we see here are the TPCH benchmarks.

00:35:20 TPCH is a database query benchmark standard, which is used by every query engine to show how fast it is.

00:35:29 And those are really hard questions that really, really collects the muscles of a query engine.

00:35:35 So you have joins on several tables, different group bys, different nested group bys, et cetera.

00:35:41 And yeah, yeah, I really tried to make those other tools faster.

00:35:44 But so in memory, Dask and Modin, it was really hard to make stuff faster than Pandas except for Podas.

00:35:51 Well, on a few occasions.

00:35:53 Once we include I/O, all those tools first needed to go via Pandas.

00:35:58 And yeah, what this sort of shows is that we have Pandas, which is a single threaded data frame and data frame engine.

00:36:05 And then we have tools that parallelize funnels and it's not always, they don't just parallelizing funnels doesn't make it faster.

00:36:13 So if we have a filter or a element wise modification, parallelization is easy.

00:36:18 You just split it up in chunks and do your parallelization and then those tools go through it.

00:36:23 Yeah.

00:36:23 10 cores, you can start 10 threads and I can take one 10th of the data and start to answer yes or no for the filter question, for example, right?

00:36:31 people don't realize that a lot of data frame operations are not embarrassingly parallel.

00:36:36 A group by is definitely not embarrassingly parallel.

00:36:39 A filter or sorry, a join needs to shuffle.

00:36:42 Doesn't not embarrassingly parallel.

00:36:45 That's why you see those tools being slower than pandas.

00:36:48 Because they, their string data and they, then you have problems.

00:36:52 We need to do multi-processing or we need to send those Python objects to, to, to another project and we copy data, which is slow or we need to do multi-threading and we're bound by the GIL over single thread, and then there are steeper benches.

00:37:03 Yeah, I think there's some interesting parallels for Dask and Polars.

00:37:09 On these benchmarks, at least, you're showing much better performance than Dask.

00:37:13 I've had Matthew Rocklin on a couple times to talk about Dask and some of the work they're doing there.

00:37:18 It's very cool, and one of the things that I think Dask is interesting for is allowing you to scale your code out to multi-cores on your machine, or even distributed grid computing or process data that doesn't fit in memory.

00:37:33 And they can behind the scenes juggle all that for you.

00:37:36 I feel like Polar's kind of has a different way, but attempts to solve some of those problems as well.

00:37:42 The Polar has full control over it.

00:37:44 Over everything.

00:37:45 So it's built from the ground up.

00:37:47 It controls I/O, it controls their own memory, it controls which trap gets which data.

00:37:52 And in DOS, it goes through, it takes this other tool and then parallelizes that.

00:37:57 but it is limited by what this other tool also is limited by.

00:38:01 But I think, so on a single machine, it has those challenges.

00:38:04 I think Dask Distributor does have these challenges.

00:38:07 And I think for Distributor, they work really well.

00:38:10 Yeah, the interesting part with Dask, I think, is that it's kind of like Pandas, but it scales in all these interesting ways.

00:38:17 AcRusts cores, bigger memory, but also acRusts machines, and then, you know, acRusts cores, acRusts machines, like all that stuff.

00:38:23 I feel like Dask is a little bit, maybe it's trying to solve like a little bit bigger compute problem, like how can we use a cluster of computers to answer these questions?

00:38:32 The documentation also says it for themselves.

00:38:34 They say that they're probably not faster than Pandas on a single machine.

00:38:38 So they're more for the, for the large big data.

00:38:41 Yeah.

00:38:41 But Pandas wants to be, and a lot faster on a single machine, but also wants to be able to do out of store processing on a single machine.

00:38:48 So if you, we don't support all queries yet, but we want to, we already do basic Devoids, groupbytes, sorts, predicates, element wise operations.

00:38:58 And then we can process, uh, I process Python gigabytes on my, yeah.

00:39:02 On my laptop.

00:39:02 That's pretty good.

00:39:04 Your laptop probably doesn't have 500 gigs.

00:39:06 No, no, no, no.

00:39:06 It's 16 gigs.

00:39:07 Yeah.

00:39:08 Nice.

00:39:09 It's probably actually a value to, as you develop this product to not have too massive of a computer to work on.

00:39:16 If you had a $5,000 workstation, you know, you might be a little out of touch with many people using your code and so on.

00:39:25 Although I think there, I think boilers like scaling on a single machine makes sense for different reasons as well.

00:39:32 I think a lot of people talk about distributed, but if you think about the complexity of distributed, you need to send data, shuffle data over the network to other machines.

00:39:41 So there are a lot of people using boilers in our discord who have one terabyte of rack and say it's cheaper and a lot faster than Spark because They can walk all this fast on a single machine.

00:39:52 And while two, they have a, a beefy machine with like one on the 20 boards and they don't have to go over the network to parallelize.

00:40:01 And yeah, so I think times are changing.

00:40:04 I think also scaling out data on a single machine is getting more and more.

00:40:08 It is one of the areas in which it's interesting is GPUs.

00:40:12 Do you have any integration with GPUs or any of those sort of things?

00:40:15 Not suggesting that necessarily is even a good idea.

00:40:18 I'm just wondering if it does.

00:40:19 I get this question, but I'm not really convinced I can get the memory.

00:40:23 I can get the data fast enough into the memory.

00:40:25 I, we want to process gigabytes of data.

00:40:28 I, the challenge already on the CPU is, is getting the data or cache or memory fast enough on a CPU.

00:40:35 This I don't know.

00:40:37 I don't know.

00:40:38 Yeah.

00:40:38 So maybe we could talk really quickly about platforms that it runs on.

00:40:42 You know, I just, this is the very first show that I'm doing on my M2 Pro processor, which is fun.

00:40:49 I literally been using it for like an hour and a half, so I don't really have much to say, but it looks neat anyway, you know, that's very different than an Intel machine, which is different than a Raspberry Pi, which is different than, you know, some version of Linux running on arm.

00:41:02 Or on AMD.

00:41:03 So where, where do these, what's the reach?

00:41:07 Well, we sported, we sported, we don't.

00:41:10 So partners also has a lot of like SIMD optimizations and these for a single instruction mock data where for instance, if you do a floating point operation and that doing a single floating point at a time, you can fill in those vector lanes into your CPU with a fit eight floating points in a single operation can compute eight at a time.

00:41:30 And they have eight times the parallelism on a single point.

00:41:32 Those instructions are only activated for Intel.

00:41:36 So we don't have these instructions activated for ARM, but we do compile to ARM.

00:41:41 I think it performs on.

00:41:43 Yeah.

00:41:44 But so if the standard machines, right?

00:41:46 Mac OS, windows Linux, or we're all good to go.

00:41:50 And it ships as a wheel.

00:41:51 So you don't have to have any, you don't have to have Rusty or anything like that.

00:41:54 Chips are hanging around.

00:41:55 We also have Kunda.

00:41:57 But Kunda is always a bit lagging BI.

00:42:00 So I could try to install a bit because we can, we control this volume.

00:42:05 Yeah, exactly.

00:42:06 You push it out to, to the IPI and that's what pip sees and it's going to go.

00:42:10 to go right pretty much instantly.

00:42:12 I guess it's worth pointing out while we're sitting here is, um, not that thing I highlighted this.

00:42:16 You do have a whole section in your user guide, the Polar's book called coming from pandas that actually talks about the differences, not just how do I do this versus, you know, this operation and pandas versus Polar's, but it also talks about some of the philosophy, like this lazy concepts that we've spoken about and a query optimization.

00:42:36 I feel like we covered it pretty well.

00:42:38 Yeah.

00:42:39 Unless there's maybe some other stuff that you want to throw in here really quick, but I mostly just want to throw this out as resource, because I know many people are coming from pandas and they may be interested in this, and this is probably a good place to start.

00:42:50 I'll link to it in the show notes.

00:42:51 I think the most controversial one is that we don't have the multi-index.

00:42:55 You don't have anything other than zero based zero one, two, three.

00:42:59 You know, where is it in the array type of, yeah.

00:43:00 Well, we can, we will support beta structures that make new groups faster, like index in a database sense, but it will not It will not chase the cement.

00:43:11 Great.

00:43:11 That's important.

00:43:13 Okay.

00:43:14 Yeah.

00:43:14 So I encourage people who are mostly pandas people that come down here and, you know, look through this.

00:43:19 It's, it's pretty straightforward.

00:43:20 Another thing that I think is interesting and we're talking about maybe is we could touch a little bit on some of the, how can I, and your user guide you've got, how can I work with IO?

00:43:32 How can I work with time series?

00:43:33 How can I work with multi-processing and so on?

00:43:36 What do you think is good to highlight out of here?

00:43:38 Well, the user guard is a bit out of data.

00:43:40 So you can see your own.

00:43:42 So the princess IO is changing.

00:43:45 Oilers writes as its own IO readers.

00:43:49 So we've written our own C-SPRE reader, JSON reader, RK, IPC, Arrow.

00:43:55 And that's all in our control.

00:43:57 But for interaction with databases, it's often a bit more complicated.

00:44:01 Deal with different drivers, different ways.

00:44:04 And currently we do this with ConnectorX, which is really great.

00:44:07 and allows us to read from a lot of different databases, but it doesn't allow us to write from databases yet. And this is happening, this is luckily changing. I want to play a bit why.

00:44:17 So Kotlin is built upon the Arrow memory specification. And the Arrow memory specification is sort of standard of how memory or data, our memory for columnar data should look into, how columnar data should be, should be represented in memory. And this is becoming a new standard.

00:44:34 and Mark is using it, Dremel, Pandas itself.

00:44:38 For instance, if you read a Parquet in Pandas, it reads in first into error memory and then copies that into Pandas memory.

00:44:45 So the error memory specification coming as standard, and this is a way to share data to processes, to other libraries within a process without copying data.

00:44:57 We can just swap our pointers if we know that we both support error.

00:45:00 Oh, how so arrow defines basically a in memory.

00:45:04 It looks like this.

00:45:05 And if you both agree on that, we can just swap out quite the same.

00:45:08 Right.

00:45:09 Because a.net object, a C plus plus object and a Python object, those don't look like anything similar to any of them.

00:45:16 Right.

00:45:17 In, in memory.

00:45:17 And yeah, so, so this is from the Apache arrow project.

00:45:22 Yeah.

00:45:22 And this is really, really used a lot by a lot different tools already.

00:45:28 And currently there is coming the ADBC, which is the Apache Arrow database connector, which will solve all those problems because then we can write read arrives from a lot of databases in Arrow and then it will be really fast and very easy for us to do.

00:45:41 So luckily we, we, that's one of those foundations of boilers.

00:45:46 I'm really happy about it because supporting Arrow and using Arrow memory gives us a lot of interaction, interval with other library.

00:45:54 Yeah.

00:45:55 That's interesting.

00:45:56 And when you think of pandas, you know, it's kind of built on top of NumPy as its core foundation and it can exchange NumPy arrays with other things that do that.

00:46:05 So Apache Arrow is kind of, kind of your, your base.

00:46:09 Yeah.

00:46:10 Well, it's kind of full circle because Apache Arrow is started by Wes McKinney.

00:46:14 Wes McKinney being known as the creator of pandas.

00:46:17 And when he got out of pandas, he thought, okay, the memory representation of NumPy is just not, we should not use it.

00:46:24 And then he was inspired to build Touch Arrow, which mirrored from Palm Browser.

00:46:29 Yeah.

00:46:30 So that's how you learn about these projects, right?

00:46:32 This is how you, you realize, Oh, we, we had put this thing in place.

00:46:36 Maybe we work better.

00:46:37 Right.

00:46:38 You, you work on a project for five years and you're like, if I got a chance to start over, but it's too late now, but every now and then you do actually get a chance to start over.

00:46:47 Yeah.

00:46:48 I didn't realize that Wes was involved with both.

00:46:50 I mean, I knew from Pandas, but I didn't realize it's.

00:46:52 Yeah.

00:46:53 CTO, alternate with share.

00:46:55 You already started a Jero and that's a Jero is sort of super big, like use everywhere, but sort of middleware like it's end users are developers and not end users are developers who build tools and not developers who use library.

00:47:10 That's like that.

00:47:11 Right.

00:47:11 You might not even know that you're using it.

00:47:13 You just use, I just use pullers and oh, by the way, it happens to internally be better because of this.

00:47:20 Yeah.

00:47:20 Yeah.

00:47:20 Very cool.

00:47:21 Okay.

00:47:21 Let's see, we've got a little bit of time left to talk about it.

00:47:24 So for example, some of these, how can I, let me just touch on a couple that are nice here.

00:47:29 So you talked about ConnectorX, you talked about the database, but it's like three lines of code to define a connection string, define a SQL query, and then you just say PL.readSQL, and there you go.

00:47:42 You call it DataFrame or what do you call the thing you get back here?

00:47:45 >> So reading is always DataFrame, scanning will be a SQL.

00:47:49 Got it.

00:47:49 Okay.

00:47:49 Is there a scan SQL as well?

00:47:52 This might happen in the future.

00:47:55 The challenge is, are we going to push back our optimizations?

00:47:59 So you write a orders query and then we must translate that into SQL, into the SQL we send to their database.

00:48:07 But that needs to be consistent over different databases.

00:48:10 That's a whole other rabbit hole we might get into.

00:48:12 I'm not sure it's right because you can already do many of these operations in the SQL query that you're sending over, right?

00:48:20 You have sort of two layers of query engines and optimizers and query plans.

00:48:25 And it's not like you can't add on additional filters, joins, sorts, and so on before it ever gets back to you.

00:48:33 It would be terrible if someone writes select star from table and then writes the filters in polars and then the database has sent all those data over the network.

00:48:42 So yeah, ideally we'd be able to push those predicates down into the SQL.

00:48:47 Yeah, but you know, somebody is going to do it because they're more comfortable writing polar API in Python than they are writing T-SQL.

00:48:54 Yeah.

00:48:54 You will not.

00:48:56 Yeah.

00:48:56 If it's possible, someone will write it.

00:48:58 It's not optimal.

00:48:59 That's right.

00:49:00 That is right.

00:49:01 Let's see.

00:49:02 What else can you do here?

00:49:03 So you can, we've already talked about the CSV files and this is the part of that I was talking about where you've got the, the, the toggle to see the rust code and the Python code, so I think people might appreciate that parquet files.

00:49:15 So Parquet files is a more efficient format.

00:49:19 Maybe talk about using Parquet files versus CSV and why you might want to get rid of your CSV and like store these intermediate files and then load them.

00:49:28 Parquet is a really good file reader.

00:49:31 I really did my best on that one.

00:49:33 But you can use Parquet or Arrow, IPC because the data is tight.

00:49:39 There's no ambiguity on reading.

00:49:41 We know type it is.

00:49:43 Right.

00:49:43 Because CSV files, even though it might be representing a date, it's still a string.

00:49:47 Yeah.

00:49:50 It's slow to parse it.

00:49:51 Yeah.

00:49:52 It also, we can just, so it, okay.

00:49:55 Interact really nicely with query optimization.

00:49:58 So we can select just a single column from the file without touching any of the other columns.

00:50:02 We can read statistics.

00:50:04 And so for K file can write statistics, which knows, okay, this page is got this maximum value, this minimum value.

00:50:11 And if you have written a part of the query, which has also already give me the result where the value is larger than this, and we see that the, that, that statistics say it cannot be in this file.

00:50:22 We can just skip the whole column.

00:50:24 We don't have to read.

00:50:25 Yeah.

00:50:26 Oh, interesting.

00:50:26 Oh, okay.

00:50:27 There are a lot of optimizations, which sort of best work is work.

00:50:31 You don't have to do and proclaim.

00:50:33 Exactly.

00:50:34 Or you've done it when you created the file and you never do it again or something like that.

00:50:39 Yeah.

00:50:39 Yeah.

00:50:39 So you've got a read parquet, a scan parquet, I suppose that's the data frame versus lazy frame.

00:50:45 And then you also have the ability to write them.

00:50:47 That's pretty interesting.

00:50:48 JSON, multiple files.

00:50:50 Yeah.

00:50:51 Yeah.

00:50:51 There's just a whole bunch of how do I, how can I rather, but a bunch of neat things.

00:50:55 What else would you like to highlight here?

00:50:56 The next most important thing I want to touch on is the expression API.

00:51:00 So that's a bit, you go a bit higher.

00:51:02 So you swallow up or is it, they got their own, one of the goals of the The Poilers API is to keep the API small, but give you a lot of things you can do.

00:51:14 And this is where the Poilers expressions come in.

00:51:16 So Poilers expressions are expressions of what you want to do, which are run and parallelized on a query engine and can combine them in depth.

00:51:25 So an expression takes a series and produces a series.

00:51:28 And because the input is the same as the output, you can combine them.

00:51:31 And as you can see, we can do pretty complicated stuff.

00:51:34 And you can keep chaining them.

00:51:36 And this is the same, like, um, I'd like to see it.

00:51:40 Francis the Python vocabulary is quite small.

00:51:43 So we have a while we have a loop.

00:51:45 We have a variable assignment.

00:51:46 If you, I think it fits into maybe two, two pieces of paper, but with this, you can write any program you want with the combination of all those, all those.

00:51:56 Yeah.

00:51:56 This vocabulary.

00:51:57 Yeah.

00:51:58 And that's what we want to do with the Polish questions as well.

00:52:00 So we, you've got a lot of small building blocks, which can be combined into.

00:52:06 Yeah.

00:52:06 So somebody could say I want to select a column back, but then I don't want the actual values. I want the unique ones, a uniqueness.

00:52:15 So if there's duplicate, remove those and you can do a .account. Then you can add an alias, which gives it a new, which basically defines the column name.

00:52:23 You could read it as...

00:52:24 It's not names, it's some other one.

00:52:26 You could read it as an as. So take column names as unique names. As is the keyword in Python, so I'm allowed to use it.

00:52:33 Right.

00:52:34 I mean, something else.

00:52:35 Yeah.

00:52:36 That's that's interesting.

00:52:37 Okay.

00:52:38 Yeah.

00:52:38 So people, they use these expressions to do lots of transformations and filtering and things like that.

00:52:46 That's Yeah.

00:52:46 So this is precious.

00:52:47 Can we use in a select on different places, but the knowledge of expressions extrapolates to different locations.

00:52:54 So you can do it in a, in a select statement and then you select, well, on that, we select this expression and you get a result, but you can also do this And then the same logic applies.

00:53:02 It runs on the same engine and we make sure everything is positive.

00:53:06 And this is really powerful because it's so expensive.

00:53:11 People don't have to use custom apply with Lambda because when you use a Lambda, it's like bug to us.

00:53:16 It will be slow because it's Python and we don't know what will happen.

00:53:19 So a Lambda is, it will be slow.

00:53:22 It will kill parallelization because it kills, but yeah, a Lambda is three times better.

00:53:26 Right.

00:53:27 three times back.

00:53:28 Right.

00:53:29 It gets in the way of a lot of your optimizations and a lot of your speedups there.

00:53:34 That's why we want to make this expression API very complete so you don't need that as much.

00:53:39 Yeah, so people who are wanting to get this, get seriously into this, they should check out chapter three expressions, right?

00:53:45 And just go through there.

00:53:46 Probably, especially, you know, sort of browse through the Python examples so they can see where, go back and see what they need to learn more about, but it's a very interesting API.

00:53:56 The speed is very compelling thing.

00:53:59 Yeah.

00:53:59 I think it's a cool project.

00:54:00 Like I said, how many people we got here?

00:54:02 13,000 people using it already.

00:54:04 So that's a pretty big community.

00:54:06 Yeah.

00:54:06 So if you're interested in project, we have a discord where, where you can chat with us and ask questions and see how you can best do things.

00:54:14 Pretty active there.

00:54:15 Cool.

00:54:15 The discord's linked right off the homepage.

00:54:17 So that's awesome.

00:54:18 People can find it there.

00:54:19 Contributions.

00:54:20 People want to make contributions.

00:54:21 I'm sure you're willing to accept PRs and other feedback.

00:54:25 Or you put in a really large PR, please first open an issue with a, with a, with a, to start a discussion of this is, uh, this contribution is welcome.

00:54:34 And we also have a few getting started, good for the contributors.

00:54:39 Okay.

00:54:39 Yes.

00:54:40 You've, you've tagged or labeled some of the issues as look here.

00:54:44 If you want to get, get into this.

00:54:46 Yeah.

00:54:46 I must say, I think we're an interesting project to contribute to because we're, you can, it's not, not everything is set in stone.

00:54:53 So there are still places where you can play.

00:54:56 I'm not sure.

00:54:57 There's still interesting work to be done.

00:54:59 It's not completely 100% polished and finalized.

00:55:04 Yeah.

00:55:04 On the periphery.

00:55:05 Uh, yeah.

00:55:06 Yeah.

00:55:07 Yeah.

00:55:07 Very cool.

00:55:07 Let's wrap it up with a comment from the audience here.

00:55:10 Ajit says excellent content guys.

00:55:12 This certainly helps me kickstart my journey from pandas to pollers.

00:55:15 Awesome.

00:55:16 Awesome.

00:55:17 I'm glad glad to help.

00:55:18 I'm sure it will.

00:55:19 Many people do that.

00:55:20 So Richie, let's close it out with final call action.

00:55:23 people are interested in this project, they want to start playing and learning Polars, maybe try it out on some of their code that is, and is at the moment.

00:55:30 What do they do?

00:55:31 I'd recommend if you have a new project, just start in Polars.

00:55:34 And because you can also rewrite some formats, but the most fun experience will just start a new project in Polars.

00:55:42 And because then you can really enjoy what Polars offers.

00:55:45 Learning expression API, learn how you use it declaratively and yeah, we'll be, then it will be most fun.

00:55:52 Absolutely.

00:55:53 Sounds great.

00:55:53 And like we did point out, it has the to and from hand as data frames.

00:55:58 So you can work on a section of your code and still have it consistent, right.

00:56:02 With, with other parts that have to be, you can progressively rewrite some performance heavy parts.

00:56:08 Or I also think supporters is really strict on the, on the schema and the types.

00:56:13 It's also, if you write any ETL, you will be really happy to do that part of us because you can check the scheme of lazy frame before executing it.

00:56:21 Then you know, the apps core running the query and the data comes in and it doesn't apply to this schema and you can fail fast instead of having strange outputs.

00:56:31 Oh, that's interesting because you definitely don't want zero when you expected something else because it could have pars or other weird, whatever.

00:56:40 Right.

00:56:40 Yeah.

00:56:40 So this was my, um, for missing data and Polar's doesn't change the schema.

00:56:45 So Polar's is the schema is defined by the operations and the data and not by their values in data. So you can directly check behind.

00:56:55 Excellent. All right. Well, congratulations on a cool project. I'm glad we got to share with everybody. Thanks for coming on the show.

00:57:02 Bye.

00:57:03 You bet. Bye.

00:57:04 This has been another episode of Talk Python to Me. Thank you to our sponsors. Be sure to check out what they're offering. It really helps support the show. Typy is here to take on the challenge of rapidly transforming a bare algorithm in Python into a full-fledged decision support system for end users.

00:57:20 Get started with Typy core and GUI for free at talkpython.fm/typy, T-A-I-P-Y.

00:57:27 Earn extra income from sharing your software development opinion at user interviews.

00:57:32 Head over to talkpython.fm/userinterviews to participate today.

00:57:37 Want to level up your Python?

00:57:39 We have one of the largest catalogs of Python video courses over at TalkPython.

00:57:43 Our content ranges from true beginners to deeply advanced topics like memory and async.

00:57:48 And best of all, there's not a subscription in sight.

00:57:51 Check it out for yourself at training.talkpython.fm.

00:57:54 Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

00:57:58 We should be right at the top.

00:58:00 You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.

00:58:09 We're live streaming most of our recordings these days.

00:58:12 If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:58:20 This is your host, Michael Kennedy.

00:58:22 Thanks so much for listening.

00:58:23 I really appreciate it.

00:58:24 Now get out there and write some Python code.

00:58:26 (upbeat music)

00:58:29 [Music]

00:58:46 [BLANK_AUDIO]

