WEBVTT

00:00:00.000 --> 00:00:02.040
(keyboard clacking)

00:00:02.040 --> 00:00:04.600
- Ian, welcome to Talk Python to Me.

00:00:04.600 --> 00:00:06.360
- Hey, hey Michael, good to see you again.

00:00:06.360 --> 00:00:08.640
- Yeah, great to see you again.

00:00:08.640 --> 00:00:10.280
It has been a little while.

00:00:10.280 --> 00:00:11.920
It seems like not so long ago,

00:00:11.920 --> 00:00:16.100
and yet, when I pull up the episode that we did together,

00:00:16.100 --> 00:00:21.000
sure enough, it says March 7th, 2018.

00:00:21.000 --> 00:00:22.360
Wow.

00:00:22.360 --> 00:00:23.520
- The years are short.

00:00:23.520 --> 00:00:24.340
The years are short.

00:00:24.340 --> 00:00:25.880
They go by really fast.

00:00:25.880 --> 00:00:27.120
- They sure do.

00:00:27.120 --> 00:00:30.520
So, back then, we were talking about Python

00:00:30.520 --> 00:00:32.500
and biology and genomics,

00:00:32.500 --> 00:00:36.300
and it sounds like you're still doing genetic-type things

00:00:36.300 --> 00:00:40.160
and still doing Python and all that kind of stuff.

00:00:40.160 --> 00:00:41.720
- For sure, yeah, definitely.

00:00:41.720 --> 00:00:43.680
I work for a company called Genome Oncology.

00:00:43.680 --> 00:00:46.480
We do precision oncology software,

00:00:46.480 --> 00:00:49.080
helping folks make sense of genomics

00:00:49.080 --> 00:00:50.920
and trying to help cancer patients.

00:00:50.920 --> 00:00:51.760
- That's awesome.

00:00:51.760 --> 00:00:55.880
There's different levels of helping people with software.

00:00:56.800 --> 00:01:01.800
- On one level, we probably have ad retargeting.

00:01:01.800 --> 00:01:06.760
On the other, we've got medical benefits

00:01:06.760 --> 00:01:09.840
and looking for helping people

00:01:09.840 --> 00:01:12.240
who are suffering socially or whatever.

00:01:12.240 --> 00:01:14.600
So, it's gotta feel good to write software

00:01:14.600 --> 00:01:17.560
that is making a difference in people's lives.

00:01:17.560 --> 00:01:18.400
- That's right.

00:01:18.400 --> 00:01:21.000
I did spend a lot of the 2000s making e-commerce websites,

00:01:21.000 --> 00:01:23.120
and that wasn't exactly the most fulfilling thing.

00:01:23.120 --> 00:01:25.440
I learned a lot, but it wasn't as exciting

00:01:25.440 --> 00:01:26.280
as what I'm doing now,

00:01:26.280 --> 00:01:29.080
or at least as fulfilling as what I'm doing now.

00:01:29.080 --> 00:01:30.280
- Nice.

00:01:30.280 --> 00:01:33.080
Were those earlier websites in Python?

00:01:33.080 --> 00:01:33.920
- No.

00:01:33.920 --> 00:01:37.040
So, yeah, I mean, I was all Java for the most part,

00:01:37.040 --> 00:01:39.360
and finally, with this company,

00:01:39.360 --> 00:01:43.520
I knocked out a prototype in Django a few years ago,

00:01:43.520 --> 00:01:45.520
and my boss at the time was like,

00:01:45.520 --> 00:01:47.160
you did that so fast,

00:01:47.160 --> 00:01:48.720
you should do some more stuff in Python.

00:01:48.720 --> 00:01:51.640
So, that's kinda how it evolved.

00:01:51.640 --> 00:01:52.460
And now, basically,

00:01:52.460 --> 00:01:55.200
most of our core back end is Python,

00:01:55.200 --> 00:01:58.640
and we use a lot of Svelte for the user interfaces.

00:01:58.640 --> 00:02:00.000
- Okay, beautiful.

00:02:00.000 --> 00:02:01.320
It's easy to forget.

00:02:01.320 --> 00:02:06.840
Like, five years ago, 10 years ago,

00:02:06.840 --> 00:02:08.480
people were questioning

00:02:08.480 --> 00:02:10.280
whether Python should be something you should use.

00:02:10.280 --> 00:02:11.200
Is it a real language?

00:02:11.200 --> 00:02:12.040
Should you really use it?

00:02:12.040 --> 00:02:13.360
Is it safe to use?

00:02:13.360 --> 00:02:16.960
Maybe you should use a Java or a C# or something like that,

00:02:16.960 --> 00:02:19.720
because this is a real project.

00:02:19.720 --> 00:02:20.560
It's interesting.

00:02:20.560 --> 00:02:22.640
I don't hear that nearly as much anymore, do you?

00:02:22.640 --> 00:02:23.640
- No, no.

00:02:23.640 --> 00:02:25.200
I grew up a Boston sports fan,

00:02:25.200 --> 00:02:26.040
and it was like,

00:02:26.040 --> 00:02:28.320
being a Boston sports fan was terrible for the longest time,

00:02:28.320 --> 00:02:29.160
and now it's like,

00:02:29.160 --> 00:02:32.280
okay, we don't wanna hear about your problems right now.

00:02:32.280 --> 00:02:33.160
And same thing with Python.

00:02:33.160 --> 00:02:34.440
It's like, I like Python.

00:02:34.440 --> 00:02:35.320
It's like, yeah, great.

00:02:35.320 --> 00:02:36.680
So does everybody else in the world.

00:02:36.680 --> 00:02:39.120
So, yeah, it's really not the issue anymore.

00:02:39.120 --> 00:02:40.600
Now, it's not the cool thing to play with,

00:02:40.600 --> 00:02:43.360
so now you gotta go to Rust or something else.

00:02:43.360 --> 00:02:44.840
- You know what's shiny?

00:02:44.840 --> 00:02:46.360
LLMs are shiny.

00:02:46.360 --> 00:02:48.080
- LLMs are very shiny, for sure.

00:02:48.080 --> 00:02:50.160
- Yeah, we can talk about them today.

00:02:50.160 --> 00:02:51.280
- Yeah, that sounds great.

00:02:51.280 --> 00:02:52.120
Let's do it.

00:02:52.120 --> 00:02:53.120
- All right, before we do that, though,

00:02:53.120 --> 00:02:53.960
just, you know,

00:02:53.960 --> 00:02:56.640
it has been a good five years since you've been on the show.

00:02:56.640 --> 00:02:57.480
Tell people,

00:02:57.480 --> 00:03:00.680
I guess that you already introduced yourself.

00:03:00.680 --> 00:03:02.320
- Yeah, okay.

00:03:02.320 --> 00:03:05.360
- So, let's jump in.

00:03:05.360 --> 00:03:07.600
So, first of all,

00:03:07.600 --> 00:03:11.920
we're gonna talk about building applications

00:03:11.920 --> 00:03:16.560
that are basically powered by LLMs that you plug into, right?

00:03:16.560 --> 00:03:17.400
- Yep, yep.

00:03:17.400 --> 00:03:20.560
- Before we get into creating LLMs,

00:03:20.560 --> 00:03:21.400
just for you,

00:03:21.480 --> 00:03:26.480
where do LLMs play a role for you

00:03:26.480 --> 00:03:28.920
in software development these days?

00:03:28.920 --> 00:03:29.760
- Sure.

00:03:29.760 --> 00:03:31.320
So, you know, like everybody else,

00:03:31.320 --> 00:03:33.320
I had been playing with,

00:03:33.320 --> 00:03:35.520
so I do natural language processing as part of my job,

00:03:35.520 --> 00:03:37.760
right, so using spaCy was a big part

00:03:37.760 --> 00:03:41.040
of the information extraction stack that we use,

00:03:41.040 --> 00:03:42.800
'cause we have to deal with a lot of medical data,

00:03:42.800 --> 00:03:44.520
and medical data is just unstructured

00:03:44.520 --> 00:03:48.040
and has to be cleaned up before it can be used.

00:03:48.040 --> 00:03:49.480
So, that was my exposure.

00:03:49.480 --> 00:03:53.840
I had seen GPTs and the idea of generating text,

00:03:53.840 --> 00:03:54.840
just starting from that,

00:03:54.840 --> 00:03:56.840
didn't really make much sense to me at the time,

00:03:56.840 --> 00:03:58.160
but then obviously, like everybody else,

00:03:58.160 --> 00:03:59.440
when ChatGPT came out, I was like,

00:03:59.440 --> 00:04:00.720
"Oh, I get this now."

00:04:00.720 --> 00:04:03.080
Like, this thing does, you know,

00:04:03.080 --> 00:04:05.480
it can basically learn in the context

00:04:05.480 --> 00:04:07.320
and it can actually produce something that's interesting

00:04:07.320 --> 00:04:09.920
and you can use it for things like information extraction.

00:04:09.920 --> 00:04:11.560
So, just like everybody else,

00:04:11.560 --> 00:04:13.800
I kind of woke up to them around that time

00:04:13.800 --> 00:04:15.560
that they got released,

00:04:15.560 --> 00:04:17.040
and I use them all the time, right?

00:04:17.040 --> 00:04:19.480
So, ChatGPT 4 is really what I use.

00:04:19.480 --> 00:04:22.000
I would recommend, if you can afford the $20 a month,

00:04:22.000 --> 00:04:25.960
it's still the best model that there is as of January 2024,

00:04:25.960 --> 00:04:27.120
and I use that for coding.

00:04:27.120 --> 00:04:30.440
I don't really like the coding tools, the copilots,

00:04:30.440 --> 00:04:33.960
but there's definitely folks that swear by them.

00:04:33.960 --> 00:04:37.280
My workflow is more of, I have a problem,

00:04:37.280 --> 00:04:39.280
work with the chatbot to try to like, you know,

00:04:39.280 --> 00:04:40.680
think through all the edge cases,

00:04:40.680 --> 00:04:44.240
and then think through the test case, the tests,

00:04:44.240 --> 00:04:45.680
and then I think through the code, right?

00:04:45.680 --> 00:04:47.720
And then the actual typing of the code,

00:04:47.720 --> 00:04:49.640
yeah, I'll have it do a lot of the boilerplate stuff,

00:04:49.640 --> 00:04:52.720
but then kind of shaping the APIs and things like that.

00:04:52.720 --> 00:04:54.240
I kind of like to do that myself still.

00:04:54.240 --> 00:04:57.080
I'm kind of old school.

00:04:57.080 --> 00:04:58.280
- I guess I'm old school as well,

00:04:58.280 --> 00:05:00.160
'cause I'm like right there with you.

00:05:00.160 --> 00:05:03.840
For me, I don't generally run copilot

00:05:03.840 --> 00:05:06.560
or those kinds of things in my editors.

00:05:06.560 --> 00:05:10.440
I do have some features turned on,

00:05:10.440 --> 00:05:13.600
but primarily it's just really nice autocomplete.

00:05:13.600 --> 00:05:14.440
You know what I mean?

00:05:14.440 --> 00:05:17.000
It seems like it almost just knows

00:05:17.000 --> 00:05:19.640
what I want to type anyway, and that's getting better.

00:05:19.640 --> 00:05:22.240
I don't know if anyone's noticed recently,

00:05:22.240 --> 00:05:24.080
one of the recent releases of PyCharm,

00:05:24.080 --> 00:05:27.600
it starts to autocomplete whole lines,

00:05:27.600 --> 00:05:28.840
and I don't know where it's getting this from,

00:05:28.840 --> 00:05:31.480
and I think I have the AI features turned off.

00:05:31.480 --> 00:05:33.360
At least it says I have no license.

00:05:33.360 --> 00:05:35.080
Guess that means they're turned off.

00:05:35.080 --> 00:05:38.080
So it must be something more built into it,

00:05:38.080 --> 00:05:40.560
and that's pretty excellent.

00:05:40.560 --> 00:05:42.880
But for me, I find I'm pretty content

00:05:42.880 --> 00:05:44.720
to just sit and write code.

00:05:44.720 --> 00:05:49.720
However, the more specific the unknowns are,

00:05:49.720 --> 00:05:51.640
the more willing I'm like,

00:05:51.640 --> 00:05:53.840
oh, I need to go to ChatGPT for this.

00:05:53.840 --> 00:05:57.000
Like for example, how do you use Pydantic?

00:05:57.000 --> 00:06:00.640
Well, I'll probably just go look at a quick code sample

00:06:00.640 --> 00:06:02.520
and see that so I can understand it.

00:06:02.520 --> 00:06:07.520
But if I have this time string with the date like this,

00:06:07.520 --> 00:06:12.520
the month like this, and then it has the time zone like that,

00:06:12.520 --> 00:06:13.640
how do I parse that?

00:06:13.640 --> 00:06:16.680
Or how do I generate another one like that in Python?

00:06:16.680 --> 00:06:18.080
And here's the answer.

00:06:18.080 --> 00:06:20.360
Or I have this giant weird string,

00:06:20.360 --> 00:06:23.240
and I want this part of it as extracted

00:06:23.240 --> 00:06:24.720
with a regular expression.

00:06:24.720 --> 00:06:26.440
- Right, regular expression, I was just gonna say that.

00:06:26.440 --> 00:06:27.520
- Oh my gosh.

00:06:27.520 --> 00:06:28.840
- You don't have to write another one of those.

00:06:28.840 --> 00:06:29.800
Yeah, it's great.

00:06:29.800 --> 00:06:30.840
- Yeah, it's pretty much like,

00:06:30.840 --> 00:06:32.400
do you need to detect the end of a line

00:06:32.400 --> 00:06:33.680
straight to ChatGPT?

00:06:33.680 --> 00:06:36.120
Not really, but it's like almost any level

00:06:36.120 --> 00:06:37.800
of regular expression.

00:06:37.800 --> 00:06:39.400
I'm like, well, I need some AI for this,

00:06:39.400 --> 00:06:42.320
'cause this is not time well spent for me.

00:06:42.320 --> 00:06:43.160
But it's all right.

00:06:43.160 --> 00:06:45.920
- Yeah, one big tip I would give people though

00:06:45.920 --> 00:06:48.400
is that these chatbots, they wanna please you.

00:06:48.400 --> 00:06:51.400
So you have to ask it to criticize you.

00:06:51.400 --> 00:06:53.040
You have to say, here's some piece of code,

00:06:53.040 --> 00:06:54.760
tell me all the ways it's wrong.

00:06:54.760 --> 00:06:57.360
Right, and you have to also ask

00:06:57.360 --> 00:06:59.280
for lots of different examples,

00:06:59.280 --> 00:07:01.600
because it just starts to get more creative,

00:07:01.600 --> 00:07:02.640
more things that it says.

00:07:02.640 --> 00:07:04.400
It really thinks by talking,

00:07:04.400 --> 00:07:06.680
which is a really weird thing to consider.

00:07:06.680 --> 00:07:08.560
It's definitely some things to keep in mind

00:07:08.560 --> 00:07:09.400
when you're working with these things.

00:07:09.520 --> 00:07:12.840
- And they do have these really weird things.

00:07:12.840 --> 00:07:14.280
Like if you compliment them,

00:07:14.280 --> 00:07:16.480
or if you ask it, you sort of tell it,

00:07:16.480 --> 00:07:18.440
like, I really want you to tell me.

00:07:18.440 --> 00:07:19.960
It actually makes a difference, right?

00:07:19.960 --> 00:07:21.280
It's not just like a search engine.

00:07:21.280 --> 00:07:22.480
Like, well, of course, what does it care?

00:07:22.480 --> 00:07:24.280
Just put these keywords in and they come out.

00:07:24.280 --> 00:07:26.400
Like, no, you've kind of got to like,

00:07:26.400 --> 00:07:27.560
you've got to talk to it a little bit.

00:07:27.560 --> 00:07:29.200
- I've seen people threatening them,

00:07:29.200 --> 00:07:32.160
or like saying that someone's being held ransom,

00:07:32.160 --> 00:07:35.200
or, you know, I like to say, my boss is really mad at me.

00:07:35.200 --> 00:07:36.360
Like, help me out here, right?

00:07:36.360 --> 00:07:38.560
And like, see if it'll generate some better code.

00:07:39.560 --> 00:07:41.520
- Look, you're not being a good user.

00:07:41.520 --> 00:07:45.320
You're trying to trick me.

00:07:45.320 --> 00:07:47.720
I've been a good chatbot, and you've been a bad user,

00:07:47.720 --> 00:07:49.840
and I'm not going to help you anymore.

00:07:49.840 --> 00:07:52.520
That was actually basically a conversation

00:07:52.520 --> 00:07:54.200
from being in the early days.

00:07:54.200 --> 00:07:56.560
- Yeah, the Sydney episode, yeah, that was crazy, right?

00:07:56.560 --> 00:07:57.400
Super funny.

00:07:57.400 --> 00:07:59.240
- How funny?

00:07:59.240 --> 00:08:00.880
All right, well, I'm sure a lot of people out there

00:08:00.880 --> 00:08:02.760
are using AI these days.

00:08:02.760 --> 00:08:05.600
I think I saw a quote from, I think it was from GitHub,

00:08:05.600 --> 00:08:09.440
saying over 50% of developers are using Copilot,

00:08:09.440 --> 00:08:14.280
which, crazy, but I mean, not that surprising.

00:08:14.280 --> 00:08:16.200
50% of the people are using autocomplete,

00:08:16.200 --> 00:08:19.640
so I guess it's kind of like that, right?

00:08:19.640 --> 00:08:21.160
- They're great tools, they're going to keep evolving.

00:08:21.160 --> 00:08:22.440
There's some other ones I'm keeping an eye on.

00:08:22.440 --> 00:08:24.000
There's one called Console,

00:08:24.000 --> 00:08:25.360
which just takes a different approach.

00:08:25.360 --> 00:08:27.040
They use some stronger models.

00:08:27.040 --> 00:08:29.600
And then there's a website called Find, P-H-I-N-D,

00:08:29.600 --> 00:08:31.600
that allows you to do some searching,

00:08:31.600 --> 00:08:33.400
that they've built their own custom model.

00:08:33.400 --> 00:08:34.880
Really interesting companies that are doing

00:08:34.880 --> 00:08:37.120
some really cool things.

00:08:37.120 --> 00:08:40.120
And then Perplexity is like the search replacement

00:08:40.120 --> 00:08:42.160
that a lot of folks are very excited about using

00:08:42.160 --> 00:08:43.000
instead of Google.

00:08:43.000 --> 00:08:45.200
So there's a lot of different tools out there.

00:08:45.200 --> 00:08:47.400
You could spend all your day just kind of playing around

00:08:47.400 --> 00:08:48.320
and learning these things,

00:08:48.320 --> 00:08:50.480
but you got to actually kind of get some stuff done, too.

00:08:50.480 --> 00:08:52.160
- Yeah, you got to pick something and go, right?

00:08:52.160 --> 00:08:56.680
Because with all the churn and growth

00:08:56.680 --> 00:08:57.680
and experimentation we got,

00:08:57.680 --> 00:08:59.200
you probably could try a new tool every day

00:08:59.200 --> 00:09:01.840
and still not try them all, you know?

00:09:01.840 --> 00:09:02.680
- Exactly, right.

00:09:02.680 --> 00:09:03.520
- You got to be farther behind,

00:09:03.520 --> 00:09:05.400
so you got to pick something and go.

00:09:05.400 --> 00:09:06.240
- And go, yep.

00:09:06.240 --> 00:09:08.760
- Let's talk about writing some code.

00:09:08.760 --> 00:09:11.760
- Yeah, the next thing you're going to do

00:09:11.760 --> 00:09:16.440
after you use a chatbot is to hit an API.

00:09:16.440 --> 00:09:18.160
Like if you're going to program an app

00:09:18.160 --> 00:09:20.120
and that app is going to have LLM inside of it,

00:09:20.120 --> 00:09:22.080
large language models inside of it,

00:09:22.080 --> 00:09:24.520
APIs are pretty much the next step, right?

00:09:24.520 --> 00:09:28.160
So OpenAI has different models that are available.

00:09:28.160 --> 00:09:30.240
This is a webpage that I just saw recently

00:09:30.240 --> 00:09:32.340
that'll actually compare the different models

00:09:32.340 --> 00:09:33.180
that are out there.

00:09:33.180 --> 00:09:35.400
So there's obviously the big guy, which is OpenAI,

00:09:35.400 --> 00:09:37.680
and you can get that through Azure as well

00:09:37.680 --> 00:09:39.200
if you have a Microsoft arrangement.

00:09:39.200 --> 00:09:42.280
And there's some security reasons or HIPAA compliance

00:09:42.280 --> 00:09:45.360
and some other reasons that you might want to talk

00:09:45.360 --> 00:09:47.840
through Azure instead of going directly to OpenAI.

00:09:47.840 --> 00:09:50.560
I defer to your IT department about that.

00:09:50.560 --> 00:09:54.800
Google has Gemini, which they just released the pro version,

00:09:54.800 --> 00:09:58.000
which I believe is as strong as 3.5, roughly.

00:09:58.840 --> 00:10:02.020
That is interesting because if you don't care

00:10:02.020 --> 00:10:03.500
about them training on your data,

00:10:03.500 --> 00:10:05.980
if like whatever you're doing is just like

00:10:05.980 --> 00:10:07.500
not super proprietary or something

00:10:07.500 --> 00:10:09.420
you're trying to keep secret,

00:10:09.420 --> 00:10:12.340
they're offering free API access

00:10:12.340 --> 00:10:14.720
at I believe 60 words per minute, right?

00:10:14.720 --> 00:10:17.140
So basically one a second, you can call this thing

00:10:17.140 --> 00:10:18.340
and there's no charge.

00:10:18.340 --> 00:10:20.820
So I don't know how long that's going to last.

00:10:20.820 --> 00:10:21.940
So if you have an interesting project

00:10:21.940 --> 00:10:24.620
that you want to use in a large language model for,

00:10:24.620 --> 00:10:26.260
you might want to look at that.

00:10:26.260 --> 00:10:28.140
- Especially if it's already open data

00:10:28.140 --> 00:10:29.640
that you're playing with.

00:10:29.640 --> 00:10:30.480
- Exactly right.

00:10:30.480 --> 00:10:32.200
- Data you've somehow published to the web

00:10:32.200 --> 00:10:35.520
that has certainly been consumed by these things.

00:10:35.520 --> 00:10:37.760
- Yep, and these models are going to train on it, right?

00:10:37.760 --> 00:10:38.720
That's the trade, right?

00:10:38.720 --> 00:10:40.600
They're trying to get more tokens,

00:10:40.600 --> 00:10:41.440
is what they call it, right?

00:10:41.440 --> 00:10:43.760
The tokens are what they need to actually

00:10:43.760 --> 00:10:44.800
make these models smarter.

00:10:44.800 --> 00:10:47.560
So everyone's just hunting for more tokens.

00:10:47.560 --> 00:10:49.920
And I think this is part of their strategy for that.

00:10:49.920 --> 00:10:52.720
And then there's also a CLAWD, Bianthropic.

00:10:52.720 --> 00:10:54.240
And then after that, you get into the,

00:10:54.240 --> 00:10:56.620
kind of the open source APIs as well.

00:10:57.880 --> 00:11:00.000
- There's some really powerful open source ones out there.

00:11:00.000 --> 00:11:02.060
- Yep, and if you scroll down,

00:11:02.060 --> 00:11:02.900
yeah, scroll down here.

00:11:02.900 --> 00:11:04.880
- Yeah, this is Docspot, for people listening,

00:11:04.880 --> 00:11:05.700
docspot.ai.

00:11:05.700 --> 00:11:10.240
Is its sole purpose just to tell you

00:11:10.240 --> 00:11:11.640
price comparisons and stuff like that?

00:11:11.640 --> 00:11:12.480
Or does it have more?

00:11:12.480 --> 00:11:15.600
- Yeah, I assume this company's got some product,

00:11:15.600 --> 00:11:16.880
unfortunately I don't know what it is.

00:11:16.880 --> 00:11:19.720
But I saw this link that they put out there

00:11:19.720 --> 00:11:20.900
and it's a calculator.

00:11:20.900 --> 00:11:23.640
So you basically can put what tokens, how many tokens,

00:11:23.640 --> 00:11:25.720
there's input tokens and there's output tokens, right?

00:11:25.720 --> 00:11:28.520
So they're gonna charge more on the output tokens.

00:11:28.520 --> 00:11:31.520
That's for the most part, some of the libraries,

00:11:31.520 --> 00:11:33.720
some of the models are more equal.

00:11:33.720 --> 00:11:36.040
And then what they do is if you can figure out

00:11:36.040 --> 00:11:38.120
like roughly how big a message is gonna be,

00:11:38.120 --> 00:11:39.600
both the input and the output,

00:11:39.600 --> 00:11:40.720
how many calls you're gonna make,

00:11:40.720 --> 00:11:44.560
you can use that to then calculate basically the cost.

00:11:44.560 --> 00:11:47.560
And the cost is always at like, tokens per thousand,

00:11:47.560 --> 00:11:51.800
or dollars or pennies, really, pennies per thousand tokens.

00:11:51.800 --> 00:11:53.680
And then it's just a math equation at that point.

00:11:53.680 --> 00:11:56.160
And what you'll find is calling GPT-4

00:11:56.160 --> 00:11:57.800
is gonna be super expensive.

00:11:57.800 --> 00:12:00.480
And then calling a small seven,

00:12:00.480 --> 00:12:02.240
what's called a 7B model from Mistral

00:12:02.240 --> 00:12:03.560
is gonna be the cheapest.

00:12:03.560 --> 00:12:06.160
And you're just gonna look for these different providers.

00:12:06.160 --> 00:12:07.000
And there was really--

00:12:07.000 --> 00:12:08.320
- These really are different.

00:12:08.320 --> 00:12:12.000
Like for example, OpenAI Azure GPT-4

00:12:12.000 --> 00:12:16.480
is a little over 3 cents per call.

00:12:16.480 --> 00:12:21.480
Whereas GPT-3.5 Turbo is 1/10 of 1 cent.

00:12:22.840 --> 00:12:25.240
It's a big difference there.

00:12:25.240 --> 00:12:28.840
11 cents versus $3 to have a conversation with it.

00:12:28.840 --> 00:12:31.480
- It's crazy.

00:12:31.480 --> 00:12:33.080
Yes, it's a very wide difference.

00:12:33.080 --> 00:12:35.560
And it's all based on how much compute

00:12:35.560 --> 00:12:36.680
do these models take, right?

00:12:36.680 --> 00:12:40.240
'Cause the bigger the model, the more accurate it is.

00:12:40.240 --> 00:12:42.200
But also the more expensive it is for them to run it.

00:12:42.200 --> 00:12:44.600
So that's why there's such a cost difference.

00:12:44.600 --> 00:12:47.840
- Yeah, recently interviewed,

00:12:47.840 --> 00:12:49.320
just released a while ago,

00:12:49.320 --> 00:12:52.820
interviewed because of time shifting on podcasts.

00:12:52.820 --> 00:12:54.920
Mark Russinovich, CTO of Azure.

00:12:54.920 --> 00:12:57.760
And we talked about all the crazy stuff that they're doing

00:12:57.760 --> 00:13:00.120
for coming up with the,

00:13:00.120 --> 00:13:03.000
just running these computers that handle all of this compute

00:13:03.000 --> 00:13:05.800
and it's really a lot.

00:13:05.800 --> 00:13:08.000
- Yep, for sure, for sure.

00:13:08.000 --> 00:13:11.880
It's, there's so many,

00:13:11.880 --> 00:13:13.760
and there was a GPU shortage for a while.

00:13:13.760 --> 00:13:15.040
I don't know if that's still going on.

00:13:15.040 --> 00:13:18.120
And obviously, the big companies are buying

00:13:18.120 --> 00:13:19.720
hundreds of thousands of these GPUs

00:13:19.720 --> 00:13:22.360
to get the scale they need.

00:13:22.360 --> 00:13:23.200
- Yeah.

00:13:23.200 --> 00:13:25.620
- And so once you figure out which API you wanna use,

00:13:25.620 --> 00:13:27.640
then you wanna talk about the library.

00:13:27.640 --> 00:13:29.920
So now, most of these providers,

00:13:29.920 --> 00:13:32.320
they have a Python library that they offer.

00:13:32.320 --> 00:13:36.300
I know OpenAI does and Google Gemini does.

00:13:36.300 --> 00:13:38.180
But there's also open source ones, right?

00:13:38.180 --> 00:13:41.140
'Cause they're not very complicated to talk to.

00:13:41.140 --> 00:13:43.740
It's just basically HTTP requests.

00:13:43.740 --> 00:13:45.100
So it's just really a matter of like,

00:13:45.100 --> 00:13:47.180
what's the ergonomics you're looking for as a developer

00:13:47.180 --> 00:13:49.620
to interact with these things.

00:13:49.620 --> 00:13:50.900
And most importantly,

00:13:50.900 --> 00:13:52.540
make sure you're maintaining optionality, right?

00:13:52.540 --> 00:13:57.540
Like it's great to do a prototype with one of these models,

00:13:57.540 --> 00:13:59.540
but recognize you might wanna switch

00:13:59.540 --> 00:14:01.800
either for cost reasons or performance reasons

00:14:01.800 --> 00:14:02.920
or what have you.

00:14:02.920 --> 00:14:05.960
And, you know, LangChain, for instance,

00:14:05.960 --> 00:14:08.820
has a ton of the providers as part of,

00:14:08.820 --> 00:14:12.220
you basically are just switching a few arguments

00:14:12.220 --> 00:14:14.360
when you're switching between them.

00:14:14.360 --> 00:14:16.820
And then Simon Willison has, you know,

00:14:16.820 --> 00:14:20.660
a Python fame, has an LLM project where he's defined,

00:14:20.660 --> 00:14:22.220
you know, basically a set of,

00:14:22.220 --> 00:14:24.700
and it's really clean just the way he's organized it,

00:14:24.700 --> 00:14:26.780
because you can just add plugins as you need them, right?

00:14:26.780 --> 00:14:28.420
So you don't have to install

00:14:28.420 --> 00:14:29.780
all the different libraries that are out there.

00:14:29.780 --> 00:14:31.140
And I think LangChain is kind of following

00:14:31.140 --> 00:14:32.780
a similar approach there.

00:14:32.780 --> 00:14:35.780
I think they're coming up with a LangChain core capability

00:14:35.780 --> 00:14:38.300
where you can just kind of bring in things as you need them.

00:14:38.300 --> 00:14:43.300
And so the idea is you're now coding against these libraries

00:14:43.300 --> 00:14:47.740
and you're trying to bring together, you know,

00:14:47.740 --> 00:14:49.380
the text you need to have analyzed

00:14:49.380 --> 00:14:50.980
or whatever your use case is,

00:14:50.980 --> 00:14:53.380
and then it'll come back with the generation.

00:14:53.380 --> 00:14:55.220
And you can also not just use them on the cloud,

00:14:55.220 --> 00:14:56.820
you can use open source ones as well

00:14:56.820 --> 00:14:58.980
and run them locally on your local computer.

00:14:58.980 --> 00:15:02.340
- I'd never really thought about

00:15:02.340 --> 00:15:06.100
my architectural considerations,

00:15:06.100 --> 00:15:08.380
I guess, of these sorts of things.

00:15:08.380 --> 00:15:10.100
But of course you want to set up

00:15:10.100 --> 00:15:11.980
some kind of abstraction layer

00:15:11.980 --> 00:15:16.980
so you're not completely tied into some provider.

00:15:18.380 --> 00:15:20.220
It could be that it becomes too expensive.

00:15:20.220 --> 00:15:21.540
It could be that it becomes too slow,

00:15:21.540 --> 00:15:25.020
but it also might just be something that's better.

00:15:25.020 --> 00:15:26.940
It could be something else that comes along that's better

00:15:26.940 --> 00:15:30.180
and you're like, "Eh, we could switch, it's 25% better."

00:15:30.180 --> 00:15:34.060
But it's like a week to pull all the details

00:15:34.060 --> 00:15:36.300
of this one LLM out and put the new ones in,

00:15:36.300 --> 00:15:37.580
and so it's not worth it, right?

00:15:37.580 --> 00:15:41.900
So you like having, being tied to a particular database

00:15:41.900 --> 00:15:43.340
rather than more general--

00:15:43.340 --> 00:15:45.580
- Especially at this moment in time, right?

00:15:45.580 --> 00:15:47.340
Every couple of months, something,

00:15:47.500 --> 00:15:49.060
so something from the bottom up

00:15:49.060 --> 00:15:50.700
is getting better and better,

00:15:50.700 --> 00:15:54.220
meaning LLAMA came out a year ago

00:15:54.220 --> 00:15:56.860
and then LLAMA 2 and Mistral and Mixtral,

00:15:56.860 --> 00:15:59.260
and LLAMA 3 is gonna be coming out later

00:15:59.260 --> 00:16:00.340
this year, we believe.

00:16:00.340 --> 00:16:02.740
And so those models, which are smaller

00:16:02.740 --> 00:16:04.860
and cheaper and easier to use,

00:16:04.860 --> 00:16:07.660
or not easier to use, but they're just cheaper,

00:16:07.660 --> 00:16:12.300
is, those things are happening all the time.

00:16:12.300 --> 00:16:14.300
So being able to be flexible and nimble

00:16:14.300 --> 00:16:15.820
and kind of change where you are

00:16:15.820 --> 00:16:19.300
is gonna be crucial, at least for the next couple of years.

00:16:19.300 --> 00:16:21.340
- Yeah, the example that I gave was databases, right?

00:16:21.340 --> 00:16:25.740
And databases have been kind of a known commodity

00:16:25.740 --> 00:16:28.260
since the '80s, or what, 1980s?

00:16:28.260 --> 00:16:30.180
And of course, there's new ones that come along,

00:16:30.180 --> 00:16:31.660
but they're kind of all the same,

00:16:31.660 --> 00:16:34.620
and we got, there was MySQL,

00:16:34.620 --> 00:16:38.020
now there's Postgres that people love, right?

00:16:38.020 --> 00:16:41.420
So that is changing way, way slower than this.

00:16:41.420 --> 00:16:42.900
And people are like, "Well, we gotta think about

00:16:42.900 --> 00:16:44.900
those kinds of, don't get tied into that."

00:16:44.900 --> 00:16:46.340
Well, sure.

00:16:46.340 --> 00:16:47.180
- Right, and--

00:16:47.180 --> 00:16:48.780
- It's way less stable.

00:16:48.780 --> 00:16:53.780
- Right, and people, and create layers of abstraction

00:16:53.780 --> 00:16:55.900
there too, is right, you got SQLAlchemy,

00:16:55.900 --> 00:16:59.540
and then Sebastian from FastAPI has SQLModel,

00:16:59.540 --> 00:17:01.900
that's a layer on top of SQLAlchemy,

00:17:01.900 --> 00:17:04.100
and then there's also folks that just like

00:17:04.100 --> 00:17:07.340
writing clean ANSI SQL, and you can hopefully

00:17:07.340 --> 00:17:09.340
be able to port that from database to database as well.

00:17:09.340 --> 00:17:12.940
So, yeah, it's the same principles,

00:17:12.940 --> 00:17:16.500
separation of concerns, so you can kind of be flexible.

00:17:16.500 --> 00:17:18.540
- All right, so you talked about LangChain,

00:17:18.540 --> 00:17:21.180
just give us a sense real quick of what LangChain is.

00:17:21.180 --> 00:17:25.100
- Yeah, I mean, this was a great project

00:17:25.100 --> 00:17:26.260
from a timing perspective.

00:17:26.260 --> 00:17:28.260
I believe they invented it and released it

00:17:28.260 --> 00:17:30.100
right around the time Chats GPT came out.

00:17:30.100 --> 00:17:33.540
It's a very comprehensive library with lots of,

00:17:33.540 --> 00:17:35.380
I mean, the best part about LangChain to me

00:17:35.380 --> 00:17:37.620
is the documentation and the code samples, right?

00:17:37.620 --> 00:17:39.500
Because if you want to learn how to interact

00:17:39.500 --> 00:17:42.020
with a different large language model,

00:17:42.020 --> 00:17:43.980
or work with a vector database,

00:17:43.980 --> 00:17:45.700
and there's another library called LamaIndex

00:17:45.700 --> 00:17:47.700
that does a really good job at this as well,

00:17:47.700 --> 00:17:50.060
they have tons and tons of documentation and examples.

00:17:50.060 --> 00:17:53.860
So you can kind of look at those and try to understand it.

00:17:53.860 --> 00:17:56.140
The chaining part really came from the idea of like,

00:17:56.140 --> 00:17:58.980
okay, prompt the large language model, it gives a response,

00:17:58.980 --> 00:18:01.620
now I'm gonna take that response and prompt,

00:18:01.620 --> 00:18:04.700
again, with a new prompt using that output.

00:18:04.700 --> 00:18:07.660
The challenge with that is the reliability

00:18:07.660 --> 00:18:08.500
of these models, right?

00:18:08.500 --> 00:18:10.380
They're not gonna get close,

00:18:10.380 --> 00:18:13.860
they're not close to 100% accurate on these types of tasks.

00:18:13.860 --> 00:18:17.540
The idea of agents as well as another thing

00:18:17.540 --> 00:18:19.700
that you might build with a LangChain,

00:18:19.700 --> 00:18:22.620
and the idea there is basically the agent is,

00:18:22.620 --> 00:18:25.900
getting a task, coming up with a plan for that task,

00:18:25.900 --> 00:18:28.620
and then kind of stepping through those tasks

00:18:28.620 --> 00:18:30.380
to get the job done.

00:18:30.380 --> 00:18:31.700
Once again, we're just not there yet

00:18:31.700 --> 00:18:34.780
as far as those technologies,

00:18:34.780 --> 00:18:36.940
just because of the reliability.

00:18:36.940 --> 00:18:39.020
And then there's also a bunch of security concerns

00:18:39.020 --> 00:18:40.820
that are out there too,

00:18:40.820 --> 00:18:42.220
that you should definitely be aware of,

00:18:42.220 --> 00:18:44.180
like one term to Google

00:18:44.180 --> 00:18:46.780
and make sure you understand is prompt injection.

00:18:46.780 --> 00:18:48.980
So Simon, once again, he's got a great blog,

00:18:48.980 --> 00:18:51.180
and he's got a great blog article,

00:18:51.180 --> 00:18:54.140
or just even that tag on his blog is,

00:18:54.140 --> 00:18:56.780
tons of articles around prompt injection.

00:18:56.780 --> 00:18:59.060
And prompt injection is basically the idea

00:18:59.060 --> 00:19:02.340
of you have an app, a user says something in the app,

00:19:02.340 --> 00:19:05.620
or like types into the whatever the input is,

00:19:05.620 --> 00:19:07.420
and whatever text that they're sending through,

00:19:07.420 --> 00:19:08.620
just like with SQL injection,

00:19:08.620 --> 00:19:10.300
it kind of hijacks the conversation

00:19:10.300 --> 00:19:12.340
and causes the large language model

00:19:12.340 --> 00:19:14.100
to kind of do a different thing.

00:19:14.100 --> 00:19:17.460
- Yeah, you'll hear a little Bobby Llama, we call him.

00:19:17.460 --> 00:19:21.260
- Right, exactly right.

00:19:21.260 --> 00:19:24.420
And then the other wild one is like,

00:19:24.420 --> 00:19:26.420
people are putting stuff up on the internet,

00:19:26.420 --> 00:19:29.620
so that when the large language model browses for webpages

00:19:29.620 --> 00:19:32.180
and brings back text, it's reading the HTML

00:19:32.180 --> 00:19:33.980
or reading the text in the HTML,

00:19:33.980 --> 00:19:35.380
and it's causing the large language model

00:19:35.380 --> 00:19:37.060
to behave in some unexpected way.

00:19:37.060 --> 00:19:40.780
So there's lots of crazy challenges out there.

00:19:40.780 --> 00:19:45.580
- I'm sure there's a lot of adversarial stuff

00:19:45.580 --> 00:19:46.780
happening to these things,

00:19:46.780 --> 00:19:49.260
as they're both trying to gather data

00:19:49.260 --> 00:19:51.060
and then trying to run, right?

00:19:51.060 --> 00:19:55.380
I saw the most insane, I don't know,

00:19:55.380 --> 00:19:58.820
I guess it was an article, I saw it on RSS somewhere.

00:19:58.820 --> 00:20:02.220
It was saying that on Amazon,

00:20:02.220 --> 00:20:05.500
there's all these knockoff brands that are trying to,

00:20:05.500 --> 00:20:08.580
you know, instead of Gucci, you have a Gucci

00:20:08.580 --> 00:20:10.300
or I don't know, whatever, right?

00:20:10.300 --> 00:20:13.540
And they're getting so lazy.

00:20:13.540 --> 00:20:14.460
I don't know what the right word is,

00:20:14.460 --> 00:20:17.940
that they're using LLMs to try to write a description

00:20:17.940 --> 00:20:21.420
that is sort of in the style of Gucci, let's say.

00:20:21.420 --> 00:20:22.380
- Right.

00:20:22.380 --> 00:20:23.460
- And it'll come back and say,

00:20:23.460 --> 00:20:26.220
I'm sorry, I'm a large language model.

00:20:26.220 --> 00:20:28.860
I'm not, my rules forbid me

00:20:28.860 --> 00:20:32.660
from doing brand trademark violation.

00:20:32.660 --> 00:20:33.500
- Right, right.

00:20:33.500 --> 00:20:35.900
- That's what the Amazon listing says on Amazon.

00:20:35.900 --> 00:20:37.820
They just take it and they just straight pump it straight,

00:20:37.820 --> 00:20:39.980
whatever it says, it just goes straight into Amazon.

00:20:39.980 --> 00:20:41.380
- Yep, yep, yep, like Google, like,

00:20:41.380 --> 00:20:43.380
sorry, I'm not, sorry, as a large language model

00:20:43.380 --> 00:20:44.260
or sorry as a whatever.

00:20:44.260 --> 00:20:45.100
Yeah, you can find all.

00:20:45.100 --> 00:20:46.740
- Exactly, there's like the product listings

00:20:46.740 --> 00:20:48.100
are full of that.

00:20:48.100 --> 00:20:49.300
It's amazing.

00:20:49.300 --> 00:20:50.340
It's amazing. - It's crazy.

00:20:50.340 --> 00:20:51.460
It's crazy.

00:20:51.460 --> 00:20:53.620
- So certainly the reliability of that is,

00:20:53.620 --> 00:20:56.220
they could probably use some testing

00:20:56.220 --> 00:20:58.180
and those kinds of things.

00:20:58.180 --> 00:21:00.340
Owen out there asks, like,

00:21:00.340 --> 00:21:02.740
I wonder if for local LLM models,

00:21:02.740 --> 00:21:04.500
there's a similar site as DocSpots

00:21:04.500 --> 00:21:06.740
that show you like what you need to run it locally.

00:21:06.740 --> 00:21:08.780
So that's an interesting question also,

00:21:08.780 --> 00:21:11.980
segue to maybe talk about like some local stuff.

00:21:11.980 --> 00:21:15.620
- Yeah, so yeah, LLM Studio, this is a new product.

00:21:15.620 --> 00:21:17.780
I honestly haven't had a chance to like really dig in

00:21:17.780 --> 00:21:19.180
and understand who created this

00:21:19.180 --> 00:21:24.020
and make sure that the privacy stuff is up to snuff.

00:21:24.020 --> 00:21:26.060
But I've played around with it locally.

00:21:26.060 --> 00:21:27.500
It seems to work great.

00:21:27.500 --> 00:21:29.180
It's really slick, really nice user interface.

00:21:29.180 --> 00:21:31.340
So if you're just wanting to get your feet wet

00:21:31.340 --> 00:21:33.180
and try to understand some of these models,

00:21:33.180 --> 00:21:34.900
I'd download that and check it out.

00:21:34.900 --> 00:21:37.620
There's a ton of models up on Hugging Face.

00:21:37.620 --> 00:21:39.660
This product seems to just basically link right

00:21:39.660 --> 00:21:44.660
into the Hugging Face interface and grabs models.

00:21:44.660 --> 00:21:47.740
And so some of the models you wanna look for

00:21:47.740 --> 00:21:49.700
are right now as in January, right?

00:21:49.700 --> 00:21:54.700
There's Mistral 7B, MISTRAL.

00:21:54.700 --> 00:21:57.500
There's another one called PHY2.

00:21:57.500 --> 00:21:59.180
Those are two of the smaller models

00:21:59.180 --> 00:22:03.660
that should run pretty well on like a commercial grade GPU

00:22:03.660 --> 00:22:06.540
or an M1 or an M2 Mac, if that's what you have,

00:22:06.540 --> 00:22:09.500
and start playing with them.

00:22:09.500 --> 00:22:11.460
And they're quantized,

00:22:11.460 --> 00:22:12.700
which means they're just kind of made

00:22:12.700 --> 00:22:14.940
to take a little bit less space,

00:22:14.940 --> 00:22:17.220
which is good from like a virtual RAM

00:22:17.220 --> 00:22:19.220
with regards to these GPUs.

00:22:19.220 --> 00:22:23.700
And there's a account on Hugging Face called The Bloke.

00:22:23.700 --> 00:22:26.500
If you look for him, you'll see all his different fine,

00:22:26.500 --> 00:22:29.100
there's different fine tunes and things like that.

00:22:29.100 --> 00:22:30.900
And there's a group called Nous,

00:22:30.900 --> 00:22:33.220
I think is how you pronounce it, N-O-U-S.

00:22:33.220 --> 00:22:35.300
And they've got some of the fine tunes

00:22:35.300 --> 00:22:37.980
that are basically the highest performing ones

00:22:37.980 --> 00:22:38.980
that are out there.

00:22:38.980 --> 00:22:40.740
So if you're really looking for a high performing

00:22:40.740 --> 00:22:43.620
local model that can actually help you with code

00:22:43.620 --> 00:22:46.900
or reasoning, those are definitely the way to get started.

00:22:46.900 --> 00:22:49.620
- Excellent.

00:22:49.620 --> 00:22:51.300
Yeah, this one seems pretty nice.

00:22:51.300 --> 00:22:53.780
I also haven't played with it, I just learned about it,

00:22:53.780 --> 00:22:55.500
but it's looking really good.

00:22:55.500 --> 00:22:59.020
I had played with, what was it, GPT4ALL?

00:22:59.020 --> 00:23:00.820
I think it was. - Yep, yep, yep.

00:23:00.820 --> 00:23:02.780
- Was the one that I played with.

00:23:02.780 --> 00:23:05.260
Sometimes it looks a little bit nicer than that,

00:23:05.260 --> 00:23:07.820
for some reason, I don't know how different it really is.

00:23:07.820 --> 00:23:11.500
- No, I mean, it's all the idea of downloading these files

00:23:11.500 --> 00:23:13.060
and running them locally.

00:23:13.060 --> 00:23:14.300
And these are just user interfaces

00:23:14.300 --> 00:23:15.740
that make it a little easier.

00:23:15.740 --> 00:23:20.580
The original project that made this stuff possible

00:23:20.580 --> 00:23:22.900
was a project called Llama CPP.

00:23:22.900 --> 00:23:26.860
There's a Python library that can work with that directly.

00:23:26.860 --> 00:23:28.980
There's another project called Llama File,

00:23:28.980 --> 00:23:31.020
where if you download the whole thing,

00:23:31.020 --> 00:23:33.100
it actually runs no matter where you are.

00:23:33.100 --> 00:23:36.340
I think it runs on Mac and Linux and Windows and BSD

00:23:36.340 --> 00:23:37.260
or whatever it is.

00:23:37.260 --> 00:23:40.220
I mean, it's an amazing technology

00:23:40.220 --> 00:23:44.660
that this one put together, it's really impressive.

00:23:44.660 --> 00:23:48.500
And then you can actually just use Google Colab too.

00:23:48.500 --> 00:23:51.460
So Google Colab has some GPUs with it.

00:23:51.460 --> 00:23:53.620
I think if you upgrade it to the $10 a month version,

00:23:53.620 --> 00:23:57.060
I think you get some better GPUs access.

00:23:57.060 --> 00:23:59.500
So if you actually wanna get a hand of running,

00:23:59.500 --> 00:24:01.180
instead of, and so this is a little bit different, right?

00:24:01.180 --> 00:24:03.140
So instead of calling an API,

00:24:03.140 --> 00:24:04.580
when you're using Google Colab,

00:24:04.580 --> 00:24:06.940
you can actually use a library called Hugging Face,

00:24:06.940 --> 00:24:08.500
and then you can actually load these things

00:24:08.500 --> 00:24:10.220
directly into your memory

00:24:10.220 --> 00:24:13.340
and then into your actual Python environment,

00:24:13.340 --> 00:24:14.940
and then you're working with it directly.

00:24:14.940 --> 00:24:17.100
So it just takes a little bit of work

00:24:17.100 --> 00:24:19.340
to make sure you're running it on the GPU,

00:24:19.340 --> 00:24:20.620
'cause if you're running it on the CPU,

00:24:20.620 --> 00:24:21.980
it's gonna be a lot slower.

00:24:21.980 --> 00:24:24.660
- Yeah, it definitely makes a big difference.

00:24:24.660 --> 00:24:27.860
There's a tool that I use that for a long time,

00:24:27.860 --> 00:24:30.660
right on the CPU, and they rewrote it to run on the GPU.

00:24:30.660 --> 00:24:32.620
Even on my M2 Pro,

00:24:32.620 --> 00:24:34.980
it was like three times faster or something.

00:24:34.980 --> 00:24:36.660
- Yep, for sure, for sure.

00:24:36.660 --> 00:24:39.020
- Yeah, it makes a big difference.

00:24:39.020 --> 00:24:40.860
So with this, let's see here,

00:24:40.860 --> 00:24:42.540
says some things that you can do.

00:24:42.540 --> 00:24:45.580
Which one?

00:24:45.580 --> 00:24:46.860
I've read a bunch of these all at the same time,

00:24:46.860 --> 00:24:47.940
so I've gotta remember.

00:24:47.940 --> 00:24:49.980
So with the LM Studio,

00:24:50.060 --> 00:24:52.540
plus you run the LMs offline

00:24:52.540 --> 00:24:55.900
and use models through an open AI,

00:24:55.900 --> 00:24:56.740
that's what I was looking for,

00:24:56.740 --> 00:25:00.020
the open AI compatible local server.

00:25:00.020 --> 00:25:01.380
- Right, yeah, so--

00:25:01.380 --> 00:25:03.740
- You could basically get an API for any of these

00:25:03.740 --> 00:25:06.140
and then start programming against it, right?

00:25:06.140 --> 00:25:09.740
- Exactly right, and it's basically the same interface,

00:25:09.740 --> 00:25:13.620
so same APIs for posting in response

00:25:13.620 --> 00:25:16.660
of the JSON schema that's going back and forth.

00:25:16.660 --> 00:25:18.660
So you're programming against that interface,

00:25:18.660 --> 00:25:21.460
and then you basically port it and move it to another,

00:25:21.460 --> 00:25:24.180
to the open AI models if you wanted to as well.

00:25:24.180 --> 00:25:26.340
So everyone's kind of coalescing around open AI

00:25:26.340 --> 00:25:28.660
as kind of like the quote unquote standard,

00:25:28.660 --> 00:25:31.060
but there's nothing, there's really no,

00:25:31.060 --> 00:25:33.500
there's no mode around that standard as well, right?

00:25:33.500 --> 00:25:36.820
'Cause anybody can kind of adopt it and use it.

00:25:36.820 --> 00:25:39.700
- There's not like a W3C committee choosing.

00:25:39.700 --> 00:25:42.620
- Correct, correct.

00:25:42.620 --> 00:25:43.460
And yeah, so--

00:25:43.460 --> 00:25:46.180
- The market will choose for us, let's go.

00:25:46.180 --> 00:25:48.020
- Yep, yeah, and it seems to be,

00:25:48.020 --> 00:25:49.020
it seems to be working out well,

00:25:49.020 --> 00:25:53.020
and that's another benefit of Simon's LLM project, right?

00:25:53.020 --> 00:25:55.060
He's got the ability to kind of switch back and forth

00:25:55.060 --> 00:26:00.060
between these different libraries and APIs as well.

00:26:00.060 --> 00:26:03.500
- Yeah, this LM Studio says,

00:26:03.500 --> 00:26:06.500
"This app does not collect data nor monitor your actions.

00:26:06.500 --> 00:26:08.580
Your data stays local on your machine,

00:26:08.580 --> 00:26:09.420
free for personal use."

00:26:09.420 --> 00:26:10.580
All that sounds great.

00:26:10.580 --> 00:26:12.340
"For business use, please get in touch."

00:26:12.340 --> 00:26:13.860
I always just like these, like,

00:26:13.860 --> 00:26:16.540
if you gotta ask, it's too much to type.

00:26:16.540 --> 00:26:17.860
- Probably, yeah.

00:26:17.860 --> 00:26:20.260
- So I'm not using it for personal use,

00:26:20.260 --> 00:26:23.300
just so if anybody's watching, yeah, just plain old.

00:26:23.300 --> 00:26:27.540
- Yeah, and it's, you know,

00:26:27.540 --> 00:26:28.980
either they just haven't thought it through

00:26:28.980 --> 00:26:30.820
and they just don't wanna talk about it yet.

00:26:30.820 --> 00:26:32.420
- Sure, I think that's part of it.

00:26:32.420 --> 00:26:33.820
- I just probably imagine it's probably,

00:26:33.820 --> 00:26:35.300
it's like, "Ah, we haven't figured out a business model.

00:26:35.300 --> 00:26:37.780
Just, I don't know, shoot us a note."

00:26:37.780 --> 00:26:39.660
- Yep, they're concentrating on the product,

00:26:39.660 --> 00:26:40.860
which makes sense.

00:26:40.860 --> 00:26:43.300
- Yeah, so then the other one is Llamafile,

00:26:43.300 --> 00:26:45.820
llamafile.ai that you mentioned.

00:26:45.820 --> 00:26:46.860
This packages it up.

00:26:46.860 --> 00:26:50.740
I guess, going back to the LM Studio real quick,

00:26:50.740 --> 00:26:52.900
one of the things that's cool about this

00:26:52.900 --> 00:26:57.020
is if it's the OpenAI API, right,

00:26:57.020 --> 00:26:58.940
this little local server that you can play with,

00:26:58.940 --> 00:27:03.060
but then you can pick LLMs such as Llama, Falcon,

00:27:03.060 --> 00:27:07.780
Replit, all the different ones, right,

00:27:07.780 --> 00:27:09.340
Starcoder and so on.

00:27:09.340 --> 00:27:12.420
It would let you write an app

00:27:12.420 --> 00:27:13.980
as if it was going to OpenAI

00:27:13.980 --> 00:27:16.380
and then just start swapping in models.

00:27:16.900 --> 00:27:18.340
And go like, "Oh, if we switch to this model,

00:27:18.340 --> 00:27:19.180
how'd that work?"

00:27:19.180 --> 00:27:20.500
But you don't even have to change any code, right?

00:27:20.500 --> 00:27:22.660
Just probably maybe a string that says

00:27:22.660 --> 00:27:25.060
which model to initialize.

00:27:25.060 --> 00:27:28.020
- Correct, well, you're getting to one of the tricks though,

00:27:28.020 --> 00:27:30.060
is then the prompts themselves.

00:27:30.060 --> 00:27:32.020
Right, so the models themselves,

00:27:32.020 --> 00:27:35.060
yeah, the models themselves act differently.

00:27:35.060 --> 00:27:37.580
And part of this whole world

00:27:37.580 --> 00:27:40.060
is what they call prompt engineering, right?

00:27:40.060 --> 00:27:42.460
So prompt engineering is really just exploring

00:27:42.460 --> 00:27:44.900
how to interact with these models,

00:27:44.900 --> 00:27:47.620
how to make sure that they're kind of

00:27:47.620 --> 00:27:50.660
in the right mind space to tackle your problem.

00:27:50.660 --> 00:27:52.700
A lot of the times that people get,

00:27:52.700 --> 00:27:53.820
when they struggle with these things,

00:27:53.820 --> 00:27:55.900
it's really just, they've really got to think

00:27:55.900 --> 00:27:58.820
more like a psychiatrist when they're working with a model.

00:27:58.820 --> 00:28:01.940
Basically getting them kind of prepared.

00:28:01.940 --> 00:28:03.860
One of the tricks people did, figured out early,

00:28:03.860 --> 00:28:07.620
was you're a genius at software development,

00:28:07.620 --> 00:28:09.380
like compliment the thing, make it feel like,

00:28:09.380 --> 00:28:14.380
"Oh, I'm gonna behave like I'm a world rockstar programmer

00:28:14.860 --> 00:28:16.060
"and now I'm perfect." - Well, it's gonna give you

00:28:16.060 --> 00:28:17.780
average, but if you tell them, "I'm genius,"

00:28:17.780 --> 00:28:19.300
then let's start, we'll do that.

00:28:19.300 --> 00:28:21.460
- Right, right, 'cause I mean,

00:28:21.460 --> 00:28:23.900
and there was also a theory like that in December

00:28:23.900 --> 00:28:26.300
that the large language models were getting dumber

00:28:26.300 --> 00:28:27.340
because it was the holidays

00:28:27.340 --> 00:28:29.660
and people don't work as hard, right?

00:28:29.660 --> 00:28:32.180
It's really hard to know which of these

00:28:32.180 --> 00:28:33.820
kind of things are true or not,

00:28:33.820 --> 00:28:36.060
but it's definitely true that each model

00:28:36.060 --> 00:28:36.900
is a little bit different.

00:28:36.900 --> 00:28:39.580
And if you write a prompt that works really well

00:28:39.580 --> 00:28:42.540
on one model, even if it's a stronger model

00:28:42.540 --> 00:28:44.700
or a weaker model, and then you port it to another model

00:28:44.700 --> 00:28:47.620
and it's, you know, and that's,

00:28:47.620 --> 00:28:50.540
then the stronger model works worse, right?

00:28:50.540 --> 00:28:52.460
It can be very counterintuitive at times

00:28:52.460 --> 00:28:55.420
and you just gotta, you've gotta test things out.

00:28:55.420 --> 00:28:57.700
And that really gets to the idea of evals, right?

00:28:57.700 --> 00:29:01.380
So evaluation is really a key problem, right?

00:29:01.380 --> 00:29:03.540
Making sure that if you're gonna be writing prompts

00:29:03.540 --> 00:29:05.300
and you're gonna be building, you know,

00:29:05.300 --> 00:29:09.220
different retrieval augmented generation solutions,

00:29:09.220 --> 00:29:11.820
you need to know about prompt injection

00:29:11.820 --> 00:29:13.660
and you need to know about prompt engineering

00:29:13.660 --> 00:29:17.020
and you need to know what these things can and can't do.

00:29:17.020 --> 00:29:19.420
You know, there's, you know, one trick

00:29:19.420 --> 00:29:21.540
is what they call few shot prompting,

00:29:21.540 --> 00:29:24.140
which is, you know, if you want it to do data extraction,

00:29:24.140 --> 00:29:26.260
you can say, oh, okay, I want you to extract data

00:29:26.260 --> 00:29:29.580
from texts that I give you in JSON.

00:29:29.580 --> 00:29:31.060
If you give it a few examples,

00:29:31.060 --> 00:29:32.980
like wildly different examples,

00:29:32.980 --> 00:29:35.500
because the giving it a bunch of similar stuff,

00:29:35.500 --> 00:29:37.540
it might kind of cause it to just coalesce

00:29:37.540 --> 00:29:38.940
around those similar examples.

00:29:38.940 --> 00:29:42.000
But if you can give it a wildly different set of examples,

00:29:42.000 --> 00:29:45.060
that's called in context learning or few shot prompting,

00:29:45.060 --> 00:29:48.060
and it will do a better job at that specific task for you.

00:29:48.060 --> 00:29:51.540
- Okay, that's super neat.

00:29:51.540 --> 00:29:54.060
When you're creating your apps,

00:29:54.060 --> 00:29:58.460
do you do things like, here's the input from the program

00:29:58.460 --> 00:30:00.580
or from the user or wherever it came from,

00:30:00.580 --> 00:30:03.660
but maybe before that you give it like three or four prompts

00:30:03.660 --> 00:30:05.780
and then let it have the question, right?

00:30:05.780 --> 00:30:07.380
Instead of just taking the text,

00:30:07.380 --> 00:30:11.340
like I'm gonna ask you questions about biology and genetics,

00:30:11.340 --> 00:30:13.060
and it's gonna be under this context.

00:30:13.060 --> 00:30:14.940
And I want you to favor these data sources.

00:30:14.940 --> 00:30:17.300
Now ask your question, something like this.

00:30:17.300 --> 00:30:19.420
- For sure, all those types of strategies

00:30:19.420 --> 00:30:20.740
are worth experimenting with, right?

00:30:20.740 --> 00:30:23.300
Like what actually will work for your scenario?

00:30:23.300 --> 00:30:24.140
I can't tell you, right?

00:30:24.140 --> 00:30:26.380
You gotta dig in, you gotta figure it out

00:30:26.380 --> 00:30:28.740
and you gotta try different things.

00:30:28.740 --> 00:30:29.580
And you can--

00:30:29.580 --> 00:30:30.900
- You're about to win the Nobel Prize

00:30:30.900 --> 00:30:32.420
in genetics for your work.

00:30:32.420 --> 00:30:34.900
Now I'm gonna ask you some questions.

00:30:34.900 --> 00:30:36.720
- For sure, for sure, that'll definitely work.

00:30:36.720 --> 00:30:39.460
And then threatening it that your boss is mad at you

00:30:39.460 --> 00:30:40.660
is also gonna help you too, right?

00:30:40.660 --> 00:30:41.500
For sure.

00:30:41.500 --> 00:30:44.180
- If I don't solve this problem, I'm gonna get fired.

00:30:44.180 --> 00:30:46.220
As a large language model, I can't tell you,

00:30:46.220 --> 00:30:47.380
but I'm gonna be fired.

00:30:47.380 --> 00:30:48.980
All right, well, then the answer is.

00:30:48.980 --> 00:30:50.480
- Right, right, exactly right.

00:30:50.480 --> 00:30:54.420
- Okay, so for these, they run, like you said,

00:30:54.420 --> 00:30:56.340
they run pretty much locally,

00:30:56.340 --> 00:31:00.360
these different models on LM Studio and others,

00:31:00.360 --> 00:31:02.100
like the Llama file and so on.

00:31:02.100 --> 00:31:03.500
- Yep, yep, and Lama.

00:31:03.500 --> 00:31:05.140
- I don't need a cluster.

00:31:05.140 --> 00:31:06.020
- Correct, correct.

00:31:06.020 --> 00:31:08.540
Yeah, that's, I mean, Llama CPP is really the project

00:31:08.540 --> 00:31:09.700
that should get all the credit

00:31:09.700 --> 00:31:12.580
for making this work on your laptops.

00:31:12.580 --> 00:31:17.300
And then Llama file and Llama CPP all have servers.

00:31:17.300 --> 00:31:21.260
So I'm guessing LM Studio is just exposing that server.

00:31:21.260 --> 00:31:25.060
And that's in the base Llama CPP project.

00:31:25.060 --> 00:31:27.540
And that's really what it is.

00:31:27.540 --> 00:31:32.100
It's really just about now you can post your requests.

00:31:32.100 --> 00:31:33.800
It's handling all of the work

00:31:33.800 --> 00:31:36.360
with regards to the token generation

00:31:36.360 --> 00:31:37.920
on the backend using Llama CPP,

00:31:37.920 --> 00:31:39.000
and then it's returning it to you

00:31:39.000 --> 00:31:41.720
using the HTTP kind of processes.

00:31:41.720 --> 00:31:46.560
- Is Llama originally from Meta?

00:31:46.560 --> 00:31:48.000
Is that where that came from?

00:31:48.000 --> 00:31:49.520
- Yeah, so yeah, I don't,

00:31:49.520 --> 00:31:53.320
I think there were people that were kind of using that LLM.

00:31:53.320 --> 00:31:55.040
I think people were kind of keying off

00:31:55.040 --> 00:31:57.280
the Llama thing at one point.

00:31:57.280 --> 00:32:00.680
I think Llama Index, for instance,

00:32:00.680 --> 00:32:02.920
I think that project was originally called GPT Index,

00:32:02.920 --> 00:32:04.560
and they decided, oh, I don't wanna be like,

00:32:04.560 --> 00:32:06.520
I don't wanna confuse myself with OpenAI

00:32:06.520 --> 00:32:07.840
or confuse my project with OpenAI.

00:32:07.840 --> 00:32:09.520
So they switched to Llama Index.

00:32:09.520 --> 00:32:11.440
And then of course, Meta released Llama.

00:32:11.440 --> 00:32:14.260
So, you know, you can't, you kinda,

00:32:14.260 --> 00:32:16.280
and then everything from there has kind of evolved too.

00:32:16.280 --> 00:32:17.560
Right, there's been alpacas

00:32:17.560 --> 00:32:18.680
and a bunch of other stuff as well.

00:32:18.680 --> 00:32:20.800
So, but the original Llama--

00:32:20.800 --> 00:32:21.920
- If you don't know your animals, yeah.

00:32:21.920 --> 00:32:22.820
If you don't know your animals,

00:32:22.820 --> 00:32:25.960
you can't figure out the heritage of these projects.

00:32:25.960 --> 00:32:26.800
- Correct.

00:32:26.800 --> 00:32:31.120
But Llama from Meta was the first open source,

00:32:31.120 --> 00:32:33.360
I'd say large language model of note,

00:32:33.360 --> 00:32:34.880
I guess, since ChatGPT.

00:32:34.880 --> 00:32:37.480
There were certainly other, you know,

00:32:37.480 --> 00:32:39.480
I'm not a, so one thing to caveat,

00:32:39.480 --> 00:32:40.760
I am not a researcher, right?

00:32:40.760 --> 00:32:42.640
So there's lots of folks in the ML research community

00:32:42.640 --> 00:32:44.080
that know way more than I do.

00:32:44.080 --> 00:32:48.160
But, 'cause there was like Bloom and T5

00:32:48.160 --> 00:32:49.400
and a few other large, you know,

00:32:49.400 --> 00:32:50.640
quote unquote large language models.

00:32:50.640 --> 00:32:54.460
But Llama, after ChatGPT, Llama was the big release

00:32:54.460 --> 00:32:57.040
that came from Meta in, I think, March.

00:32:57.040 --> 00:32:58.760
And then, and that was from Meta.

00:32:58.760 --> 00:33:01.600
And then they had it released under just like research

00:33:01.600 --> 00:33:05.920
use terms, and then only certain people get access to it.

00:33:05.920 --> 00:33:07.880
And then someone put a, I guess,

00:33:07.880 --> 00:33:11.080
put like a BitTorrent link or something on GitHub.

00:33:11.080 --> 00:33:13.080
And then basically the world had it.

00:33:13.080 --> 00:33:14.960
And then they did end up releasing Llama 2

00:33:14.960 --> 00:33:17.560
a few months later with more friendly terms.

00:33:17.560 --> 00:33:21.520
So that, and it was a much stronger model as well.

00:33:21.520 --> 00:33:22.360
- Nice.

00:33:22.360 --> 00:33:23.180
It's kind of the realization, like,

00:33:23.180 --> 00:33:24.320
well, if it's gonna be out there anyway,

00:33:24.320 --> 00:33:26.600
let's at least get credit for it then.

00:33:26.600 --> 00:33:27.560
- Yep, for sure.

00:33:27.560 --> 00:33:29.000
- Yeah.

00:33:29.000 --> 00:33:29.840
- For sure.

00:33:29.840 --> 00:33:32.200
And I did read something where like basically Facebook

00:33:32.200 --> 00:33:34.880
approached OpenAI for access to their models

00:33:34.880 --> 00:33:35.700
to help them write code.

00:33:35.700 --> 00:33:37.200
But the cost was so high that they decided

00:33:37.200 --> 00:33:38.560
to just go build their own, right?

00:33:38.560 --> 00:33:43.080
So it's kind of interesting how this stuff has evolved.

00:33:43.080 --> 00:33:46.080
- Like, you know, we got a big cluster of computers.

00:33:46.080 --> 00:33:47.160
- Right.

00:33:47.160 --> 00:33:48.700
Metaverse thing doesn't seem to be working yet.

00:33:48.700 --> 00:33:51.540
So let's go ahead and train a bunch of large language models.

00:33:51.540 --> 00:33:56.280
- Pass it over in the Metaverse data center.

00:33:56.280 --> 00:33:58.920
So one of the things that people will maybe talk about

00:33:58.920 --> 00:34:03.040
in this space is RAG or retrieval augmented generation.

00:34:03.040 --> 00:34:03.880
What's this?

00:34:03.880 --> 00:34:07.140
- Yeah, so I think one thing to recognize

00:34:07.140 --> 00:34:10.280
is that the large language models,

00:34:10.280 --> 00:34:13.720
if it's not in the training set and it's not in the prompt,

00:34:13.720 --> 00:34:15.320
it really doesn't know about it.

00:34:15.320 --> 00:34:18.640
And the question of like, what's reasoning

00:34:18.640 --> 00:34:20.920
and what's, you know, generalizing and things like that,

00:34:20.920 --> 00:34:22.660
those are big debates that people are having.

00:34:22.660 --> 00:34:24.440
What's intelligence, what have you.

00:34:24.440 --> 00:34:26.800
But recognizing the fact that you have this prompt

00:34:26.800 --> 00:34:28.240
and things you put in the prompt,

00:34:28.240 --> 00:34:29.760
the large language model can understand

00:34:29.760 --> 00:34:31.600
and extrapolate from is really powerful.

00:34:31.600 --> 00:34:34.260
So, and that's called in context learning.

00:34:34.260 --> 00:34:36.840
So retrieval augmented generation is the idea of,

00:34:36.840 --> 00:34:40.160
okay, I'm gonna go, I'm going to maybe ask,

00:34:40.160 --> 00:34:41.740
allow a person to ask a question.

00:34:41.740 --> 00:34:45.840
This is kind of like the common use case that I see.

00:34:45.840 --> 00:34:48.560
User asks a question, we're gonna take that question,

00:34:48.560 --> 00:34:52.640
find the relevant content, put that content in the prompt

00:34:52.640 --> 00:34:54.080
and then do something with it, right?

00:34:54.080 --> 00:34:55.680
So it might be something like summer, you know,

00:34:55.680 --> 00:34:57.840
ask a question about, you know, what, you know,

00:34:57.840 --> 00:34:59.800
how tall is the leaning tower of Pisa, right?

00:34:59.800 --> 00:35:03.280
And so now it's gonna go off and find that piece of content

00:35:03.280 --> 00:35:04.720
from Wikipedia or what have you,

00:35:04.720 --> 00:35:06.360
and then put that information in the prompt

00:35:06.360 --> 00:35:10.680
and then now the model can then respond

00:35:10.680 --> 00:35:12.220
to that question based on that text.

00:35:12.220 --> 00:35:13.500
Obviously that's a pretty simple example,

00:35:13.500 --> 00:35:15.400
but you can get more complicated and it's going out

00:35:15.400 --> 00:35:19.000
and bringing back lots of different content, slicing it up,

00:35:19.000 --> 00:35:20.700
putting in the prompt and asking a question.

00:35:20.700 --> 00:35:22.760
So now the trick is, okay,

00:35:22.760 --> 00:35:25.840
how do you actually get that content and how do you do that?

00:35:25.840 --> 00:35:29.240
Well, you know, information retrieval, search engines

00:35:29.240 --> 00:35:31.840
and things like that, that's obviously the technique,

00:35:31.840 --> 00:35:34.120
but one of the key techniques that people have been,

00:35:34.120 --> 00:35:37.120
you know, kind of discovering, rediscovering, I guess,

00:35:37.120 --> 00:35:40.080
is this idea of word embeddings or vectors.

00:35:40.080 --> 00:35:42.320
And so Word2Vec was this project that came out,

00:35:42.320 --> 00:35:44.160
I think 11 years ago or so.

00:35:44.160 --> 00:35:45.540
And, you know, there was a big,

00:35:45.540 --> 00:35:47.580
the big meme around that was,

00:35:47.580 --> 00:35:49.900
you could take the embedding for the word king,

00:35:49.900 --> 00:35:52.720
you could then subtract the embedding for the word man

00:35:52.720 --> 00:35:54.640
add the word embedding for woman,

00:35:54.640 --> 00:35:57.560
and then the end math result would actually be close

00:35:57.560 --> 00:35:59.680
to the embedding for the word queen.

00:35:59.680 --> 00:36:00.960
And so what is an embedding?

00:36:00.960 --> 00:36:01.800
What's a vector?

00:36:01.800 --> 00:36:04.160
It's basically this large floating point number

00:36:04.160 --> 00:36:08.760
that has semantic meaning inferred into it.

00:36:08.760 --> 00:36:10.960
And it's built just by training a model.

00:36:10.960 --> 00:36:12.720
So just like you train a large language model,

00:36:12.720 --> 00:36:15.500
they can trade these embedding models to basically

00:36:15.500 --> 00:36:18.740
take a word and then take a sentence and then take a,

00:36:18.740 --> 00:36:20.480
you know, a document is what, you know,

00:36:20.480 --> 00:36:24.320
what an AI can do and turn that into this big giant,

00:36:24.320 --> 00:36:29.120
you know, 200, 800, 1500, you know,

00:36:29.120 --> 00:36:31.320
depending on the size of the embedding,

00:36:31.320 --> 00:36:33.840
the floating point numbers,

00:36:33.840 --> 00:36:35.840
and then use that as a, what's called, you know,

00:36:35.840 --> 00:36:37.600
semantic similarity search.

00:36:37.600 --> 00:36:38.760
So you're basically going off

00:36:38.760 --> 00:36:40.640
and asking for similar documents.

00:36:40.640 --> 00:36:42.160
And so you get those documents

00:36:42.160 --> 00:36:43.680
and then you make your prompt.

00:36:43.680 --> 00:36:45.880
- Yeah, wild.

00:36:45.880 --> 00:36:46.720
It's really wild.

00:36:46.720 --> 00:36:50.280
So, you know, we're gonna make an 800 dimensional space

00:36:50.280 --> 00:36:53.320
and each concept gets a location in that space.

00:36:53.320 --> 00:36:56.000
And then you're gonna get another concept as a prompt.

00:36:56.000 --> 00:36:59.160
You say, what other things in this space are near it?

00:36:59.160 --> 00:37:00.400
Right? - Right.

00:37:00.400 --> 00:37:01.400
Right. - Wild.

00:37:01.400 --> 00:37:04.080
- And then what you're, you know,

00:37:04.080 --> 00:37:06.280
the hard problems that remain are,

00:37:06.280 --> 00:37:07.720
well, first you gotta figure out what you're trying to solve.

00:37:07.720 --> 00:37:09.560
So once you figure out what you're actually trying to solve,

00:37:09.560 --> 00:37:12.040
then you can start asking yourself questions like,

00:37:12.040 --> 00:37:15.680
okay, well, how do I chunk up the documents that I have?

00:37:15.680 --> 00:37:17.140
Right, and there's all these different,

00:37:17.140 --> 00:37:18.320
and there's another great place

00:37:18.320 --> 00:37:19.680
for Lama Index and LangChain.

00:37:19.680 --> 00:37:21.920
They have chunking strategies

00:37:21.920 --> 00:37:23.560
where they'll take a big giant document

00:37:23.560 --> 00:37:25.920
and break it down into sections.

00:37:25.920 --> 00:37:27.360
And then you chunk each section

00:37:27.360 --> 00:37:30.920
and then you do the embedding on just that small section.

00:37:30.920 --> 00:37:33.200
Because the idea being, can you get, you know,

00:37:33.200 --> 00:37:37.200
finer and finer sets of texts that you can then,

00:37:37.200 --> 00:37:38.980
when you do your retrieval,

00:37:38.980 --> 00:37:40.600
you get the right information back.

00:37:40.600 --> 00:37:42.340
And then the other challenge is really like

00:37:42.340 --> 00:37:43.880
the question answer problem, right?

00:37:43.880 --> 00:37:45.920
If a person's asking a question,

00:37:45.920 --> 00:37:48.200
how do you turn that question into the same kind

00:37:48.200 --> 00:37:50.280
of embedding space as the answer, right?

00:37:50.280 --> 00:37:52.120
And so there's lots of different strategies

00:37:52.120 --> 00:37:53.080
that are out there for that.

00:37:53.080 --> 00:37:55.600
And then another problem is if you're looking

00:37:55.600 --> 00:37:58.520
at the Wikipedia page for the Tower of Pisa,

00:37:58.520 --> 00:38:00.120
it might actually have like a sentence in here

00:38:00.120 --> 00:38:01.800
that says it is, you know,

00:38:01.800 --> 00:38:04.360
X number of meters tall or feet tall,

00:38:04.360 --> 00:38:06.000
but it won't actually have the word, you know,

00:38:06.000 --> 00:38:06.840
Tower of Pisa in it.

00:38:06.840 --> 00:38:09.120
So there's another chunking strategy

00:38:09.120 --> 00:38:11.360
where they call propositional trunking,

00:38:11.360 --> 00:38:14.280
where they basically use a large language model

00:38:14.280 --> 00:38:18.920
to actually redefine each sentence

00:38:18.920 --> 00:38:21.440
so that it actually has those proper nouns baked into it

00:38:21.440 --> 00:38:23.120
so that when you do the embedding,

00:38:23.120 --> 00:38:27.080
it doesn't lose some of the detail with propositions.

00:38:27.080 --> 00:38:28.320
- It's this tall, but.

00:38:28.320 --> 00:38:30.760
- It is.

00:38:30.760 --> 00:38:32.260
- It's something that replaces this tall

00:38:32.260 --> 00:38:34.680
with his actual height and things like that.

00:38:34.680 --> 00:38:35.520
- Correct, correct.

00:38:35.520 --> 00:38:37.040
- Okay, crazy.

00:38:37.040 --> 00:38:39.080
- But fundamentally you're working with unstructured data

00:38:39.080 --> 00:38:40.160
and it's kind of messy

00:38:40.160 --> 00:38:43.120
and it's not always gonna work the way you want.

00:38:43.120 --> 00:38:45.560
And there's a lot of challenges

00:38:45.560 --> 00:38:46.840
and people are trying lots of different things

00:38:46.840 --> 00:38:47.920
to make it better.

00:38:47.920 --> 00:38:48.760
- That's cool.

00:38:48.760 --> 00:38:50.840
It's not always deterministic or exactly the same,

00:38:50.840 --> 00:38:52.920
so that can be tricky as well.

00:38:52.920 --> 00:38:56.920
One of the big parts of at least this embedding stuff

00:38:56.920 --> 00:38:59.600
you're talking about are vector databases.

00:38:59.600 --> 00:39:01.040
And they used to be really rare

00:39:01.040 --> 00:39:03.520
and kind of their own specialized thing.

00:39:03.520 --> 00:39:05.720
Now they're starting to show up in lots of places.

00:39:05.720 --> 00:39:09.200
And you've shared with us this link of VectorDB comparison.

00:39:09.200 --> 00:39:10.800
I just saw that MongoDB added it.

00:39:10.800 --> 00:39:12.920
And I'm like, I didn't know that had anything to do

00:39:12.920 --> 00:39:13.760
with that.

00:39:13.760 --> 00:39:15.160
I'm probably not gonna mess with it,

00:39:15.160 --> 00:39:17.400
but it's interesting that it's just like finding its way

00:39:17.400 --> 00:39:19.320
in all these different spaces.

00:39:19.320 --> 00:39:22.480
- Yeah, it was weird there for a couple of years

00:39:22.480 --> 00:39:24.000
where people were basically like talking

00:39:24.000 --> 00:39:24.920
about vector databases,

00:39:24.920 --> 00:39:26.600
like they're their own separate thing.

00:39:26.600 --> 00:39:28.520
The vector databases are now becoming their own

00:39:28.520 --> 00:39:30.880
fully fledged, either relational database

00:39:30.880 --> 00:39:32.960
or a graph database or search engine, right?

00:39:32.960 --> 00:39:35.400
Those are kind of the three categories where,

00:39:35.400 --> 00:39:37.040
I mean, I guess Redis is its own thing too,

00:39:37.040 --> 00:39:40.280
but for the most part,

00:39:40.280 --> 00:39:42.400
those new databases, quote unquote,

00:39:42.400 --> 00:39:45.520
are now kind of trying to become more fully fledged.

00:39:45.520 --> 00:39:47.640
And vectors and semantic search

00:39:47.640 --> 00:39:49.440
is really just one feature, right?

00:39:49.440 --> 00:39:51.360
And yeah.

00:39:51.360 --> 00:39:53.160
- I was just thinking that is this thing

00:39:53.160 --> 00:39:54.000
that you're talking about,

00:39:54.000 --> 00:39:56.600
is it a product or is it a feature

00:39:56.600 --> 00:39:58.160
of a bigger product, right?

00:39:58.160 --> 00:39:59.000
- Correct.

00:39:59.000 --> 00:40:00.040
- If you already got a database,

00:40:00.040 --> 00:40:01.600
it's already doing a bunch of things.

00:40:01.600 --> 00:40:03.120
Could it just answer the vector question?

00:40:03.120 --> 00:40:04.160
Maybe, maybe not.

00:40:04.160 --> 00:40:05.080
- Exactly right.

00:40:05.080 --> 00:40:05.920
Exactly right.

00:40:05.920 --> 00:40:08.240
And the one thing to recognize is that,

00:40:08.240 --> 00:40:09.400
and then the other thing people do

00:40:09.400 --> 00:40:11.000
is they just take NumPy or what have you

00:40:11.000 --> 00:40:12.280
and just load them all into memory.

00:40:12.280 --> 00:40:13.280
And if you don't have that much data,

00:40:13.280 --> 00:40:15.680
that's actually probably gonna be the fastest

00:40:15.680 --> 00:40:17.880
and simplest way to work.

00:40:17.880 --> 00:40:19.760
But the thing you gotta recognize

00:40:19.760 --> 00:40:21.880
is the fact that there is a precision

00:40:21.880 --> 00:40:26.000
and recall and cost trade-off that happens as well.

00:40:26.000 --> 00:40:29.200
So they have to index these vectors

00:40:29.200 --> 00:40:31.760
and there's different algorithms that are used

00:40:31.760 --> 00:40:35.000
and different algorithms do better than others.

00:40:35.000 --> 00:40:36.680
So you gotta make sure you understand that as well.

00:40:36.680 --> 00:40:38.640
So, and one thing you can do

00:40:38.640 --> 00:40:40.680
is for instance, pgVector,

00:40:40.680 --> 00:40:42.920
which comes as an extension for Postgres,

00:40:42.920 --> 00:40:45.480
you can start off by not indexing at all.

00:40:45.480 --> 00:40:47.720
And you should get, I believe,

00:40:47.720 --> 00:40:49.120
hopefully I'm not misspeaking.

00:40:49.120 --> 00:40:50.280
You should get perfect recall,

00:40:50.280 --> 00:40:51.520
meaning you'll get the right answer.

00:40:51.520 --> 00:40:52.640
You'll get the, you know,

00:40:52.640 --> 00:40:56.600
if you ask for the five closest vectors to your query,

00:40:56.600 --> 00:40:57.720
you'll get the five closest,

00:40:57.720 --> 00:41:00.160
but it'll be slower than you probably want.

00:41:00.160 --> 00:41:01.480
So then you have to index it.

00:41:01.480 --> 00:41:02.920
And then what ends up happening is, you know,

00:41:02.920 --> 00:41:04.920
the next time you might only get four of those five,

00:41:04.920 --> 00:41:07.880
you'll get something else that snuck into that list.

00:41:08.680 --> 00:41:13.160
- Yeah, that's, if you got time,

00:41:13.160 --> 00:41:16.000
you're willing to spend unlimited time,

00:41:16.000 --> 00:41:20.640
then you can get the right answer, the exact answer.

00:41:20.640 --> 00:41:22.600
But I guess that's all sorts of heuristics, right?

00:41:22.600 --> 00:41:25.280
You're like, I could spend three days

00:41:25.280 --> 00:41:27.120
or I could do a Monte Carlo thing

00:41:27.120 --> 00:41:29.520
and I can give you an answer in a fraction of a second.

00:41:29.520 --> 00:41:30.360
- Right, right, right.

00:41:30.360 --> 00:41:33.360
- But it's not deterministic.

00:41:33.360 --> 00:41:34.840
All right, so then I won't go with my camera,

00:41:34.840 --> 00:41:35.680
so I turn it off.

00:41:35.680 --> 00:41:36.520
I don't know what's up with it.

00:41:36.520 --> 00:41:37.360
- Yeah, no worries, no worries.

00:41:37.360 --> 00:41:39.920
- Yeah, so you wrote a cool blog post

00:41:39.920 --> 00:41:42.600
called "What is a Custom GPT?"

00:41:42.600 --> 00:41:45.760
And we wanna talk some about building custom GPTs

00:41:45.760 --> 00:41:48.280
and with SAPI and so on.

00:41:48.280 --> 00:41:49.240
So let's talk about this.

00:41:49.240 --> 00:41:52.840
Like one of the, I think one of the challenges

00:41:52.840 --> 00:41:55.640
in why it takes so much compute for these systems

00:41:55.640 --> 00:41:57.160
is like they're open-ended.

00:41:57.160 --> 00:42:00.720
You can ask me any question about any knowledge in the world

00:42:00.720 --> 00:42:03.040
in humankind, right?

00:42:03.040 --> 00:42:04.680
You can ask about that.

00:42:04.680 --> 00:42:05.840
Let's start talking.

00:42:05.840 --> 00:42:09.640
Or it could be you can ask me about genetics.

00:42:09.640 --> 00:42:11.840
- Right.

00:42:11.840 --> 00:42:14.600
- Right, that seems like you could both get better answers

00:42:14.600 --> 00:42:17.440
if you actually only care about genetic responses,

00:42:17.440 --> 00:42:20.640
not how tall is the Leaning Tower

00:42:20.640 --> 00:42:22.520
and probably make it smaller, right?

00:42:22.520 --> 00:42:25.160
So is that kind of the idea of these custom GPTs

00:42:25.160 --> 00:42:26.400
or what is it?

00:42:26.400 --> 00:42:31.200
- No, so custom GPTs are a new capability from OpenAI

00:42:31.200 --> 00:42:34.720
and basically they're a wrapper around,

00:42:35.560 --> 00:42:37.080
a very small subset,

00:42:37.080 --> 00:42:41.240
but it's still using the OpenAI ecosystem, okay?

00:42:41.240 --> 00:42:43.400
And so what you do is you give it a name,

00:42:43.400 --> 00:42:45.720
you give it a logo, you give it a prompt.

00:42:45.720 --> 00:42:49.560
And then from there, you can also give it knowledge.

00:42:49.560 --> 00:42:51.240
You can upload PDF documents to it

00:42:51.240 --> 00:42:53.680
and it will actually slice and dice those PDF documents

00:42:53.680 --> 00:42:55.520
using some sort of vector search.

00:42:55.520 --> 00:42:57.720
We don't know how it actually works.

00:42:57.720 --> 00:43:00.200
The GPT, the cool thing is the GPT will work on your phone.

00:43:00.200 --> 00:43:01.440
Right, so I have my phone,

00:43:01.440 --> 00:43:02.800
I can have a conversation with my phone.

00:43:02.800 --> 00:43:04.720
I can take a picture, upload a picture

00:43:04.720 --> 00:43:07.280
and it will do vision analysis on it.

00:43:07.280 --> 00:43:10.600
So I get all the capabilities of OpenAI GPT 4,

00:43:10.600 --> 00:43:13.600
but a custom GPT is one that I can construct

00:43:13.600 --> 00:43:15.120
and give a custom prompt to,

00:43:15.120 --> 00:43:17.400
which basically then says, okay, now you're,

00:43:17.400 --> 00:43:18.680
and to your point, I think maybe this is

00:43:18.680 --> 00:43:19.520
where you're going with it,

00:43:19.520 --> 00:43:21.040
like, hey, now you're an expert in genomics

00:43:21.040 --> 00:43:22.440
or you're an expert in something

00:43:22.440 --> 00:43:27.280
and you're basically coaching the language model

00:43:27.280 --> 00:43:29.440
in what it can and can't do.

00:43:29.440 --> 00:43:34.440
And so basically, it's a targeted experience

00:43:34.440 --> 00:43:36.920
within the large language,

00:43:36.920 --> 00:43:39.800
within the ChatGPT ecosystem.

00:43:39.800 --> 00:43:41.920
It has access to also the OpenAI tools,

00:43:41.920 --> 00:43:44.280
like so OpenAI has the ability to do code interpreter

00:43:44.280 --> 00:43:47.320
and Dolly and it can also hit the web browser.

00:43:47.320 --> 00:43:49.120
So you have access to everything.

00:43:49.120 --> 00:43:51.560
But the interesting thing to me is the fact

00:43:51.560 --> 00:43:52.680
that you can actually tie this thing

00:43:52.680 --> 00:43:54.240
to what are called actions.

00:43:54.240 --> 00:43:56.640
So about March, I think of last year,

00:43:56.640 --> 00:43:58.280
they actually had this capability called plugins

00:43:58.280 --> 00:43:59.560
that they announced.

00:43:59.560 --> 00:44:02.480
And plugins have kind of faded to the background.

00:44:02.480 --> 00:44:04.520
I don't know if they're gonna deprecate them officially,

00:44:04.520 --> 00:44:07.080
but the basic gist with plugins is what was,

00:44:07.080 --> 00:44:09.880
you could turn that on and it can then call your API.

00:44:09.880 --> 00:44:11.080
And the cool thing about it was

00:44:11.080 --> 00:44:13.440
that it read your OpenAPI spec, right?

00:44:13.440 --> 00:44:16.560
So you write an OpenAPI spec, which is Swagger,

00:44:16.560 --> 00:44:18.160
if you're familiar with Swagger,

00:44:18.160 --> 00:44:20.120
and it basically defines what all the endpoints are,

00:44:20.120 --> 00:44:23.600
what the path is, what the inputs and outputs are,

00:44:23.600 --> 00:44:27.680
including all the classes or field level information

00:44:27.680 --> 00:44:29.240
and any constraints or what have you.

00:44:29.240 --> 00:44:32.640
So you can fully define your OpenAPI spec.

00:44:32.640 --> 00:44:34.360
It can then call that OpenAPI spec

00:44:34.360 --> 00:44:36.840
and it's basically giving it tools.

00:44:36.840 --> 00:44:38.880
So like the example that they say in the documentation

00:44:38.880 --> 00:44:39.800
is get the weather, right?

00:44:39.800 --> 00:44:41.880
So if you say, what's the weather in Boston?

00:44:41.880 --> 00:44:43.840
Well, Chetchi BT doesn't know the weather in Boston.

00:44:43.840 --> 00:44:45.120
All it knows how to do is call it,

00:44:45.120 --> 00:44:47.640
but you can call an API and it figures out

00:44:47.640 --> 00:44:49.480
how to call the API, get that information,

00:44:49.480 --> 00:44:51.640
and then it can use that to redisplay.

00:44:51.640 --> 00:44:54.080
And that's a very basic example.

00:44:54.080 --> 00:44:57.120
You can do way more complicated things than that.

00:44:57.120 --> 00:44:59.720
And it's pretty powerful.

00:44:59.720 --> 00:45:02.880
>>Okay, that sounds really pretty awesome.

00:45:02.880 --> 00:45:05.760
I thought a lot about different things that I might build.

00:45:05.760 --> 00:45:10.720
Yeah, I guess on your blog post here,

00:45:10.720 --> 00:45:14.200
you've got some key benefits and you've got some risks.

00:45:14.200 --> 00:45:16.840
You maybe wanna talk a bit about that.

00:45:16.840 --> 00:45:21.200
>>Yeah, so the first part with plugins that didn't work

00:45:21.200 --> 00:45:24.240
as well is that there was no kind of overarching

00:45:24.240 --> 00:45:27.040
custom instruction that could actually teach it

00:45:27.040 --> 00:45:28.120
how to work with your plugin.

00:45:28.120 --> 00:45:30.400
So if you couldn't put it in the API spec,

00:45:30.400 --> 00:45:32.920
then you couldn't integrate it with a bunch of other stuff

00:45:32.920 --> 00:45:35.000
or other capabilities, right?

00:45:35.000 --> 00:45:37.120
So the custom instruction is really a key thing

00:45:37.120 --> 00:45:39.440
for making these custom APIs strong.

00:45:39.440 --> 00:45:41.360
But one warning about the custom instruction,

00:45:41.360 --> 00:45:43.840
whatever you put in there, anybody can download, right?

00:45:43.840 --> 00:45:46.200
Not just the folks at OpenAI, anybody.

00:45:46.200 --> 00:45:48.000
Basically, there's GitHub projects

00:45:48.000 --> 00:45:51.760
where thousands of these, the custom prompts

00:45:51.760 --> 00:45:54.520
that people have put into their GPT,

00:45:54.520 --> 00:45:56.240
and there are now knockoffs on GPT.

00:45:56.240 --> 00:45:59.880
So it's all kind of a mess right now in the OpenAI store.

00:45:59.880 --> 00:46:02.000
I'm sure they'll clean it up, but just recognize

00:46:02.000 --> 00:46:03.760
the custom instruction is not protected

00:46:03.760 --> 00:46:05.160
and neither is the knowledge.

00:46:05.160 --> 00:46:07.520
So if you upload a PDF, there have been people

00:46:07.520 --> 00:46:10.040
that have been figuring out how to download those PDFs.

00:46:10.040 --> 00:46:13.040
And I think that that might be a solved problem now

00:46:13.040 --> 00:46:15.520
or they're working on it, but it's something to know.

00:46:15.520 --> 00:46:17.680
The other problem with plugins was,

00:46:17.680 --> 00:46:19.000
I can get a plugin working,

00:46:19.000 --> 00:46:20.920
but if they didn't approve my plugin

00:46:20.920 --> 00:46:23.040
and put it in their plugin store,

00:46:23.040 --> 00:46:24.800
I couldn't share it with other people.

00:46:24.800 --> 00:46:27.920
The way it works now is I can actually make a GPT

00:46:27.920 --> 00:46:30.080
and I can give it to you and you can use it directly,

00:46:30.080 --> 00:46:33.120
even if it's not in the OpenAPI store or OpenAI store.

00:46:33.120 --> 00:46:35.480
It is super easy to get started.

00:46:35.480 --> 00:46:39.160
They have a tool to help you generate your DALI picture.

00:46:39.160 --> 00:46:40.640
And actually, you don't even have to figure out

00:46:40.640 --> 00:46:42.120
how to do the custom instructions yourself.

00:46:42.120 --> 00:46:44.960
You can just kind of chat that into existence.

00:46:44.960 --> 00:46:46.840
But the thing that I'm really excited about

00:46:46.840 --> 00:46:48.720
is that this is free playing.

00:46:48.720 --> 00:46:54.000
So the hosting cost is basically all on the client side.

00:46:54.000 --> 00:46:56.480
You have to be a ChatGPT plus user right now

00:46:56.480 --> 00:46:58.080
to create these and use these.

00:46:58.080 --> 00:47:01.000
But the cool thing as a developer,

00:47:01.000 --> 00:47:02.520
I don't have to pay those API fees

00:47:02.520 --> 00:47:04.280
that we were talking about.

00:47:04.280 --> 00:47:06.160
And if I need to use GPT for,

00:47:06.160 --> 00:47:07.920
which I kind of do for my business right now,

00:47:07.920 --> 00:47:10.200
just because of how complicated it is,

00:47:10.200 --> 00:47:12.040
I don't have to pay those token fees

00:47:12.040 --> 00:47:16.280
for folks using my custom GPT at this moment.

00:47:16.280 --> 00:47:19.200
And then the risks go-

00:47:19.200 --> 00:47:21.440
- Where's the billing or whatever you call it

00:47:21.440 --> 00:47:23.320
for the custom GPT lie?

00:47:23.320 --> 00:47:25.040
Is that in the person who's using it?

00:47:25.040 --> 00:47:26.600
Does it have to, it goes onto their account

00:47:26.600 --> 00:47:28.200
and whatever their account?

00:47:28.200 --> 00:47:30.520
- Yeah, so everyone just, right now,

00:47:30.520 --> 00:47:35.520
open API, ChatGPT plus is $20 a month.

00:47:35.520 --> 00:47:37.040
And then there's a team's version,

00:47:37.040 --> 00:47:38.520
which I think is either 25 or 30,

00:47:38.520 --> 00:47:42.120
depending on number of users or how you pay for it.

00:47:42.120 --> 00:47:45.240
And then, and that's the cost.

00:47:45.240 --> 00:47:48.440
So right now, if you wanna use custom GPTs,

00:47:48.440 --> 00:47:51.320
everyone needs to be a ChatGPT plus user.

00:47:51.320 --> 00:47:54.720
There's no extra cost based on usage or anything like that.

00:47:54.720 --> 00:47:59.360
In fact, there's talk about revenue sharing

00:47:59.360 --> 00:48:02.120
between open AI and developers of custom GPTs,

00:48:02.120 --> 00:48:03.800
but that has not come out yet

00:48:03.800 --> 00:48:05.760
as far as like what those details are.

00:48:05.760 --> 00:48:09.120
- It does have an app store feel to it, doesn't it?

00:48:09.120 --> 00:48:10.960
- Mm-hmm, yep, for sure, for sure.

00:48:10.960 --> 00:48:14.280
And then, but there's risks too, right?

00:48:14.280 --> 00:48:15.240
Obviously anybody can,

00:48:15.240 --> 00:48:18.240
there's already been like tons of copies up there.

00:48:18.240 --> 00:48:23.240
Open AI, they're looking for their business model too,

00:48:23.240 --> 00:48:27.080
so they could, if someone has a very successful custom GPT,

00:48:27.080 --> 00:48:28.080
it's well within their right

00:48:28.080 --> 00:48:30.480
to kind of add that to the base product as well.

00:48:30.480 --> 00:48:34.240
And then prompt injection is still a thing.

00:48:34.240 --> 00:48:36.160
So if you're doing anything in your actions

00:48:36.160 --> 00:48:37.600
that actually changes something,

00:48:37.600 --> 00:48:40.680
that is consequential is what they call it,

00:48:40.680 --> 00:48:42.680
you better think very carefully,

00:48:42.680 --> 00:48:45.400
like what's the worst thing that could happen, right?

00:48:45.400 --> 00:48:47.520
'Cause whatever the worst thing that could happen is,

00:48:47.520 --> 00:48:48.440
that's what's gonna happen

00:48:48.440 --> 00:48:50.440
'cause people can figure this stuff out

00:48:50.440 --> 00:48:52.800
and they can confuse the large language models

00:48:52.800 --> 00:48:54.640
into calling them, right?

00:48:54.640 --> 00:48:56.000
- And the more valuable it is

00:48:56.000 --> 00:48:57.680
that they can make that thing happen,

00:48:57.680 --> 00:48:59.640
the more effort they're gonna put into it as well.

00:48:59.640 --> 00:49:02.080
Yeah, yeah, yeah. - For sure, for sure.

00:49:02.080 --> 00:49:04.560
And then just in general, you're, oh, go ahead.

00:49:04.560 --> 00:49:06.160
- I was gonna ask, do you think,

00:49:06.160 --> 00:49:10.960
it's easy to solve SQL injection

00:49:10.960 --> 00:49:13.140
and other forms of injection,

00:49:13.140 --> 00:49:15.480
least in principle, right?

00:49:15.480 --> 00:49:17.120
There's a education problem,

00:49:17.120 --> 00:49:20.040
there's millions of people coming along as developers

00:49:20.040 --> 00:49:22.800
and they see some demo that says,

00:49:22.800 --> 00:49:25.720
the query is like this plus the name.

00:49:25.720 --> 00:49:27.520
Wait a minute, wait.

00:49:27.520 --> 00:49:32.000
So it kind of recreates itself through not total awareness,

00:49:32.000 --> 00:49:36.160
but there's a very clear thing you do to solve that,

00:49:36.160 --> 00:49:38.040
you use parameters, you don't concatenate strings

00:49:38.040 --> 00:49:40.120
with user input, problem solved.

00:49:40.120 --> 00:49:41.880
What about prompt injection though?

00:49:41.880 --> 00:49:46.440
It's so vague how these AIs know what to do

00:49:46.440 --> 00:49:47.440
in the first place.

00:49:47.440 --> 00:49:51.800
And so then how do you completely block that off?

00:49:51.800 --> 00:49:53.040
- Unsolved problem, right?

00:49:53.040 --> 00:49:56.000
Like, and I'm definitely stealing from Simon on this

00:49:56.000 --> 00:49:57.880
'cause I've heard him say it on a few podcasts

00:49:57.880 --> 00:50:01.640
is just basically there's no solution as far as we know.

00:50:01.640 --> 00:50:03.120
So you have to design,

00:50:03.120 --> 00:50:07.140
and there's no solution to the hallucination problem either

00:50:07.140 --> 00:50:08.960
'cause that's a feature, right?

00:50:08.960 --> 00:50:11.240
That's actually what the thing is supposed to do.

00:50:11.240 --> 00:50:12.680
So when you're building these systems,

00:50:12.680 --> 00:50:15.340
you have to recognize those two facts

00:50:15.340 --> 00:50:18.840
along with some other facts that really limit

00:50:18.840 --> 00:50:20.680
what you can build with these things.

00:50:20.680 --> 00:50:24.560
- So you shouldn't use it for like legal briefs,

00:50:24.560 --> 00:50:26.520
is that what you're saying?

00:50:26.520 --> 00:50:28.620
- Well, once again, I think these things

00:50:28.620 --> 00:50:31.280
are great collaborative tools, right?

00:50:31.280 --> 00:50:32.280
The human in the loop,

00:50:32.280 --> 00:50:33.520
and that's everything that I'm building, right?

00:50:33.520 --> 00:50:34.920
So all the stuff that I'm building

00:50:34.920 --> 00:50:36.800
is assuming that the human's in the loop

00:50:36.800 --> 00:50:41.040
and what I'm trying to do is augment and amplify expertise.

00:50:41.040 --> 00:50:44.440
I'm building tools for people that know about genomics

00:50:44.440 --> 00:50:46.720
and cancer and how to help cancer patients.

00:50:46.720 --> 00:50:48.240
I'm not designing it for cancer patients

00:50:48.240 --> 00:50:50.640
who are gonna go operate on themselves, right?

00:50:50.640 --> 00:50:52.120
Like that's not the goal.

00:50:52.120 --> 00:50:56.020
The idea is there's a lot of information.

00:50:56.020 --> 00:50:59.280
These tools are super valuable

00:50:59.280 --> 00:51:03.280
from like synthesizing a variety of info,

00:51:03.280 --> 00:51:05.920
but you still need to look at the underlying citations.

00:51:05.920 --> 00:51:08.320
And ChatGPT by itself can't give you citations.

00:51:08.320 --> 00:51:10.480
Like it'll make some up.

00:51:10.480 --> 00:51:11.680
It'll say, "Oh, I think there's probably

00:51:11.680 --> 00:51:13.600
"a Wikipedia page with this link."

00:51:13.600 --> 00:51:15.400
But you actually have to,

00:51:15.400 --> 00:51:17.880
you definitely have to have an outside tool,

00:51:17.880 --> 00:51:20.520
either the web, Bing, which is, I would say,

00:51:20.520 --> 00:51:23.100
subpar for a lot of use cases,

00:51:23.100 --> 00:51:24.340
or you have to have actions

00:51:24.340 --> 00:51:26.400
that can actually bring back references

00:51:26.400 --> 00:51:27.320
and give you those links.

00:51:27.320 --> 00:51:28.440
And then the expert will then say,

00:51:28.440 --> 00:51:29.280
"Oh, okay, great.

00:51:29.280 --> 00:51:31.720
"Thanks for synthesizing this, giving me this info.

00:51:31.720 --> 00:51:33.360
"Let me go validate this myself."

00:51:33.360 --> 00:51:35.880
Right, go click on the link and go validate it.

00:51:35.880 --> 00:51:37.960
And that's really, I think that's really the sweet spot

00:51:37.960 --> 00:51:40.000
for these things, at least for the near future.

00:51:40.000 --> 00:51:42.440
- Yeah, don't ask it for the answer.

00:51:42.440 --> 00:51:45.040
Ask it to help you come up with the answer, right?

00:51:45.040 --> 00:51:46.000
- Exactly right.

00:51:46.000 --> 00:51:49.440
And then have it criticize you when you do have something,

00:51:49.440 --> 00:51:50.640
'cause then it'll do a great job

00:51:50.640 --> 00:51:53.020
of telling you everything you've done wrong.

00:51:53.020 --> 00:51:55.560
- I'm feeling too good about myself.

00:51:55.560 --> 00:51:57.200
I need you to insult me a lot.

00:51:57.200 --> 00:51:58.040
Let's get going.

00:51:58.040 --> 00:51:58.880
All right.

00:51:58.880 --> 00:52:02.320
Speaking of talking about ourselves,

00:52:02.320 --> 00:52:04.800
you've got this project called PyPI GPT.

00:52:04.800 --> 00:52:06.240
What's this about?

00:52:06.240 --> 00:52:08.140
- Yeah, so I really wanted to tell people

00:52:08.140 --> 00:52:10.360
that FastAPI and PyDiantic,

00:52:10.360 --> 00:52:12.120
'cause Python, like we were saying earlier,

00:52:12.120 --> 00:52:13.640
I don't know if it was on the call or not,

00:52:13.640 --> 00:52:16.880
but Python is the winning language, right?

00:52:16.880 --> 00:52:18.720
And I think FastAPI and PyDiantic

00:52:18.720 --> 00:52:21.280
are the winning libraries in their respective fields,

00:52:21.280 --> 00:52:22.120
and they're great.

00:52:22.120 --> 00:52:23.760
And they're perfect for this space,

00:52:23.760 --> 00:52:25.800
because you need an open API spec.

00:52:25.800 --> 00:52:29.000
English is the new programming language, right?

00:52:29.000 --> 00:52:31.920
So Andrej Koparthe, who used to work at Tesla

00:52:31.920 --> 00:52:34.240
and now works at OpenAI, has this pinned tweet

00:52:34.240 --> 00:52:35.200
where he's basically like,

00:52:35.200 --> 00:52:36.840
"English is like the hottest programming language,"

00:52:36.840 --> 00:52:37.840
or something like that.

00:52:37.840 --> 00:52:39.360
And that's really the truth,

00:52:39.360 --> 00:52:40.760
'cause even in this space

00:52:40.760 --> 00:52:43.400
where I'm building an open API spec,

00:52:43.400 --> 00:52:45.520
99% of the work is like

00:52:45.520 --> 00:52:48.360
thinking about the description of the endpoints

00:52:48.360 --> 00:52:50.760
or the description of the fields,

00:52:50.760 --> 00:52:54.480
or codifying the constraints on different fields.

00:52:54.480 --> 00:52:56.760
Like you can use these greater thans and less thans

00:52:56.760 --> 00:52:59.720
and regexes, right, to describe it.

00:52:59.720 --> 00:53:01.120
And so what I did was I said,

00:53:01.120 --> 00:53:03.840
"Okay, let's build this thing in FastAPI,"

00:53:03.840 --> 00:53:05.720
just to get an example out for folks.

00:53:05.720 --> 00:53:08.440
And then I turned it on,

00:53:08.440 --> 00:53:11.680
and I actually use ngrok as my service layer,

00:53:11.680 --> 00:53:13.960
'cause you have to have HTTPS to make this thing work.

00:53:13.960 --> 00:53:16.000
So I figured out- - Ngrok is so good.

00:53:16.000 --> 00:53:17.920
- Yep, yep. - Yeah, yeah.

00:53:17.920 --> 00:53:20.280
- I turned that on with an Nginx thing in front of it.

00:53:20.280 --> 00:53:22.560
So this library, to actually use it,

00:53:22.560 --> 00:53:25.640
you'll have to actually set that stuff up yourself.

00:53:25.640 --> 00:53:27.120
You have to download it, you have to run it,

00:53:27.120 --> 00:53:29.880
you have to either get it on a server with HTTPS,

00:53:29.880 --> 00:53:31.520
with Let's Encrypt or something.

00:53:31.520 --> 00:53:33.880
But then once you've turned it on,

00:53:33.880 --> 00:53:35.040
then you can actually see

00:53:35.040 --> 00:53:37.320
how it generates the open API spec,

00:53:37.320 --> 00:53:39.760
how to configure the GPT.

00:53:39.760 --> 00:53:41.280
I didn't do much work with regards

00:53:41.280 --> 00:53:43.080
to the custom instructions that I came up with.

00:53:43.080 --> 00:53:45.160
I just said, "Hey, call my API, figure it out."

00:53:45.160 --> 00:53:46.080
And it does.

00:53:46.080 --> 00:53:48.000
And so what this GPT does is it basically says,

00:53:48.000 --> 00:53:49.800
"Okay, given a package name and a version number,

00:53:49.800 --> 00:53:51.600
"it's gonna go and grab this data

00:53:51.600 --> 00:53:53.160
"from the SQLite database that I found

00:53:53.160 --> 00:53:54.460
"that has this information,

00:53:54.460 --> 00:53:55.520
"and then bring it back to you."

00:53:55.520 --> 00:53:56.960
It's the least interesting GPT

00:53:56.960 --> 00:53:58.000
I could come up with, I guess.

00:53:58.000 --> 00:54:00.720
But it shows kind of the mechanics, right?

00:54:00.720 --> 00:54:04.880
The mechanics of setting up the servers

00:54:04.880 --> 00:54:08.080
and the application within FastAPI,

00:54:08.080 --> 00:54:12.320
the kind of the little bits that you have to flip

00:54:12.320 --> 00:54:16.160
to make sure that OpenAI can understand

00:54:16.160 --> 00:54:18.560
your OpenAPI spec.

00:54:18.560 --> 00:54:21.600
I bumble through OpenAI and OpenAPI all the time.

00:54:21.600 --> 00:54:23.520
And make sure that they can talk to each other.

00:54:23.520 --> 00:54:25.280
And then it will then do the right thing

00:54:25.280 --> 00:54:28.680
and call your server and bring the answers back.

00:54:28.680 --> 00:54:32.080
And there's a bunch of little flags and information

00:54:32.080 --> 00:54:33.940
you need to know about actions

00:54:33.940 --> 00:54:37.320
that are on the OpenAPI documentation.

00:54:37.320 --> 00:54:39.320
And so I tried to pull that all together

00:54:39.320 --> 00:54:42.180
into one simple little project for people to look at.

00:54:42.180 --> 00:54:44.320
- That's cool.

00:54:44.320 --> 00:54:45.140
So you can ask it questions like,

00:54:45.140 --> 00:54:47.760
"Tell me about FastAPI, this version."

00:54:47.760 --> 00:54:49.000
And it'll come back and do it.

00:54:49.000 --> 00:54:50.080
- Correct.

00:54:50.080 --> 00:54:51.560
I was hoping to do something a little better,

00:54:51.560 --> 00:54:53.440
like, "Hey, here's my requirements file."

00:54:53.440 --> 00:54:56.840
And go, "Tell me, am I on the latest version

00:54:56.840 --> 00:54:58.800
"of everything or whatever?"

00:54:58.800 --> 00:54:59.880
Something more interesting.

00:54:59.880 --> 00:55:01.000
I just didn't have time.

00:55:01.000 --> 00:55:01.840
So--

00:55:01.840 --> 00:55:03.440
- Can you ask it questions such as,

00:55:03.440 --> 00:55:05.320
"What's the difference between this version

00:55:05.320 --> 00:55:07.120
"and that version?"

00:55:07.120 --> 00:55:09.000
- You could, if that information's in the database.

00:55:09.000 --> 00:55:10.700
I actually don't know if it is.

00:55:10.700 --> 00:55:12.980
So it's, and then obviously you could also

00:55:12.980 --> 00:55:14.520
hit the PyPI server.

00:55:14.520 --> 00:55:15.360
And I didn't do that.

00:55:15.360 --> 00:55:17.280
I just wanted to, I don't wanna be, you know,

00:55:17.280 --> 00:55:20.560
hitting anybody's server indiscriminately at this point.

00:55:20.560 --> 00:55:23.640
But that would be a great use case, right?

00:55:23.640 --> 00:55:25.280
So like someone could take this

00:55:25.280 --> 00:55:29.220
and certainly add some capabilities.

00:55:29.220 --> 00:55:33.020
The thing that is valuable that I'm trying to showcase

00:55:33.020 --> 00:55:36.360
is the fact that ChatGPT and large language models,

00:55:36.360 --> 00:55:38.120
while they do have the world's information

00:55:38.120 --> 00:55:41.820
kind of compressed, you know, at a point in time,

00:55:41.820 --> 00:55:43.480
they are still not a database, right?

00:55:43.480 --> 00:55:46.000
They don't do well when you're basically trying

00:55:46.000 --> 00:55:48.280
to make sure you have a comprehensive query

00:55:48.280 --> 00:55:49.800
and you've brought back all the information.

00:55:49.800 --> 00:55:51.120
And they're also not good from like

00:55:51.120 --> 00:55:52.800
a up-to-date perspective, right?

00:55:52.800 --> 00:55:53.860
There's a cutoff date.

00:55:53.860 --> 00:55:56.200
Thankfully, they finally updated that recently.

00:55:56.200 --> 00:55:58.340
I think it's now April of 2023.

00:55:58.340 --> 00:56:01.280
But at some point, it just doesn't know about newer things.

00:56:02.240 --> 00:56:05.800
And so a GPT is a really interesting way of doing that.

00:56:05.800 --> 00:56:07.440
Like I'm gonna put it out in the universe

00:56:07.440 --> 00:56:08.640
and hopefully someone will do it.

00:56:08.640 --> 00:56:11.280
Make me a modern Python GPT,

00:56:11.280 --> 00:56:14.400
which is basically like get the new version of Pydantic

00:56:14.400 --> 00:56:16.360
and Polars and a few other libraries

00:56:16.360 --> 00:56:18.660
that ChatGPT does a bad job at,

00:56:18.660 --> 00:56:21.580
just because they're in underactive development

00:56:21.580 --> 00:56:24.620
during the time that ChatGPT was getting trained.

00:56:24.620 --> 00:56:28.300
So, you know, that's the perfect use case

00:56:28.300 --> 00:56:30.180
for these types of custom GPTs

00:56:30.180 --> 00:56:34.200
with knowledge in a PDF file or an API backing it up.

00:56:34.200 --> 00:56:37.780
- I think there's a ton of value in being able

00:56:37.780 --> 00:56:40.620
to feed a little bit of your information,

00:56:40.620 --> 00:56:43.700
some of your documents or your code repository

00:56:43.700 --> 00:56:46.500
or something to a GPT

00:56:46.500 --> 00:56:48.900
and then be able to ask it questions.

00:56:48.900 --> 00:56:49.740
- Exactly right.

00:56:49.740 --> 00:56:50.560
- Right?

00:56:50.560 --> 00:56:51.400
- Yeah.

00:56:51.400 --> 00:56:52.220
- Yeah.

00:56:52.220 --> 00:56:54.820
Like, you know, tell me about the security vulnerabilities

00:56:54.820 --> 00:56:56.020
that you see in the code.

00:56:56.020 --> 00:57:00.000
Like, is there anywhere where I'm missing some test

00:57:00.000 --> 00:57:03.860
or I'm calling a function in a way that's known to be bad?

00:57:03.860 --> 00:57:07.260
And, you know, like that kind of stuff is really tricky,

00:57:07.260 --> 00:57:09.680
but it's also tricky because it doesn't,

00:57:09.680 --> 00:57:11.340
even if you paste in a little bit of code,

00:57:11.340 --> 00:57:12.900
it's not the whole project, right?

00:57:12.900 --> 00:57:14.820
So, you know, to put a little bit more in there

00:57:14.820 --> 00:57:16.460
is pretty awesome.

00:57:16.460 --> 00:57:17.300
- Yeah, for sure.

00:57:17.300 --> 00:57:20.140
And yeah, being able to give it, you know,

00:57:20.140 --> 00:57:22.980
all the code from some of these code repositories, right?

00:57:22.980 --> 00:57:25.140
Like, and bringing back the relevant information.

00:57:25.140 --> 00:57:27.020
So I think there is a kind of this race.

00:57:27.020 --> 00:57:28.460
There's gonna be other, you know, cool,

00:57:28.460 --> 00:57:31.540
there's another cool project called SourceGraph and Cody

00:57:31.540 --> 00:57:33.720
that we can talk about that will, you know,

00:57:33.720 --> 00:57:36.800
run on your local server and basically indexes

00:57:36.800 --> 00:57:39.320
your code base and it'll bring back relevant snippets

00:57:39.320 --> 00:57:42.360
from your code base and answer questions kind of in context.

00:57:42.360 --> 00:57:46.400
And, you know, long-term, and then there's a new project,

00:57:46.400 --> 00:57:49.880
or I don't know how new, Codium, they had a new paper

00:57:49.880 --> 00:57:52.560
where they talked about flow engineering.

00:57:52.560 --> 00:57:55.280
And flow engineering is just basically that same concept

00:57:55.280 --> 00:57:58.660
of the human in the loop with the LLM, with the code.

00:57:58.660 --> 00:58:01.920
That's the magic combination of kind of those entities

00:58:01.920 --> 00:58:03.740
kind of iterating with each other.

00:58:03.740 --> 00:58:07.760
So, and I think these, you know,

00:58:07.760 --> 00:58:09.060
these tools are definitely gonna evolve

00:58:09.060 --> 00:58:13.600
and you really wanna have the ability to have access

00:58:13.600 --> 00:58:15.920
to your specific information

00:58:15.920 --> 00:58:17.620
to answer your specific questions.

00:58:17.620 --> 00:58:19.920
- Cody is new to me.

00:58:19.920 --> 00:58:22.480
- Yeah, it's good. - Cody.dev.

00:58:22.480 --> 00:58:27.160
And it's little subtitle or whatever is,

00:58:27.160 --> 00:58:30.160
Cody is a coding assistant that uses AI,

00:58:30.160 --> 00:58:31.760
understand your code base, right?

00:58:31.760 --> 00:58:33.720
It was saying, what was it?

00:58:33.720 --> 00:58:38.200
It was about your entire code base,

00:58:38.200 --> 00:58:40.120
APIs, implementations, and idioms.

00:58:40.120 --> 00:58:42.320
Like that's kind of what I was suggesting,

00:58:42.320 --> 00:58:43.600
at least for code, right?

00:58:43.600 --> 00:58:46.960
- Yeah, and Sourcegraph, those folks really understand

00:58:46.960 --> 00:58:50.240
code indexing and searching.

00:58:50.240 --> 00:58:51.640
Like that's what the first product was.

00:58:51.640 --> 00:58:52.880
I think we're kind of just teed up ready

00:58:52.880 --> 00:58:54.640
for this large language model moment.

00:58:54.640 --> 00:58:57.480
And then they said, oh, let's just put Cody on top of that.

00:58:57.480 --> 00:58:59.640
So this thing will run, it will understand your code

00:58:59.640 --> 00:59:01.840
and it will kind of bring things together for you.

00:59:01.840 --> 00:59:04.480
So, and these folks do podcasts all the time.

00:59:04.480 --> 00:59:05.680
I'd reach out to them.

00:59:05.680 --> 00:59:08.040
- Okay, yeah, interesting.

00:59:08.040 --> 00:59:09.520
It's quite neat looking.

00:59:09.520 --> 00:59:10.720
I think I'm gonna give it a try.

00:59:10.720 --> 00:59:14.440
It both plugs into PyCharm and VS Code.

00:59:14.440 --> 00:59:15.560
So that's pretty neat.

00:59:15.560 --> 00:59:18.620
All right, well.

00:59:18.620 --> 00:59:20.880
- Very cool.

00:59:20.880 --> 00:59:21.720
- Let's see.

00:59:21.720 --> 00:59:23.480
So I think we're starting to get a little bit short

00:59:23.480 --> 00:59:25.000
on time here, but, you know,

00:59:25.000 --> 00:59:27.520
for people who want to play with the PyPI GPT,

00:59:27.520 --> 00:59:29.960
maybe as an example, to just cut the readme

00:59:29.960 --> 00:59:31.440
and it's easy to get from there.

00:59:31.440 --> 00:59:33.520
What do you need to tell them?

00:59:33.520 --> 00:59:35.200
- Yeah, I put a make file in there.

00:59:35.200 --> 00:59:36.600
So you know exactly like the steps

00:59:36.600 --> 00:59:38.840
to kind of make the environment, download the files

00:59:38.840 --> 00:59:42.240
and just ping me, follow me on Twitter,

00:59:42.240 --> 00:59:45.120
I'm more and ping me if you need anything there.

00:59:45.120 --> 00:59:48.600
I'm also on LinkedIn and GitHub, right?

00:59:48.600 --> 00:59:50.120
So you can certainly reach out

00:59:50.120 --> 00:59:52.400
if you have any challenges.

00:59:52.400 --> 00:59:53.960
- Excellent.

00:59:53.960 --> 00:59:56.080
All right, well, yeah, go ahead.

00:59:56.080 --> 00:59:58.880
- Yeah, I'd say like the last thing that I'm, you know,

00:59:58.880 --> 01:00:00.840
folks that are actually in the medical space, right?

01:00:00.840 --> 01:00:03.640
So the thing that I'm working on right now actively

01:00:03.640 --> 01:00:07.160
is how to integrate this thing with our knowledge base,

01:00:07.160 --> 01:00:08.000
right?

01:00:08.000 --> 01:00:10.720
So I have a knowledge base of hand curated trials,

01:00:10.720 --> 01:00:13.880
hand curated therapies and other information.

01:00:13.880 --> 01:00:17.120
I've built it so that, you know,

01:00:17.120 --> 01:00:19.400
my custom GPT can actually work with that.

01:00:19.400 --> 01:00:21.360
I've come up with some, I'd say novel,

01:00:21.360 --> 01:00:22.840
at least I haven't seen anybody else

01:00:22.840 --> 01:00:24.400
and I haven't seen any research

01:00:24.400 --> 01:00:28.400
approaching things the same way I am

01:00:28.400 --> 01:00:30.200
that handles some of the other challenges

01:00:30.200 --> 01:00:31.160
that are out there, right?

01:00:31.160 --> 01:00:34.200
So for instance, the context window is a challenge.

01:00:34.200 --> 01:00:36.480
So the context window is the amount of text

01:00:36.480 --> 01:00:40.680
that's in there and how it gets processed.

01:00:40.680 --> 01:00:45.120
If you're making decisions and you're changing course,

01:00:45.120 --> 01:00:48.440
the chatbot will lose track of those changes, right?

01:00:48.440 --> 01:00:50.120
So if you're, you know, experimenting

01:00:50.120 --> 01:00:52.280
or going down one path of inquiry

01:00:52.280 --> 01:00:54.600
and then you switch to another path,

01:00:54.600 --> 01:00:57.840
it can get confused and forget that you switched paths.

01:00:57.840 --> 01:00:58.680
And it can also--

01:00:58.680 --> 01:01:00.720
- It's still not a space to hold all that information.

01:01:00.720 --> 01:01:03.800
Like, well, it forgot the last three things,

01:01:03.800 --> 01:01:04.640
the first three things you told it.

01:01:04.640 --> 01:01:06.560
It only knows four and you think it knows seven

01:01:06.560 --> 01:01:09.280
and it's working incomplete, right?

01:01:09.280 --> 01:01:10.120
- Yep.

01:01:10.120 --> 01:01:12.280
And, you know, one of the key things is

01:01:12.280 --> 01:01:15.760
you actually want it to forget some things as well, right?

01:01:15.760 --> 01:01:18.280
So those are all interesting challenges.

01:01:18.280 --> 01:01:22.360
And I'm actually working with these custom GPTs

01:01:22.360 --> 01:01:26.520
to kind of change the way that the collaboration works

01:01:26.520 --> 01:01:29.840
between the human, the expert,

01:01:29.840 --> 01:01:32.440
the large language model or the assistant,

01:01:32.440 --> 01:01:35.880
and my backend, my actual retrieval model,

01:01:35.880 --> 01:01:38.560
the API that's actually doing stuff.

01:01:38.560 --> 01:01:43.560
- So are researchers and MDs and PhDs at your company

01:01:43.880 --> 01:01:46.760
talking about this thing and making use of it?

01:01:46.760 --> 01:01:47.600
- For sure.

01:01:47.600 --> 01:01:49.840
Yeah, I mean, we're in active development right now.

01:01:49.840 --> 01:01:51.120
We have a few key opinion leaders

01:01:51.120 --> 01:01:53.640
that are working with us and collaborating with us,

01:01:53.640 --> 01:01:54.920
but we're always looking for more folks

01:01:54.920 --> 01:01:57.600
that are in the field that actually are.

01:01:57.600 --> 01:02:00.960
And right now you need kind of the cutting edge people

01:02:00.960 --> 01:02:04.080
'cause this stuff's not ready for prime time.

01:02:04.080 --> 01:02:07.400
Clinical decision support is a really hard problem.

01:02:07.400 --> 01:02:09.960
And, but we need the folks that are,

01:02:09.960 --> 01:02:11.200
that wanna get ahead of it

01:02:11.200 --> 01:02:13.040
'cause we know that there are doctors

01:02:13.040 --> 01:02:13.880
and there are patients

01:02:13.880 --> 01:02:15.920
that are asking ChatGPT questions right now.

01:02:15.920 --> 01:02:18.400
And even if it says, I'm not a medical expert, blah, blah,

01:02:18.400 --> 01:02:19.240
blah.

01:02:19.240 --> 01:02:20.560
And at the end of the day,

01:02:20.560 --> 01:02:22.400
we actually don't have enough doctors, right?

01:02:22.400 --> 01:02:23.880
That's the other scary thing

01:02:23.880 --> 01:02:26.860
is we don't have enough doctors, patients want answers.

01:02:26.860 --> 01:02:29.680
How do we build solutions that can allow this expertise

01:02:29.680 --> 01:02:33.720
to get more democratized and more into folks' hands?

01:02:33.720 --> 01:02:37.160
And I'm hoping our tool along

01:02:37.160 --> 01:02:38.520
with these large language models

01:02:38.520 --> 01:02:41.640
can help relieve some of that burden.

01:02:41.640 --> 01:02:46.640
- Well, it might not be as 100% accurate, 100% precise,

01:02:46.640 --> 01:02:49.440
but neither are doctors, right?

01:02:49.440 --> 01:02:51.040
They get stuff wrong.

01:02:51.040 --> 01:02:54.640
You just need to be in the realm of as good as a doctor.

01:02:54.640 --> 01:02:59.160
You don't need to be completely without making a mistake.

01:02:59.160 --> 01:03:01.860
And that's, I think, a challenge

01:03:01.860 --> 01:03:05.940
that we're just gonna have to get used to in general, right?

01:03:05.940 --> 01:03:08.760
I joked about the legal brief thing

01:03:08.760 --> 01:03:11.480
'cause someone got in trouble for submitting a brief

01:03:11.480 --> 01:03:14.160
and hallucinations. - For sure.

01:03:14.160 --> 01:03:15.760
- And there's certain circumstances

01:03:15.760 --> 01:03:17.240
where maybe it's just not acceptable,

01:03:17.240 --> 01:03:21.680
but AI self-driven cars, people crash,

01:03:21.680 --> 01:03:24.640
but that's like a human mistake.

01:03:24.640 --> 01:03:26.160
But when a machine makes it,

01:03:26.160 --> 01:03:29.040
it's a pre-programmed, predetermined mistake.

01:03:29.040 --> 01:03:29.880
Something like that.

01:03:29.880 --> 01:03:33.080
It doesn't feel the same as if the machine made a mistake.

01:03:33.080 --> 01:03:35.320
So if a machine makes a recommendation,

01:03:35.320 --> 01:03:37.400
like you need this cancer treatment,

01:03:37.400 --> 01:03:40.180
or you're fine, you don't need it, and it was wrong,

01:03:41.180 --> 01:03:43.020
people are not gonna be as forgiving.

01:03:43.020 --> 01:03:45.180
- Agree. - But that doesn't mean

01:03:45.180 --> 01:03:47.620
there's not value to be gained from systems

01:03:47.620 --> 01:03:48.980
that can help you, right?

01:03:48.980 --> 01:03:51.260
- I always appreciate those machine learning papers

01:03:51.260 --> 01:03:53.940
that'll show the tracking of over time

01:03:53.940 --> 01:03:55.900
of how the models have gotten better and better,

01:03:55.900 --> 01:03:56.980
and they put the human in there,

01:03:56.980 --> 01:03:59.060
and you can see that the human has already gotten eclipsed

01:03:59.060 --> 01:04:02.860
by the models, and that specific problem, right?

01:04:02.860 --> 01:04:06.040
'Cause it's also recognizing that a lot of this stuff,

01:04:06.040 --> 01:04:07.540
these models that are doing tasks

01:04:07.540 --> 01:04:08.860
are doing one specific task.

01:04:08.860 --> 01:04:09.980
They're not doing a whole job.

01:04:09.980 --> 01:04:13.100
They're not doing an end-to-end process.

01:04:13.100 --> 01:04:14.780
They're answering a medical question,

01:04:14.780 --> 01:04:16.580
or they're looking at an image

01:04:16.580 --> 01:04:19.180
and finding all the cats or whatever it's supposed to do.

01:04:19.180 --> 01:04:23.360
So, and to your point, though,

01:04:23.360 --> 01:04:26.100
humans aren't perfect at these tasks either.

01:04:26.100 --> 01:04:27.700
- Yeah.

01:04:27.700 --> 01:04:29.940
I think mostly people are gonna be using this kind of stuff

01:04:29.940 --> 01:04:33.180
to help them come up with these answers, right?

01:04:33.180 --> 01:04:35.860
My weird Amazon description example

01:04:35.860 --> 01:04:39.220
is gonna be the edge case, not the go-to.

01:04:39.220 --> 01:04:40.540
- Agreed, agreed.

01:04:40.540 --> 01:04:42.900
- Yeah, you came in, you spoke to the chatbot,

01:04:42.900 --> 01:04:45.140
here's your diagnosis, have a good day, right?

01:04:45.140 --> 01:04:47.180
Not so much, more like,

01:04:47.180 --> 01:04:48.500
I need some help thinking through this.

01:04:48.500 --> 01:04:52.700
What are some studies that have addressed this, right?

01:04:52.700 --> 01:04:54.300
And those kinds of questions.

01:04:54.300 --> 01:04:57.500
- Yep, and I hesitate to say it's just a better search

01:04:57.500 --> 01:04:59.300
engine, 'cause I actually think it's got

01:04:59.300 --> 01:05:00.500
way more potential than that.

01:05:00.500 --> 01:05:01.340
- I agree.

01:05:01.340 --> 01:05:03.420
- It's a conversation.

01:05:03.420 --> 01:05:05.980
It can iterate back and forth,

01:05:05.980 --> 01:05:07.540
and what I'm actually trying to do

01:05:07.540 --> 01:05:09.820
is build some state into it, right?

01:05:09.820 --> 01:05:14.180
Some structured way of kind of remembering

01:05:14.180 --> 01:05:16.180
what the conversation was,

01:05:16.180 --> 01:05:18.220
and using a lot of the techniques

01:05:18.220 --> 01:05:19.620
that these large language models are good at

01:05:19.620 --> 01:05:22.100
to actually, to make that actually happen.

01:05:22.100 --> 01:05:24.420
And so that you can actually build a system

01:05:24.420 --> 01:05:27.620
so that the human and the assistant and the backend

01:05:27.620 --> 01:05:31.540
all kind of know what the other party is thinking about

01:05:31.540 --> 01:05:33.140
and that they all work together.

01:05:33.140 --> 01:05:35.340
- Nice.

01:05:35.340 --> 01:05:40.340
So for your genomics custom GPT thing

01:05:40.340 --> 01:05:42.380
that you're making internally,

01:05:42.380 --> 01:05:45.100
is that gonna become a product eventually

01:05:45.100 --> 01:05:46.260
if other people are interested?

01:05:46.260 --> 01:05:48.260
Is there some way they can keep tabs on it,

01:05:48.260 --> 01:05:49.740
or is it just internal only?

01:05:49.740 --> 01:05:51.500
- Definitely reach out to me.

01:05:51.500 --> 01:05:53.460
So we're building different versions of GPTs.

01:05:53.460 --> 01:05:55.780
Like we're gonna have a GPT for our curation team

01:05:55.780 --> 01:05:57.580
that curates knowledge,

01:05:57.580 --> 01:05:59.380
and we're building a GPT that, you know,

01:05:59.380 --> 01:06:01.380
my hope is that it'll go to physicians,

01:06:01.380 --> 01:06:03.860
to oncologists and genomic counselors

01:06:03.860 --> 01:06:06.580
and other providers that could actually use this thing.

01:06:06.580 --> 01:06:10.900
Eventually, if it becomes, you know, robust enough

01:06:10.900 --> 01:06:12.100
and stable enough,

01:06:12.100 --> 01:06:14.380
and I don't feel like we're doing a disservice,

01:06:14.380 --> 01:06:16.060
we could certainly make a version of that available

01:06:16.060 --> 01:06:17.180
for cancer patients as well.

01:06:17.180 --> 01:06:18.740
I would, you know, I'd love to have that.

01:06:18.740 --> 01:06:21.820
I just wanna make sure that it's done in a responsible way.

01:06:21.820 --> 01:06:23.420
- Yeah, absolutely.

01:06:23.420 --> 01:06:26.560
Well, I honestly hope that you actually do such a good job

01:06:26.560 --> 01:06:28.680
that we don't have to have cancer research anymore,

01:06:28.680 --> 01:06:29.620
but that's-

01:06:29.620 --> 01:06:31.500
- That's really the end goal.

01:06:31.500 --> 01:06:33.620
That's really the end goal.

01:06:33.620 --> 01:06:35.140
- Yes, that is definitely the end goal.

01:06:35.140 --> 01:06:36.700
And that's really exciting too.

01:06:36.700 --> 01:06:39.380
So is that the new drugs that are coming out,

01:06:39.380 --> 01:06:41.580
new treatments that are coming out,

01:06:41.580 --> 01:06:44.860
it's really just about making sure people are aware of it,

01:06:44.860 --> 01:06:46.620
making sure that they're getting the genetic testing

01:06:46.620 --> 01:06:47.460
that they need, right?

01:06:47.460 --> 01:06:49.880
So if you have a loved one that has,

01:06:49.880 --> 01:06:52.180
unfortunately has cancer,

01:06:52.180 --> 01:06:53.940
make sure that they're at least asking their doctor

01:06:53.940 --> 01:06:55.340
the question about genomic testing

01:06:55.340 --> 01:06:56.980
to make sure that they're getting

01:06:56.980 --> 01:06:58.980
the best possible treatment.

01:06:58.980 --> 01:07:00.180
- Sure.

01:07:00.180 --> 01:07:01.100
Sounds good.

01:07:01.100 --> 01:07:02.820
All right, well,

01:07:02.820 --> 01:07:04.740
quickly before we get out of here,

01:07:04.740 --> 01:07:07.900
recommendation on some libraries,

01:07:07.900 --> 01:07:10.780
some project that maybe we haven't talked about yet.

01:07:10.780 --> 01:07:11.620
Something you can tell people,

01:07:11.620 --> 01:07:13.340
like, "Oh, this would be awesome."

01:07:13.340 --> 01:07:14.980
- Yeah, well, we ran out of time.

01:07:14.980 --> 01:07:16.980
I was gonna talk about some of these Pydantic projects.

01:07:16.980 --> 01:07:19.940
So there's Marvin, Instructor, and Outlines.

01:07:19.940 --> 01:07:22.900
So folks should definitely look at those.

01:07:22.900 --> 01:07:24.500
So basically what you do is you take,

01:07:24.500 --> 01:07:26.740
you can describe stuff as Pydantic,

01:07:26.740 --> 01:07:28.140
and then it'll actually just extract it

01:07:28.140 --> 01:07:30.140
right into that Pydantic model for you.

01:07:32.460 --> 01:07:35.220
So Marvin and Outlines and Instructor.

01:07:35.220 --> 01:07:36.260
So check those guys out.

01:07:36.260 --> 01:07:37.180
They're awesome.

01:07:37.180 --> 01:07:40.020
And then the other one that I actually had teed up

01:07:40.020 --> 01:07:41.140
was VisiCalc.

01:07:41.140 --> 01:07:44.460
So VisiCalc is like this crazy command line tool.

01:07:44.460 --> 01:07:45.300
It's awesome.

01:07:45.300 --> 01:07:48.060
Like, you can basically look at giant CSV files

01:07:48.060 --> 01:07:49.020
all in the command line.

01:07:49.020 --> 01:07:51.340
It has like these hotkeys that you can do.

01:07:51.340 --> 01:07:52.180
- Okay.

01:07:52.180 --> 01:07:54.980
- And it, sorry, not VisiCalc, VisiData.

01:07:54.980 --> 01:07:55.820
- VisiData, okay.

01:07:55.820 --> 01:07:56.820
- VisiData.

01:07:56.820 --> 01:07:58.620
And so basically it's just,

01:07:58.620 --> 01:08:00.780
it's basically Excel inside your terminal.

01:08:00.780 --> 01:08:05.620
And this was before Rich and the Textual project.

01:08:05.620 --> 01:08:07.300
And it was just like, it was kind of mind blowing

01:08:07.300 --> 01:08:09.220
all the stuff that this person was able to figure out

01:08:09.220 --> 01:08:10.300
how to make work.

01:08:10.300 --> 01:08:13.660
- Yeah, that's super amazing.

01:08:13.660 --> 01:08:15.180
I just wanted to give a shout out one more thing,

01:08:15.180 --> 01:08:17.780
'cause your VisiData reminded me of something

01:08:17.780 --> 01:08:20.580
I just came across called BTOP.

01:08:20.580 --> 01:08:21.420
- Yep, yep, yep.

01:08:21.420 --> 01:08:22.420
- If people have servers out there

01:08:22.420 --> 01:08:25.100
and they need to know what's going on with their server,

01:08:25.100 --> 01:08:27.000
where's my, I need a picture for this.

01:08:28.580 --> 01:08:32.340
Yeah, it's like a nice visualization.

01:08:32.340 --> 01:08:34.980
I don't know why there's no pictures on this homepage.

01:08:34.980 --> 01:08:37.820
No, but yeah, check it out.

01:08:37.820 --> 01:08:39.180
There's also BPyTOP.

01:08:39.180 --> 01:08:42.820
It's pretty amazing what people can do in the terminal,

01:08:42.820 --> 01:08:44.260
right?

01:08:44.260 --> 01:08:45.100
- Yeah, it's awesome.

01:08:45.100 --> 01:08:47.500
- It's like, I don't know where the,

01:08:47.500 --> 01:08:49.020
there used to be pictures in this, whatever.

01:08:49.020 --> 01:08:49.860
Oh, there they are.

01:08:49.860 --> 01:08:53.140
They're just responsive design themselves out.

01:08:53.140 --> 01:08:56.140
But yeah, if you want a live, a bunch of live graphs,

01:08:56.140 --> 01:08:57.380
every time I see stuff like this,

01:08:57.380 --> 01:08:59.060
the VisiData or this,

01:08:59.060 --> 01:09:01.700
or what the textual folks are working on,

01:09:01.700 --> 01:09:03.660
it's just like, I can't believe they built this, right?

01:09:03.660 --> 01:09:06.220
Like I'm working at the level of Colorama.

01:09:06.220 --> 01:09:08.100
This string is right here.

01:09:08.100 --> 01:09:08.940
- Right, right, right.

01:09:08.940 --> 01:09:10.100
- And they're like, oh yeah, we're gonna build this.

01:09:10.100 --> 01:09:11.540
- I got an emoji to show up, right?

01:09:11.540 --> 01:09:12.380
I'm excited.

01:09:12.380 --> 01:09:13.780
- Yes, exactly, yes.

01:09:13.780 --> 01:09:15.740
A rocket ship is there, not just tech.

01:09:15.740 --> 01:09:17.340
(laughing)

01:09:17.340 --> 01:09:18.500
Yeah, pretty excellent.

01:09:18.500 --> 01:09:22.780
All right, well, Ian, thank you for being here.

01:09:22.780 --> 01:09:24.380
And keep up the good work.

01:09:24.860 --> 01:09:27.460
I know so many people are using LLMs,

01:09:27.460 --> 01:09:30.220
but not that many people are creating LLMs.

01:09:30.220 --> 01:09:32.700
And as developers, we love to create things.

01:09:32.700 --> 01:09:34.580
We already have the tools to do it.

01:09:34.580 --> 01:09:39.580
People can check out your GitHub repo on the PyPI and GPT

01:09:39.580 --> 01:09:42.220
and use it as a starting place, right?

01:09:42.220 --> 01:09:43.060
- Sounds great.

01:09:43.060 --> 01:09:45.820
Yeah, and definitely reach out if you have any questions.

01:09:45.820 --> 01:09:47.540
- Excellent.

01:09:47.540 --> 01:09:48.620
Well, thanks for coming back on the show.

01:09:48.620 --> 01:09:49.460
Catch you later.

01:09:49.460 --> 01:09:50.620
- Great, good to talk to you.

01:09:50.620 --> 01:09:51.460
Bye-bye.

01:09:51.460 --> 01:09:52.300
- Yeah, you bet, bye.

01:09:52.300 --> 01:10:02.300
[BLANK_AUDIO]

